{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-10T04:24:58.506357Z",
     "start_time": "2024-03-10T04:24:56.996081Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fastcore.basics import Path, AttrDict\n",
    "from dataset import SPLID\n",
    "import torch\n",
    "\n",
    "import os\n",
    "config = AttrDict(\n",
    "    challenge_data_dir = Path('~/Projects/splid-comp/dataset').expanduser(),\n",
    "    valid_ratio = 0.1,\n",
    "    kernel_size = 5,\n",
    "    tolerance= 6, # Default evaluation tolerance\n",
    ")\n",
    "\n",
    "# Define the directory paths\n",
    "train_data_dir = config.challenge_data_dir / \"train\"\n",
    "\n",
    "# Load the ground truth data\n",
    "ground_truth = config.challenge_data_dir / 'train_labels.csv'\n",
    "\n",
    "datalist = []\n",
    "\n",
    "# Searching for training data within the dataset folder\n",
    "for file in os.listdir(train_data_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        datalist.append(os.path.join(train_data_dir, file))\n",
    "\n",
    "# Sort the training data and labels\n",
    "datalist = sorted(datalist, key=lambda i: int(os.path.splitext(os.path.basename(i))[0]))\n",
    "    \n",
    "\n",
    "train_datalist, test_datalist = train_test_split(datalist, test_size=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-10T04:56:34.579534Z",
     "start_time": "2024-03-10T04:24:59.446211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1615 files...\n",
      "Loaded file 0 of 1615\n",
      "Loaded file 50 of 1615\n",
      "Loaded file 100 of 1615\n",
      "Loaded file 150 of 1615\n",
      "Loaded file 200 of 1615\n",
      "Loaded file 250 of 1615\n",
      "Loaded file 300 of 1615\n",
      "Loaded file 350 of 1615\n",
      "Loaded file 400 of 1615\n",
      "Loaded file 450 of 1615\n",
      "Loaded file 500 of 1615\n",
      "Loaded file 550 of 1615\n",
      "Loaded file 600 of 1615\n",
      "Loaded file 650 of 1615\n",
      "Loaded file 700 of 1615\n",
      "Loaded file 750 of 1615\n",
      "Loaded file 800 of 1615\n",
      "Loaded file 850 of 1615\n",
      "Loaded file 900 of 1615\n",
      "Loaded file 950 of 1615\n",
      "Loaded file 1000 of 1615\n",
      "Loaded file 1050 of 1615\n",
      "Loaded file 1100 of 1615\n",
      "Loaded file 1150 of 1615\n",
      "Loaded file 1200 of 1615\n",
      "Loaded file 1250 of 1615\n",
      "Loaded file 1300 of 1615\n",
      "Loaded file 1350 of 1615\n",
      "Loaded file 1400 of 1615\n",
      "Loaded file 1450 of 1615\n",
      "Loaded file 1500 of 1615\n",
      "Loaded file 1550 of 1615\n",
      "Loaded file 1600 of 1615\n",
      "Joining dataframes...\n",
      "Done!\n",
      "Loading 285 files...\n",
      "Loaded file 0 of 285\n",
      "Loaded file 50 of 285\n",
      "Loaded file 100 of 285\n",
      "Loaded file 150 of 285\n",
      "Loaded file 200 of 285\n",
      "Loaded file 250 of 285\n",
      "Joining dataframes...\n",
      "Done!\n",
      "Start model training\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 2.0355868697166444\n",
      "  batch 100 loss: 1.6960068225860596\n",
      "  batch 150 loss: 1.4111592650413514\n",
      "  batch 200 loss: 1.2191187071800231\n",
      "LOSS train 1.2191187071800231 valid 1.0632008959849675\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.0886766993999482\n",
      "  batch 100 loss: 1.0385822606086732\n",
      "  batch 150 loss: 1.0279465126991272\n",
      "  batch 200 loss: 0.9147723627090454\n",
      "LOSS train 0.9147723627090454 valid 0.9302112874057558\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 0.8805691528320313\n",
      "  batch 100 loss: 0.916999808549881\n",
      "  batch 150 loss: 0.9029167956113815\n",
      "  batch 200 loss: 0.8224581933021545\n",
      "LOSS train 0.8224581933021545 valid 0.8251345041725371\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.8479911422729492\n",
      "  batch 100 loss: 0.7782055723667145\n",
      "  batch 150 loss: 0.7514493346214295\n",
      "  batch 200 loss: 0.8102119863033295\n",
      "LOSS train 0.8102119863033295 valid 0.7262752966748344\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.8100911283493042\n",
      "  batch 100 loss: 0.7266714137792587\n",
      "  batch 150 loss: 0.7491698414087296\n",
      "  batch 200 loss: 0.663956635594368\n",
      "LOSS train 0.663956635594368 valid 0.668436119125949\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.7018001174926758\n",
      "  batch 100 loss: 0.6496859562397003\n",
      "  batch 150 loss: 0.6449770814180374\n",
      "  batch 200 loss: 0.7029971379041672\n",
      "LOSS train 0.7029971379041672 valid 0.6591818705201149\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.6408086973428726\n",
      "  batch 100 loss: 0.6722827643156052\n",
      "  batch 150 loss: 0.6521923917531968\n",
      "  batch 200 loss: 0.6110181319713592\n",
      "LOSS train 0.6110181319713592 valid 0.6338432447777854\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.6805839005112648\n",
      "  batch 100 loss: 0.58963583111763\n",
      "  batch 150 loss: 0.6117936539649963\n",
      "  batch 200 loss: 0.5969475531578063\n",
      "LOSS train 0.5969475531578063 valid 0.5928314030170441\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.591646508872509\n",
      "  batch 100 loss: 0.6782834950089455\n",
      "  batch 150 loss: 0.5297368434071541\n",
      "  batch 200 loss: 0.5797931617498397\n",
      "LOSS train 0.5797931617498397 valid 0.5647709866364797\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.6127422526478767\n",
      "  batch 100 loss: 0.5790420466661453\n",
      "  batch 150 loss: 0.5618606728315353\n",
      "  batch 200 loss: 0.5431407761573791\n",
      "LOSS train 0.5431407761573791 valid 0.5365767880446382\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.5508525758981705\n",
      "  batch 100 loss: 0.5850338274240494\n",
      "  batch 150 loss: 0.5103035551309586\n",
      "  batch 200 loss: 0.5447132590413094\n",
      "LOSS train 0.5447132590413094 valid 0.5258818239801459\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.6162072694301606\n",
      "  batch 100 loss: 0.5236062821745873\n",
      "  batch 150 loss: 0.5140450456738472\n",
      "  batch 200 loss: 0.5662288397550583\n",
      "LOSS train 0.5662288397550583 valid 0.524045349823104\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.5719674915075302\n",
      "  batch 100 loss: 0.509872998893261\n",
      "  batch 150 loss: 0.5241310778260231\n",
      "  batch 200 loss: 0.5461649358272552\n",
      "LOSS train 0.5461649358272552 valid 0.4933728960653146\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.5264739179611206\n",
      "  batch 100 loss: 0.5207559326291085\n",
      "  batch 150 loss: 0.5062845727801323\n",
      "  batch 200 loss: 0.5136370131373406\n",
      "LOSS train 0.5136370131373406 valid 0.5068840425875452\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.4428832283616066\n",
      "  batch 100 loss: 0.5227106919884682\n",
      "  batch 150 loss: 0.565271575152874\n",
      "  batch 200 loss: 0.4908538377285004\n",
      "LOSS train 0.4908538377285004 valid 0.48727064538333154\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.5187235134840011\n",
      "  batch 100 loss: 0.49524214655160903\n",
      "  batch 150 loss: 0.5097710511088371\n",
      "  batch 200 loss: 0.4602916580438614\n",
      "LOSS train 0.4602916580438614 valid 0.5119789445565807\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.494659244120121\n",
      "  batch 100 loss: 0.4529099038243294\n",
      "  batch 150 loss: 0.548622294664383\n",
      "  batch 200 loss: 0.4983897790312767\n",
      "LOSS train 0.4983897790312767 valid 0.45885083534651333\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.5113399964570999\n",
      "  batch 100 loss: 0.47093363642692565\n",
      "  batch 150 loss: 0.44139053076505663\n",
      "  batch 200 loss: 0.4805742114782333\n",
      "LOSS train 0.4805742114782333 valid 0.48108280822634697\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.49506365388631823\n",
      "  batch 100 loss: 0.44981312811374663\n",
      "  batch 150 loss: 0.5155596190690994\n",
      "  batch 200 loss: 0.42947457015514373\n",
      "LOSS train 0.42947457015514373 valid 0.4902128411663903\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.47636387676000597\n",
      "  batch 100 loss: 0.4782985392212868\n",
      "  batch 150 loss: 0.43519880294799806\n",
      "  batch 200 loss: 0.46252978682518003\n",
      "LOSS train 0.46252978682518003 valid 0.4542257934808731\n",
      "EPOCH 21:\n",
      "  batch 50 loss: 0.5189414486289025\n",
      "  batch 100 loss: 0.4619212993979454\n",
      "  batch 150 loss: 0.4180729678273201\n",
      "  batch 200 loss: 0.41181566208601\n",
      "LOSS train 0.41181566208601 valid 0.44977039140131736\n",
      "EPOCH 22:\n",
      "  batch 50 loss: 0.4366801941394806\n",
      "  batch 100 loss: 0.46023575723171234\n",
      "  batch 150 loss: 0.45284753561019897\n",
      "  batch 200 loss: 0.4527912962436676\n",
      "LOSS train 0.4527912962436676 valid 0.43575097289350295\n",
      "EPOCH 23:\n",
      "  batch 50 loss: 0.38518712371587754\n",
      "  batch 100 loss: 0.42721028536558153\n",
      "  batch 150 loss: 0.4786136245727539\n",
      "  batch 200 loss: 0.43319976568222046\n",
      "LOSS train 0.43319976568222046 valid 0.437960644149118\n",
      "EPOCH 24:\n",
      "  batch 50 loss: 0.46026987373828887\n",
      "  batch 100 loss: 0.39377767652273177\n",
      "  batch 150 loss: 0.41067173689603803\n",
      "  batch 200 loss: 0.43806681990623475\n",
      "LOSS train 0.43806681990623475 valid 0.43115104155408013\n",
      "EPOCH 25:\n",
      "  batch 50 loss: 0.3791765469312668\n",
      "  batch 100 loss: 0.4585061165690422\n",
      "  batch 150 loss: 0.42567747056484223\n",
      "  batch 200 loss: 0.4001327934861183\n",
      "LOSS train 0.4001327934861183 valid 0.4304847191605303\n",
      "EPOCH 26:\n",
      "  batch 50 loss: 0.4102034944295883\n",
      "  batch 100 loss: 0.41032626867294314\n",
      "  batch 150 loss: 0.42221787065267563\n",
      "  batch 200 loss: 0.41633962869644164\n",
      "LOSS train 0.41633962869644164 valid 0.4345914835317267\n",
      "EPOCH 27:\n",
      "  batch 50 loss: 0.4231270086765289\n",
      "  batch 100 loss: 0.41014324337244035\n",
      "  batch 150 loss: 0.38797520637512206\n",
      "  batch 200 loss: 0.3715128144621849\n",
      "LOSS train 0.3715128144621849 valid 0.433259060813321\n",
      "EPOCH 28:\n",
      "  batch 50 loss: 0.3773878583312035\n",
      "  batch 100 loss: 0.37628810226917264\n",
      "  batch 150 loss: 0.4194755411148071\n",
      "  batch 200 loss: 0.4499017481505871\n",
      "LOSS train 0.4499017481505871 valid 0.4311900134715769\n",
      "EPOCH 29:\n",
      "  batch 50 loss: 0.3537862634658813\n",
      "  batch 100 loss: 0.39481675773859026\n",
      "  batch 150 loss: 0.39262658685445784\n",
      "  batch 200 loss: 0.38710320979356766\n",
      "LOSS train 0.38710320979356766 valid 0.39979735140999156\n",
      "EPOCH 30:\n",
      "  batch 50 loss: 0.40775117158889773\n",
      "  batch 100 loss: 0.3810221302509308\n",
      "  batch 150 loss: 0.3729211787879467\n",
      "  batch 200 loss: 0.39160815462470056\n",
      "LOSS train 0.39160815462470056 valid 0.39546241238713264\n",
      "EPOCH 31:\n",
      "  batch 50 loss: 0.41145955830812453\n",
      "  batch 100 loss: 0.3958020731806755\n",
      "  batch 150 loss: 0.36571659505367277\n",
      "  batch 200 loss: 0.3516994750499725\n",
      "LOSS train 0.3516994750499725 valid 0.41604893654584885\n",
      "EPOCH 32:\n",
      "  batch 50 loss: 0.40091368436813357\n",
      "  batch 100 loss: 0.3433745050430298\n",
      "  batch 150 loss: 0.4134467577934265\n",
      "  batch 200 loss: 0.3462556555867195\n",
      "LOSS train 0.3462556555867195 valid 0.41630416363477707\n",
      "EPOCH 33:\n",
      "  batch 50 loss: 0.3333389922976494\n",
      "  batch 100 loss: 0.3949631063640118\n",
      "  batch 150 loss: 0.34978263795375825\n",
      "  batch 200 loss: 0.361593582034111\n",
      "LOSS train 0.361593582034111 valid 0.40771373071604305\n",
      "EPOCH 34:\n",
      "  batch 50 loss: 0.3283162051439285\n",
      "  batch 100 loss: 0.3693833988904953\n",
      "  batch 150 loss: 0.3928778538107872\n",
      "  batch 200 loss: 0.34132554501295087\n",
      "LOSS train 0.34132554501295087 valid 0.38795423486994374\n",
      "EPOCH 35:\n",
      "  batch 50 loss: 0.3999290469288826\n",
      "  batch 100 loss: 0.3066011501848698\n",
      "  batch 150 loss: 0.37856586575508117\n",
      "  batch 200 loss: 0.3793032017350197\n",
      "LOSS train 0.3793032017350197 valid 0.410028837621212\n",
      "EPOCH 36:\n",
      "  batch 50 loss: 0.3256186938285828\n",
      "  batch 100 loss: 0.3386892259120941\n",
      "  batch 150 loss: 0.3627438449859619\n",
      "  batch 200 loss: 0.34390986829996106\n",
      "LOSS train 0.34390986829996106 valid 0.4396534843577279\n",
      "EPOCH 37:\n",
      "  batch 50 loss: 0.37096025198698046\n",
      "  batch 100 loss: 0.37822269797325136\n",
      "  batch 150 loss: 0.32933060720562934\n",
      "  batch 200 loss: 0.30659515857696534\n",
      "LOSS train 0.30659515857696534 valid 0.4128310318208403\n",
      "EPOCH 38:\n",
      "  batch 50 loss: 0.33620291873812674\n",
      "  batch 100 loss: 0.3491729773581028\n",
      "  batch 150 loss: 0.36590611055493355\n",
      "  batch 200 loss: 0.31685839176177977\n",
      "LOSS train 0.31685839176177977 valid 0.39598028568757904\n",
      "EPOCH 39:\n",
      "  batch 50 loss: 0.3678826256096363\n",
      "  batch 100 loss: 0.3266716620326042\n",
      "  batch 150 loss: 0.3456268906593323\n",
      "  batch 200 loss: 0.3228928254544735\n",
      "LOSS train 0.3228928254544735 valid 0.3680717067586051\n",
      "EPOCH 40:\n",
      "  batch 50 loss: 0.3302606835961342\n",
      "  batch 100 loss: 0.3550828172266483\n",
      "  batch 150 loss: 0.3133848349750042\n",
      "  batch 200 loss: 0.3236837339401245\n",
      "LOSS train 0.3236837339401245 valid 0.38061777378122014\n",
      "EPOCH 41:\n",
      "  batch 50 loss: 0.3187678271532059\n",
      "  batch 100 loss: 0.3448278769850731\n",
      "  batch 150 loss: 0.3203777715563774\n",
      "  batch 200 loss: 0.33321931555867196\n",
      "LOSS train 0.33321931555867196 valid 0.3720356946190198\n",
      "EPOCH 42:\n",
      "  batch 50 loss: 0.3591808620095253\n",
      "  batch 100 loss: 0.3250753179192543\n",
      "  batch 150 loss: 0.30061896294355395\n",
      "  batch 200 loss: 0.28758032888174057\n",
      "LOSS train 0.28758032888174057 valid 0.373800122903453\n",
      "EPOCH 43:\n",
      "  batch 50 loss: 0.3343681007623672\n",
      "  batch 100 loss: 0.35459317117929456\n",
      "  batch 150 loss: 0.31330870136618616\n",
      "  batch 200 loss: 0.3250728404521942\n",
      "LOSS train 0.3250728404521942 valid 0.3928178877880176\n",
      "EPOCH 44:\n",
      "  batch 50 loss: 0.3595339675247669\n",
      "  batch 100 loss: 0.27961424440145494\n",
      "  batch 150 loss: 0.3107333492487669\n",
      "  batch 200 loss: 0.30145358487963675\n",
      "LOSS train 0.30145358487963675 valid 0.37943118231164086\n",
      "EPOCH 45:\n",
      "  batch 50 loss: 0.3282721365988255\n",
      "  batch 100 loss: 0.3380493527650833\n",
      "  batch 150 loss: 0.3106022728979588\n",
      "  batch 200 loss: 0.30246695891022685\n",
      "LOSS train 0.30246695891022685 valid 0.38194561522040105\n",
      "EPOCH 46:\n",
      "  batch 50 loss: 0.2783636449277401\n",
      "  batch 100 loss: 0.34646615892648697\n",
      "  batch 150 loss: 0.2756052502989769\n",
      "  batch 200 loss: 0.3162395714223385\n",
      "LOSS train 0.3162395714223385 valid 0.364119668594665\n",
      "EPOCH 47:\n",
      "  batch 50 loss: 0.2867703592777252\n",
      "  batch 100 loss: 0.3134841299057007\n",
      "  batch 150 loss: 0.2975198364257812\n",
      "  batch 200 loss: 0.3001950392127037\n",
      "LOSS train 0.3001950392127037 valid 0.36984144772092503\n",
      "EPOCH 48:\n",
      "  batch 50 loss: 0.2793234398961067\n",
      "  batch 100 loss: 0.2994139078259468\n",
      "  batch 150 loss: 0.34285843938589095\n",
      "  batch 200 loss: 0.3014541926980019\n",
      "LOSS train 0.3014541926980019 valid 0.3560430906299088\n",
      "EPOCH 49:\n",
      "  batch 50 loss: 0.3169423118233681\n",
      "  batch 100 loss: 0.3007516813278198\n",
      "  batch 150 loss: 0.2900852246582508\n",
      "  batch 200 loss: 0.2850524462759495\n",
      "LOSS train 0.2850524462759495 valid 0.37792361134456265\n",
      "EPOCH 50:\n",
      "  batch 50 loss: 0.2676448802649975\n",
      "  batch 100 loss: 0.2748564128577709\n",
      "  batch 150 loss: 0.3084025275707245\n",
      "  batch 200 loss: 0.3076325051486492\n",
      "LOSS train 0.3076325051486492 valid 0.37863720394670963\n",
      "EPOCH 51:\n",
      "  batch 50 loss: 0.31973979592323304\n",
      "  batch 100 loss: 0.292552233338356\n",
      "  batch 150 loss: 0.26799559250473975\n",
      "  batch 200 loss: 0.24375026300549507\n",
      "LOSS train 0.24375026300549507 valid 0.3656300159378184\n",
      "EPOCH 52:\n",
      "  batch 50 loss: 0.27212770238518713\n",
      "  batch 100 loss: 0.30053085297346116\n",
      "  batch 150 loss: 0.2896260292828083\n",
      "  batch 200 loss: 0.3142131593823433\n",
      "LOSS train 0.3142131593823433 valid 0.37532832680477035\n",
      "EPOCH 53:\n",
      "  batch 50 loss: 0.27115929901599883\n",
      "  batch 100 loss: 0.2599240264296532\n",
      "  batch 150 loss: 0.28514015406370163\n",
      "  batch 200 loss: 0.3140259151160717\n",
      "LOSS train 0.3140259151160717 valid 0.3562003558294641\n",
      "EPOCH 54:\n",
      "  batch 50 loss: 0.2901889982819557\n",
      "  batch 100 loss: 0.28939429819583895\n",
      "  batch 150 loss: 0.28778928935527803\n",
      "  batch 200 loss: 0.2769825382530689\n",
      "LOSS train 0.2769825382530689 valid 0.34849827198518646\n",
      "EPOCH 55:\n",
      "  batch 50 loss: 0.26873384788632393\n",
      "  batch 100 loss: 0.24339666098356247\n",
      "  batch 150 loss: 0.29729061022400854\n",
      "  batch 200 loss: 0.31401640504598616\n",
      "LOSS train 0.31401640504598616 valid 0.36067293356690144\n",
      "EPOCH 56:\n",
      "  batch 50 loss: 0.2645963001996279\n",
      "  batch 100 loss: 0.2909273175150156\n",
      "  batch 150 loss: 0.25408169865608216\n",
      "  batch 200 loss: 0.2857944467663765\n",
      "LOSS train 0.2857944467663765 valid 0.35179910788105595\n",
      "EPOCH 57:\n",
      "  batch 50 loss: 0.31115377098321917\n",
      "  batch 100 loss: 0.2718215999007225\n",
      "  batch 150 loss: 0.26612634032964705\n",
      "  batch 200 loss: 0.2805549517273903\n",
      "LOSS train 0.2805549517273903 valid 0.35629542854924995\n",
      "EPOCH 58:\n",
      "  batch 50 loss: 0.2771107037365437\n",
      "  batch 100 loss: 0.2629729203879833\n",
      "  batch 150 loss: 0.33167792215943337\n",
      "  batch 200 loss: 0.23857813373208045\n",
      "LOSS train 0.23857813373208045 valid 0.35071219152046573\n",
      "EPOCH 59:\n",
      "  batch 50 loss: 0.2709780675172806\n",
      "  batch 100 loss: 0.2674367542564869\n",
      "  batch 150 loss: 0.3025828781723976\n",
      "  batch 200 loss: 0.2576877461373806\n",
      "LOSS train 0.2576877461373806 valid 0.3648237283858988\n",
      "EPOCH 60:\n",
      "  batch 50 loss: 0.2522205910086632\n",
      "  batch 100 loss: 0.2781729692220688\n",
      "  batch 150 loss: 0.2593634800612927\n",
      "  batch 200 loss: 0.2805084064602852\n",
      "LOSS train 0.2805084064602852 valid 0.35458381060096955\n",
      "EPOCH 61:\n",
      "  batch 50 loss: 0.27718722268939017\n",
      "  batch 100 loss: 0.23916517361998557\n",
      "  batch 150 loss: 0.30939424321055414\n",
      "  batch 200 loss: 0.2925695513188839\n",
      "LOSS train 0.2925695513188839 valid 0.33491879846486783\n",
      "EPOCH 62:\n",
      "  batch 50 loss: 0.27752726584672927\n",
      "  batch 100 loss: 0.28541566118597983\n",
      "  batch 150 loss: 0.2246517151594162\n",
      "  batch 200 loss: 0.24886422768235206\n",
      "LOSS train 0.24886422768235206 valid 0.3319031194680267\n",
      "EPOCH 63:\n",
      "  batch 50 loss: 0.23244228214025497\n",
      "  batch 100 loss: 0.26442712992429734\n",
      "  batch 150 loss: 0.30010304644703867\n",
      "  batch 200 loss: 0.2306945376098156\n",
      "LOSS train 0.2306945376098156 valid 0.3561611862646209\n",
      "EPOCH 64:\n",
      "  batch 50 loss: 0.24326123610138894\n",
      "  batch 100 loss: 0.2562510076910257\n",
      "  batch 150 loss: 0.24840159945189952\n",
      "  batch 200 loss: 0.27152377739548683\n",
      "LOSS train 0.27152377739548683 valid 0.3665684543343054\n",
      "EPOCH 65:\n",
      "  batch 50 loss: 0.25271827444434164\n",
      "  batch 100 loss: 0.2681137707829475\n",
      "  batch 150 loss: 0.24526079773902892\n",
      "  batch 200 loss: 0.22851358383893966\n",
      "LOSS train 0.22851358383893966 valid 0.3827010525597466\n",
      "EPOCH 66:\n",
      "  batch 50 loss: 0.23365040838718415\n",
      "  batch 100 loss: 0.22056453675031662\n",
      "  batch 150 loss: 0.29804222732782365\n",
      "  batch 200 loss: 0.27693113312125206\n",
      "LOSS train 0.27693113312125206 valid 0.36542195454239845\n",
      "EPOCH 67:\n",
      "  batch 50 loss: 0.26182534024119375\n",
      "  batch 100 loss: 0.2732802859693766\n",
      "  batch 150 loss: 0.221887204349041\n",
      "  batch 200 loss: 0.24115503326058388\n",
      "LOSS train 0.24115503326058388 valid 0.33361747591859764\n",
      "EPOCH 68:\n",
      "  batch 50 loss: 0.22844479262828826\n",
      "  batch 100 loss: 0.25864557951688766\n",
      "  batch 150 loss: 0.3055491182208061\n",
      "  batch 200 loss: 0.2127656528353691\n",
      "LOSS train 0.2127656528353691 valid 0.36847661973701584\n",
      "EPOCH 69:\n",
      "  batch 50 loss: 0.20569100692868234\n",
      "  batch 100 loss: 0.2917157144844532\n",
      "  batch 150 loss: 0.26773679077625273\n",
      "  batch 200 loss: 0.25371182411909105\n",
      "LOSS train 0.25371182411909105 valid 0.39409206041859257\n",
      "EPOCH 70:\n",
      "  batch 50 loss: 0.22965056903660297\n",
      "  batch 100 loss: 0.24035549089312552\n",
      "  batch 150 loss: 0.23506207711994648\n",
      "  batch 200 loss: 0.25570340231060984\n",
      "LOSS train 0.25570340231060984 valid 0.3740572476138671\n",
      "EPOCH 71:\n",
      "  batch 50 loss: 0.23142556682229043\n",
      "  batch 100 loss: 0.27354884281754493\n",
      "  batch 150 loss: 0.23432622820138932\n",
      "  batch 200 loss: 0.24141708575189114\n",
      "LOSS train 0.24141708575189114 valid 0.3711323160678148\n",
      "EPOCH 72:\n",
      "  batch 50 loss: 0.25492068499326703\n",
      "  batch 100 loss: 0.26666460737586023\n",
      "  batch 150 loss: 0.22286893993616105\n",
      "  batch 200 loss: 0.22691796481609344\n",
      "LOSS train 0.22691796481609344 valid 0.4057939967347516\n",
      "EPOCH 73:\n",
      "  batch 50 loss: 0.2428821189701557\n",
      "  batch 100 loss: 0.26140247859060767\n",
      "  batch 150 loss: 0.299503735601902\n",
      "  batch 200 loss: 0.21257224649190903\n",
      "LOSS train 0.21257224649190903 valid 0.37373234352303875\n",
      "EPOCH 74:\n",
      "  batch 50 loss: 0.241290522813797\n",
      "  batch 100 loss: 0.24861872240900992\n",
      "  batch 150 loss: 0.24315934225916863\n",
      "  batch 200 loss: 0.21140303790569306\n",
      "LOSS train 0.21140303790569306 valid 0.3862462728801701\n",
      "EPOCH 75:\n",
      "  batch 50 loss: 0.23383724823594093\n",
      "  batch 100 loss: 0.2496009363234043\n",
      "  batch 150 loss: 0.23308842316269873\n",
      "  batch 200 loss: 0.22333246380090713\n",
      "LOSS train 0.22333246380090713 valid 0.39485279760426945\n",
      "EPOCH 76:\n",
      "  batch 50 loss: 0.24395079642534256\n",
      "  batch 100 loss: 0.2221551065146923\n",
      "  batch 150 loss: 0.2548957236111164\n",
      "  batch 200 loss: 0.2253402903676033\n",
      "LOSS train 0.2253402903676033 valid 0.3689070673038562\n",
      "EPOCH 77:\n",
      "  batch 50 loss: 0.24751274809241294\n",
      "  batch 100 loss: 0.237885282933712\n",
      "  batch 150 loss: 0.22649471312761307\n",
      "  batch 200 loss: 0.20779241755604744\n",
      "LOSS train 0.20779241755604744 valid 0.4013385737521781\n",
      "EPOCH 78:\n",
      "  batch 50 loss: 0.2163715809583664\n",
      "  batch 100 loss: 0.29144828379154203\n",
      "  batch 150 loss: 0.19377456977963448\n",
      "  batch 200 loss: 0.2548502434790134\n",
      "LOSS train 0.2548502434790134 valid 0.37195895094838405\n",
      "EPOCH 79:\n",
      "  batch 50 loss: 0.23350597463548184\n",
      "  batch 100 loss: 0.2638924989104271\n",
      "  batch 150 loss: 0.2346481528878212\n",
      "  batch 200 loss: 0.24497534587979317\n",
      "LOSS train 0.24497534587979317 valid 0.36215520422491765\n",
      "EPOCH 80:\n",
      "  batch 50 loss: 0.1905972595512867\n",
      "  batch 100 loss: 0.2727363596856594\n",
      "  batch 150 loss: 0.22173735737800598\n",
      "  batch 200 loss: 0.23104907102882863\n",
      "LOSS train 0.23104907102882863 valid 0.36685183747775024\n",
      "EPOCH 81:\n",
      "  batch 50 loss: 0.21197882369160653\n",
      "  batch 100 loss: 0.19112245105206965\n",
      "  batch 150 loss: 0.2285493628680706\n",
      "  batch 200 loss: 0.2548270484805107\n",
      "LOSS train 0.2548270484805107 valid 0.36450344924297595\n",
      "EPOCH 82:\n",
      "  batch 50 loss: 0.21914432428777217\n",
      "  batch 100 loss: 0.20080443143844603\n",
      "  batch 150 loss: 0.23145708978176116\n",
      "  batch 200 loss: 0.24369933009147643\n",
      "LOSS train 0.24369933009147643 valid 0.3618236395219962\n",
      "EPOCH 83:\n",
      "  batch 50 loss: 0.22234879806637764\n",
      "  batch 100 loss: 0.21428488492965697\n",
      "  batch 150 loss: 0.2197656048089266\n",
      "  batch 200 loss: 0.21847807750105858\n",
      "LOSS train 0.21847807750105858 valid 0.36496578798525864\n",
      "EPOCH 84:\n",
      "  batch 50 loss: 0.23589733600616455\n",
      "  batch 100 loss: 0.17401052594184876\n",
      "  batch 150 loss: 0.21926227085292338\n",
      "  batch 200 loss: 0.2654432578384876\n",
      "LOSS train 0.2654432578384876 valid 0.39204199777709114\n",
      "EPOCH 85:\n",
      "  batch 50 loss: 0.24803866282105447\n",
      "  batch 100 loss: 0.22422598376870156\n",
      "  batch 150 loss: 0.20653688237071038\n",
      "  batch 200 loss: 0.2200449837744236\n",
      "LOSS train 0.2200449837744236 valid 0.34580345793316764\n",
      "EPOCH 86:\n",
      "  batch 50 loss: 0.22878410652279854\n",
      "  batch 100 loss: 0.20757815405726432\n",
      "  batch 150 loss: 0.23969351708889008\n",
      "  batch 200 loss: 0.2080481854081154\n",
      "LOSS train 0.2080481854081154 valid 0.3281950672260589\n",
      "EPOCH 87:\n",
      "  batch 50 loss: 0.2013386333733797\n",
      "  batch 100 loss: 0.21069824635982515\n",
      "  batch 150 loss: 0.2372770495712757\n",
      "  batch 200 loss: 0.232699141651392\n",
      "LOSS train 0.232699141651392 valid 0.3791370177641511\n",
      "EPOCH 88:\n",
      "  batch 50 loss: 0.18873677670955658\n",
      "  batch 100 loss: 0.23416408129036426\n",
      "  batch 150 loss: 0.24260831877589226\n",
      "  batch 200 loss: 0.1830010822415352\n",
      "LOSS train 0.1830010822415352 valid 0.38314790692594314\n",
      "EPOCH 89:\n",
      "  batch 50 loss: 0.1905807562172413\n",
      "  batch 100 loss: 0.22673250913619994\n",
      "  batch 150 loss: 0.24542380705475808\n",
      "  batch 200 loss: 0.21612823039293289\n",
      "LOSS train 0.21612823039293289 valid 0.3674108704759015\n",
      "EPOCH 90:\n",
      "  batch 50 loss: 0.228536663800478\n",
      "  batch 100 loss: 0.20128276653587818\n",
      "  batch 150 loss: 0.22115457557141782\n",
      "  batch 200 loss: 0.21384085297584535\n",
      "LOSS train 0.21384085297584535 valid 0.3578722772912847\n",
      "EPOCH 91:\n",
      "  batch 50 loss: 0.22002056948840618\n",
      "  batch 100 loss: 0.19844479374587537\n",
      "  batch 150 loss: 0.2042039467394352\n",
      "  batch 200 loss: 0.21338437013328077\n",
      "LOSS train 0.21338437013328077 valid 0.40192500770919853\n",
      "EPOCH 92:\n",
      "  batch 50 loss: 0.2035585443675518\n",
      "  batch 100 loss: 0.17247485518455505\n",
      "  batch 150 loss: 0.21495649352669716\n",
      "  batch 200 loss: 0.23130221500992776\n",
      "LOSS train 0.23130221500992776 valid 0.34175295092993313\n",
      "EPOCH 93:\n",
      "  batch 50 loss: 0.18144782334566117\n",
      "  batch 100 loss: 0.20277227088809013\n",
      "  batch 150 loss: 0.24715303674340247\n",
      "  batch 200 loss: 0.22795410379767417\n",
      "LOSS train 0.22795410379767417 valid 0.35004005229307544\n",
      "EPOCH 94:\n",
      "  batch 50 loss: 0.19791753306984902\n",
      "  batch 100 loss: 0.25052552327513694\n",
      "  batch 150 loss: 0.21846021480858327\n",
      "  batch 200 loss: 0.19285340428352357\n",
      "LOSS train 0.19285340428352357 valid 0.3546381177794602\n",
      "EPOCH 95:\n",
      "  batch 50 loss: 0.1832065111398697\n",
      "  batch 100 loss: 0.2202691026031971\n",
      "  batch 150 loss: 0.20720671080052852\n",
      "  batch 200 loss: 0.19297492548823356\n",
      "LOSS train 0.19297492548823356 valid 0.35582966957655215\n",
      "EPOCH 96:\n",
      "  batch 50 loss: 0.19917347207665442\n",
      "  batch 100 loss: 0.19853824608027934\n",
      "  batch 150 loss: 0.20772181272506715\n",
      "  batch 200 loss: 0.2134637050330639\n",
      "LOSS train 0.2134637050330639 valid 0.35294887454559404\n",
      "EPOCH 97:\n",
      "  batch 50 loss: 0.21055979683995246\n",
      "  batch 100 loss: 0.17222588554024695\n",
      "  batch 150 loss: 0.22054213166236877\n",
      "  batch 200 loss: 0.2030464567244053\n",
      "LOSS train 0.2030464567244053 valid 0.3106374584345354\n",
      "EPOCH 98:\n",
      "  batch 50 loss: 0.19080815374851226\n",
      "  batch 100 loss: 0.19688457153737546\n",
      "  batch 150 loss: 0.24131589338183404\n",
      "  batch 200 loss: 0.1928968086838722\n",
      "LOSS train 0.1928968086838722 valid 0.3169362861663103\n",
      "EPOCH 99:\n",
      "  batch 50 loss: 0.1892442988604307\n",
      "  batch 100 loss: 0.21263422816991806\n",
      "  batch 150 loss: 0.18367391653358936\n",
      "  batch 200 loss: 0.20812441721558572\n",
      "LOSS train 0.20812441721558572 valid 0.3097203030354447\n",
      "EPOCH 100:\n",
      "  batch 50 loss: 0.17884155511856079\n",
      "  batch 100 loss: 0.22266601614654064\n",
      "  batch 150 loss: 0.2076743672788143\n",
      "  batch 200 loss: 0.2075560761988163\n",
      "LOSS train 0.2075560761988163 valid 0.36132077127695084\n",
      "EPOCH 101:\n",
      "  batch 50 loss: 0.16826239183545114\n",
      "  batch 100 loss: 0.22749945223331453\n",
      "  batch 150 loss: 0.2087045979499817\n",
      "  batch 200 loss: 0.20425394169986247\n",
      "LOSS train 0.20425394169986247 valid 0.3367235033462445\n",
      "EPOCH 102:\n",
      "  batch 50 loss: 0.17986752294003963\n",
      "  batch 100 loss: 0.2019103565812111\n",
      "  batch 150 loss: 0.18646502248942853\n",
      "  batch 200 loss: 0.2230663911253214\n",
      "LOSS train 0.2230663911253214 valid 0.33797712106671596\n",
      "EPOCH 103:\n",
      "  batch 50 loss: 0.21681404799222947\n",
      "  batch 100 loss: 0.17189210161566734\n",
      "  batch 150 loss: 0.1759642482548952\n",
      "  batch 200 loss: 0.20293250866234303\n",
      "LOSS train 0.20293250866234303 valid 0.39649398490372634\n",
      "EPOCH 104:\n",
      "  batch 50 loss: 0.19569880351424218\n",
      "  batch 100 loss: 0.20599390983581542\n",
      "  batch 150 loss: 0.1952574422210455\n",
      "  batch 200 loss: 0.18384937718510627\n",
      "LOSS train 0.18384937718510627 valid 0.3246468022051785\n",
      "EPOCH 105:\n",
      "  batch 50 loss: 0.18263363242149352\n",
      "  batch 100 loss: 0.19687386974692345\n",
      "  batch 150 loss: 0.1839688801765442\n",
      "  batch 200 loss: 0.1913608580827713\n",
      "LOSS train 0.1913608580827713 valid 0.3777656714535422\n",
      "EPOCH 106:\n",
      "  batch 50 loss: 0.19076432198286056\n",
      "  batch 100 loss: 0.17697878286242485\n",
      "  batch 150 loss: 0.21535445377230644\n",
      "  batch 200 loss: 0.2093840976804495\n",
      "LOSS train 0.2093840976804495 valid 0.3367173130520516\n",
      "EPOCH 107:\n",
      "  batch 50 loss: 0.17508290141820906\n",
      "  batch 100 loss: 0.15267577841877938\n",
      "  batch 150 loss: 0.20015950314700603\n",
      "  batch 200 loss: 0.21341952055692673\n",
      "LOSS train 0.21341952055692673 valid 0.3377150086065133\n",
      "EPOCH 108:\n",
      "  batch 50 loss: 0.17148060828447342\n",
      "  batch 100 loss: 0.18427826523780821\n",
      "  batch 150 loss: 0.22126745641231538\n",
      "  batch 200 loss: 0.19141676753759385\n",
      "LOSS train 0.19141676753759385 valid 0.35343933457301724\n",
      "EPOCH 109:\n",
      "  batch 50 loss: 0.1708791770786047\n",
      "  batch 100 loss: 0.17531962357461453\n",
      "  batch 150 loss: 0.20531756222248077\n",
      "  batch 200 loss: 0.16007915578782558\n",
      "LOSS train 0.16007915578782558 valid 0.36437185522582793\n",
      "EPOCH 110:\n",
      "  batch 50 loss: 0.164957073405385\n",
      "  batch 100 loss: 0.17160516679286958\n",
      "  batch 150 loss: 0.224432145729661\n",
      "  batch 200 loss: 0.18162764132022857\n",
      "LOSS train 0.18162764132022857 valid 0.3257990458773242\n",
      "EPOCH 111:\n",
      "  batch 50 loss: 0.19928208172321318\n",
      "  batch 100 loss: 0.1708357538282871\n",
      "  batch 150 loss: 0.17066428050398827\n",
      "  batch 200 loss: 0.18161718010902406\n",
      "LOSS train 0.18161718010902406 valid 0.3682531073896421\n",
      "EPOCH 112:\n",
      "  batch 50 loss: 0.17347548976540567\n",
      "  batch 100 loss: 0.2111526709422469\n",
      "  batch 150 loss: 0.17976307779550552\n",
      "  batch 200 loss: 0.1789568952471018\n",
      "LOSS train 0.1789568952471018 valid 0.35446404500140083\n",
      "EPOCH 113:\n",
      "  batch 50 loss: 0.1724482361972332\n",
      "  batch 100 loss: 0.20528126187622547\n",
      "  batch 150 loss: 0.19229588635265826\n",
      "  batch 200 loss: 0.16733669385313987\n",
      "LOSS train 0.16733669385313987 valid 0.327746220657395\n",
      "EPOCH 114:\n",
      "  batch 50 loss: 0.1573925593495369\n",
      "  batch 100 loss: 0.18318669848144054\n",
      "  batch 150 loss: 0.21069544836878776\n",
      "  batch 200 loss: 0.1851678240299225\n",
      "LOSS train 0.1851678240299225 valid 0.35989038542740875\n",
      "EPOCH 115:\n",
      "  batch 50 loss: 0.18911542415618895\n",
      "  batch 100 loss: 0.16373029455542565\n",
      "  batch 150 loss: 0.1943409339338541\n",
      "  batch 200 loss: 0.17206423543393612\n",
      "LOSS train 0.17206423543393612 valid 0.3637889687799745\n",
      "EPOCH 116:\n",
      "  batch 50 loss: 0.172420055270195\n",
      "  batch 100 loss: 0.16983926616609096\n",
      "  batch 150 loss: 0.1528985660523176\n",
      "  batch 200 loss: 0.16423183798789978\n",
      "LOSS train 0.16423183798789978 valid 0.3537185883356465\n",
      "EPOCH 117:\n",
      "  batch 50 loss: 0.18920353457331657\n",
      "  batch 100 loss: 0.20439964570105076\n",
      "  batch 150 loss: 0.15873006977140902\n",
      "  batch 200 loss: 0.19520767264068126\n",
      "LOSS train 0.19520767264068126 valid 0.33862371411588454\n",
      "EPOCH 118:\n",
      "  batch 50 loss: 0.19629468314349652\n",
      "  batch 100 loss: 0.17684580199420452\n",
      "  batch 150 loss: 0.1623329472541809\n",
      "  batch 200 loss: 0.19108538426458835\n",
      "LOSS train 0.19108538426458835 valid 0.345189467486408\n",
      "EPOCH 119:\n",
      "  batch 50 loss: 0.15830097891390324\n",
      "  batch 100 loss: 0.15568886637687684\n",
      "  batch 150 loss: 0.17933117367327214\n",
      "  batch 200 loss: 0.18987716622650624\n",
      "LOSS train 0.18987716622650624 valid 0.37846189375138944\n",
      "EPOCH 120:\n",
      "  batch 50 loss: 0.14820374712347983\n",
      "  batch 100 loss: 0.204680949896574\n",
      "  batch 150 loss: 0.1356917568296194\n",
      "  batch 200 loss: 0.18584752574563027\n",
      "LOSS train 0.18584752574563027 valid 0.35700716776773334\n",
      "EPOCH 121:\n",
      "  batch 50 loss: 0.16350844077765941\n",
      "  batch 100 loss: 0.1864765378087759\n",
      "  batch 150 loss: 0.1503954316675663\n",
      "  batch 200 loss: 0.16986192822456359\n",
      "LOSS train 0.16986192822456359 valid 0.41398010589182377\n",
      "EPOCH 122:\n",
      "  batch 50 loss: 0.13741051957011222\n",
      "  batch 100 loss: 0.1715690530091524\n",
      "  batch 150 loss: 0.19921457067131995\n",
      "  batch 200 loss: 0.18046958565711976\n",
      "LOSS train 0.18046958565711976 valid 0.3980183190562659\n",
      "EPOCH 123:\n",
      "  batch 50 loss: 0.17036357060074805\n",
      "  batch 100 loss: 0.19597366496920585\n",
      "  batch 150 loss: 0.17638534113764762\n",
      "  batch 200 loss: 0.16608673423528672\n",
      "LOSS train 0.16608673423528672 valid 0.3847211895303594\n",
      "EPOCH 124:\n",
      "  batch 50 loss: 0.17314764946699143\n",
      "  batch 100 loss: 0.15573797084391117\n",
      "  batch 150 loss: 0.18516785718500614\n",
      "  batch 200 loss: 0.180208330899477\n",
      "LOSS train 0.180208330899477 valid 0.3473079458086027\n",
      "EPOCH 125:\n",
      "  batch 50 loss: 0.1583125089108944\n",
      "  batch 100 loss: 0.17147627286612988\n",
      "  batch 150 loss: 0.17304309792816638\n",
      "  batch 200 loss: 0.20938939966261386\n",
      "LOSS train 0.20938939966261386 valid 0.35039496556338334\n",
      "EPOCH 126:\n",
      "  batch 50 loss: 0.2056723180413246\n",
      "  batch 100 loss: 0.17613125808537006\n",
      "  batch 150 loss: 0.14250092439353465\n",
      "  batch 200 loss: 0.12373623229563237\n",
      "LOSS train 0.12373623229563237 valid 0.3650456949447592\n",
      "EPOCH 127:\n",
      "  batch 50 loss: 0.19722049854695797\n",
      "  batch 100 loss: 0.16164938561618328\n",
      "  batch 150 loss: 0.1760383491963148\n",
      "  batch 200 loss: 0.1720987846329808\n",
      "LOSS train 0.1720987846329808 valid 0.3642081320285797\n",
      "EPOCH 128:\n",
      "  batch 50 loss: 0.15541418749839067\n",
      "  batch 100 loss: 0.17788183435797691\n",
      "  batch 150 loss: 0.13823691848665476\n",
      "  batch 200 loss: 0.1894212231785059\n",
      "LOSS train 0.1894212231785059 valid 0.3436525969041718\n",
      "EPOCH 129:\n",
      "  batch 50 loss: 0.1497746119648218\n",
      "  batch 100 loss: 0.16708224147558212\n",
      "  batch 150 loss: 0.16733966484665871\n",
      "  batch 200 loss: 0.1628823286294937\n",
      "LOSS train 0.1628823286294937 valid 0.345171422801084\n",
      "EPOCH 130:\n",
      "  batch 50 loss: 0.1803672568500042\n",
      "  batch 100 loss: 0.15882192753255367\n",
      "  batch 150 loss: 0.16689292326569558\n",
      "  batch 200 loss: 0.15266456626355648\n",
      "LOSS train 0.15266456626355648 valid 0.3335159422001905\n",
      "EPOCH 131:\n",
      "  batch 50 loss: 0.16056221045553684\n",
      "  batch 100 loss: 0.14419356741011144\n",
      "  batch 150 loss: 0.1987701539695263\n",
      "  batch 200 loss: 0.14925221115350723\n",
      "LOSS train 0.14925221115350723 valid 0.3487446611333225\n",
      "EPOCH 132:\n",
      "  batch 50 loss: 0.133170561902225\n",
      "  batch 100 loss: 0.15847313076257705\n",
      "  batch 150 loss: 0.17682669177651406\n",
      "  batch 200 loss: 0.1899562803655863\n",
      "LOSS train 0.1899562803655863 valid 0.36157421374486554\n",
      "EPOCH 133:\n",
      "  batch 50 loss: 0.12221209585666656\n",
      "  batch 100 loss: 0.18004327602684497\n",
      "  batch 150 loss: 0.19855453334748746\n",
      "  batch 200 loss: 0.1226431430131197\n",
      "LOSS train 0.1226431430131197 valid 0.3363100662827492\n",
      "EPOCH 134:\n",
      "  batch 50 loss: 0.1506111190468073\n",
      "  batch 100 loss: 0.15357815220952034\n",
      "  batch 150 loss: 0.1471423039585352\n",
      "  batch 200 loss: 0.19154050841927528\n",
      "LOSS train 0.19154050841927528 valid 0.33154775316102636\n",
      "EPOCH 135:\n",
      "  batch 50 loss: 0.15971231278032064\n",
      "  batch 100 loss: 0.1876993938535452\n",
      "  batch 150 loss: 0.1527688905596733\n",
      "  batch 200 loss: 0.13422055073082448\n",
      "LOSS train 0.13422055073082448 valid 0.3633607890870836\n",
      "EPOCH 136:\n",
      "  batch 50 loss: 0.13041119813919066\n",
      "  batch 100 loss: 0.16533263705670834\n",
      "  batch 150 loss: 0.14416058890521527\n",
      "  batch 200 loss: 0.17257329925894738\n",
      "LOSS train 0.17257329925894738 valid 0.33036463438636726\n",
      "EPOCH 137:\n",
      "  batch 50 loss: 0.15529035188257695\n",
      "  batch 100 loss: 0.20750243946909905\n",
      "  batch 150 loss: 0.1466118523478508\n",
      "  batch 200 loss: 0.1505394273251295\n",
      "LOSS train 0.1505394273251295 valid 0.31948352398143876\n",
      "EPOCH 138:\n",
      "  batch 50 loss: 0.1735398130118847\n",
      "  batch 100 loss: 0.13718872986733913\n",
      "  batch 150 loss: 0.13378239400684833\n",
      "  batch 200 loss: 0.18694862864911557\n",
      "LOSS train 0.18694862864911557 valid 0.3428982121662961\n",
      "EPOCH 139:\n",
      "  batch 50 loss: 0.15370480686426163\n",
      "  batch 100 loss: 0.13136414177715777\n",
      "  batch 150 loss: 0.1766139456629753\n",
      "  batch 200 loss: 0.14943023927509785\n",
      "LOSS train 0.14943023927509785 valid 0.3059423201613956\n",
      "EPOCH 140:\n",
      "  batch 50 loss: 0.14700510144233703\n",
      "  batch 100 loss: 0.14497763946652412\n",
      "  batch 150 loss: 0.15606735199689864\n",
      "  batch 200 loss: 0.17343096390366555\n",
      "LOSS train 0.17343096390366555 valid 0.37927455656851333\n",
      "EPOCH 141:\n",
      "  batch 50 loss: 0.16405197970569133\n",
      "  batch 100 loss: 0.15487649962306022\n",
      "  batch 150 loss: 0.15731605604290963\n",
      "  batch 200 loss: 0.14391885798424484\n",
      "LOSS train 0.14391885798424484 valid 0.331219390914258\n",
      "EPOCH 142:\n",
      "  batch 50 loss: 0.13192230738699437\n",
      "  batch 100 loss: 0.18827417857944964\n",
      "  batch 150 loss: 0.15883810956031083\n",
      "  batch 200 loss: 0.14023556113243102\n",
      "LOSS train 0.14023556113243102 valid 0.33400503711567986\n",
      "EPOCH 143:\n",
      "  batch 50 loss: 0.15936473347246646\n",
      "  batch 100 loss: 0.13360208868980408\n",
      "  batch 150 loss: 0.17393527396023273\n",
      "  batch 200 loss: 0.14248390354216098\n",
      "LOSS train 0.14248390354216098 valid 0.3768612851595713\n",
      "EPOCH 144:\n",
      "  batch 50 loss: 0.1766330800950527\n",
      "  batch 100 loss: 0.16584694638848305\n",
      "  batch 150 loss: 0.14428980328142643\n",
      "  batch 200 loss: 0.1639967854321003\n",
      "LOSS train 0.1639967854321003 valid 0.3634609095752239\n",
      "EPOCH 145:\n",
      "  batch 50 loss: 0.15861434534192084\n",
      "  batch 100 loss: 0.13025947865098714\n",
      "  batch 150 loss: 0.15869035623967648\n",
      "  batch 200 loss: 0.15897976830601693\n",
      "LOSS train 0.15897976830601693 valid 0.3354023950588372\n",
      "EPOCH 146:\n",
      "  batch 50 loss: 0.12324257388710975\n",
      "  batch 100 loss: 0.16937096852809191\n",
      "  batch 150 loss: 0.14503795884549617\n",
      "  batch 200 loss: 0.14681357614696025\n",
      "LOSS train 0.14681357614696025 valid 0.31962366422845256\n",
      "EPOCH 147:\n",
      "  batch 50 loss: 0.16310590244829654\n",
      "  batch 100 loss: 0.12220664020627738\n",
      "  batch 150 loss: 0.18078658297657968\n",
      "  batch 200 loss: 0.14516162052750586\n",
      "LOSS train 0.14516162052750586 valid 0.38476169378393227\n",
      "EPOCH 148:\n",
      "  batch 50 loss: 0.15949749514460565\n",
      "  batch 100 loss: 0.1338722275197506\n",
      "  batch 150 loss: 0.17099004924297334\n",
      "  batch 200 loss: 0.16422463841736318\n",
      "LOSS train 0.16422463841736318 valid 0.3004030111349291\n",
      "EPOCH 149:\n",
      "  batch 50 loss: 0.16598116770386695\n",
      "  batch 100 loss: 0.12257784366607666\n",
      "  batch 150 loss: 0.1493985737487674\n",
      "  batch 200 loss: 0.13253182888031007\n",
      "LOSS train 0.13253182888031007 valid 0.35159866884350777\n",
      "EPOCH 150:\n",
      "  batch 50 loss: 0.16535882584750652\n",
      "  batch 100 loss: 0.16601860836148263\n",
      "  batch 150 loss: 0.13637649305164815\n",
      "  batch 200 loss: 0.1398791693896055\n",
      "LOSS train 0.1398791693896055 valid 0.3663997830202182\n",
      "EPOCH 151:\n",
      "  batch 50 loss: 0.12649274729192256\n",
      "  batch 100 loss: 0.13002395745366813\n",
      "  batch 150 loss: 0.15542581900954247\n",
      "  batch 200 loss: 0.15215116262435913\n",
      "LOSS train 0.15215116262435913 valid 0.38255368794004124\n",
      "EPOCH 152:\n",
      "  batch 50 loss: 0.1274722209572792\n",
      "  batch 100 loss: 0.13920219369232656\n",
      "  batch 150 loss: 0.15614686496555805\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 39\u001B[0m\n\u001B[1;32m     36\u001B[0m last_loss \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m     37\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 39\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrn_loader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mx_batch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    672\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    673\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 674\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    676\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Projects/splid-comp/trial_models/dataset.py:75\u001B[0m, in \u001B[0;36mSPLID.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m     74\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mObjectID\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mids[idx]]\n\u001B[0;32m---> 75\u001B[0m     series \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcol_transformer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m[:, :\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns)]\n\u001B[1;32m     76\u001B[0m     series \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mnan_to_num(series)\n\u001B[1;32m     77\u001B[0m     labels \u001B[38;5;241m=\u001B[39m data[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEW\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNS\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mvalues\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:157\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 157\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    159\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    160\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    161\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    162\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    163\u001B[0m         )\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:827\u001B[0m, in \u001B[0;36mColumnTransformer.transform\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    823\u001B[0m     \u001B[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001B[39;00m\n\u001B[1;32m    824\u001B[0m     \u001B[38;5;66;03m# check that n_features_in_ is consistent\u001B[39;00m\n\u001B[1;32m    825\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_n_features(X, reset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m--> 827\u001B[0m Xs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    828\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    829\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    830\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_transform_one\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    831\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfitted\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    832\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumn_as_strings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_dataframe_and_transform_dataframe\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    833\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    834\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_output(Xs)\n\u001B[1;32m    836\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m Xs:\n\u001B[1;32m    837\u001B[0m     \u001B[38;5;66;03m# All transformers are None\u001B[39;00m\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:681\u001B[0m, in \u001B[0;36mColumnTransformer._fit_transform\u001B[0;34m(self, X, y, func, fitted, column_as_strings)\u001B[0m\n\u001B[1;32m    675\u001B[0m transformers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter(\n\u001B[1;32m    677\u001B[0m         fitted\u001B[38;5;241m=\u001B[39mfitted, replace_strings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, column_as_strings\u001B[38;5;241m=\u001B[39mcolumn_as_strings\n\u001B[1;32m    678\u001B[0m     )\n\u001B[1;32m    679\u001B[0m )\n\u001B[1;32m    680\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 681\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    682\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    683\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtransformer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrans\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfitted\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrans\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    684\u001B[0m \u001B[43m            \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_safe_indexing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    685\u001B[0m \u001B[43m            \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    686\u001B[0m \u001B[43m            \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    687\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmessage_clsname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mColumnTransformer\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    688\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_log_message\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtransformers\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    689\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    690\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrans\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtransformers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    691\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    692\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    693\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected 2D array, got 1D array instead\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e):\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     60\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     61\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     62\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     64\u001B[0m )\n\u001B[0;32m---> 65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/joblib/parallel.py:1863\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1861\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[1;32m   1862\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 1863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n\u001B[1;32m   1865\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[1;32m   1866\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[1;32m   1867\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[1;32m   1868\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[1;32m   1869\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[1;32m   1870\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/joblib/parallel.py:1789\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1786\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1788\u001B[0m \u001B[38;5;66;03m# Sequentially call the tasks and yield the results.\u001B[39;00m\n\u001B[0;32m-> 1789\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m   1790\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_dispatched_batches\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\n\u001B[1;32m   1791\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_dispatched_tasks\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:61\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# Capture the thread-local scikit-learn configuration at the time\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# Parallel.__call__ is issued since the tasks can be dispatched\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;66;03m# in a different thread depending on the backend and on the value of\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;66;03m# pre_dispatch and n_jobs.\u001B[39;00m\n\u001B[1;32m     60\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m---> 61\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\n\u001B[1;32m     62\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[43m_with_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdelayed_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdelayed_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\n\u001B[1;32m     64\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(iterable_with_config)\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:684\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    675\u001B[0m transformers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter(\n\u001B[1;32m    677\u001B[0m         fitted\u001B[38;5;241m=\u001B[39mfitted, replace_strings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, column_as_strings\u001B[38;5;241m=\u001B[39mcolumn_as_strings\n\u001B[1;32m    678\u001B[0m     )\n\u001B[1;32m    679\u001B[0m )\n\u001B[1;32m    680\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    681\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Parallel(n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs)(\n\u001B[1;32m    682\u001B[0m         delayed(func)(\n\u001B[1;32m    683\u001B[0m             transformer\u001B[38;5;241m=\u001B[39mclone(trans) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fitted \u001B[38;5;28;01melse\u001B[39;00m trans,\n\u001B[0;32m--> 684\u001B[0m             X\u001B[38;5;241m=\u001B[39m\u001B[43m_safe_indexing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    685\u001B[0m             y\u001B[38;5;241m=\u001B[39my,\n\u001B[1;32m    686\u001B[0m             weight\u001B[38;5;241m=\u001B[39mweight,\n\u001B[1;32m    687\u001B[0m             message_clsname\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnTransformer\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    688\u001B[0m             message\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_log_message(name, idx, \u001B[38;5;28mlen\u001B[39m(transformers)),\n\u001B[1;32m    689\u001B[0m         )\n\u001B[1;32m    690\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m idx, (name, trans, column, weight) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(transformers, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    691\u001B[0m     )\n\u001B[1;32m    692\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    693\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected 2D array, got 1D array instead\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e):\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/utils/__init__.py:353\u001B[0m, in \u001B[0;36m_safe_indexing\u001B[0;34m(X, indices, axis)\u001B[0m\n\u001B[1;32m    347\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    348\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSpecifying the columns using strings is only supported for \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    349\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpandas DataFrames\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    350\u001B[0m     )\n\u001B[1;32m    352\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(X, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miloc\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 353\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_pandas_indexing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(X, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    355\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _array_indexing(X, indices, indices_dtype, axis\u001B[38;5;241m=\u001B[39maxis)\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/utils/__init__.py:199\u001B[0m, in \u001B[0;36m_pandas_indexing\u001B[0;34m(X, key, key_dtype, axis)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    197\u001B[0m     \u001B[38;5;66;03m# check whether we should index with loc or iloc\u001B[39;00m\n\u001B[1;32m    198\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39miloc \u001B[38;5;28;01mif\u001B[39;00m key_dtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mint\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m X\u001B[38;5;241m.\u001B[39mloc\n\u001B[0;32m--> 199\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mindexer\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;28;01melse\u001B[39;00m indexer[key]\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/pandas/core/indexing.py:1067\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1065\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_scalar_access(key):\n\u001B[1;32m   1066\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39m_get_value(\u001B[38;5;241m*\u001B[39mkey, takeable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_takeable)\n\u001B[0;32m-> 1067\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_tuple\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1068\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1069\u001B[0m     \u001B[38;5;66;03m# we by definition only have the 0th axis\u001B[39;00m\n\u001B[1;32m   1070\u001B[0m     axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxis \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/pandas/core/indexing.py:1256\u001B[0m, in \u001B[0;36m_LocIndexer._getitem_tuple\u001B[0;34m(self, tup)\u001B[0m\n\u001B[1;32m   1253\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_multi_take_opportunity(tup):\n\u001B[1;32m   1254\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_multi_take(tup)\n\u001B[0;32m-> 1256\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_tuple_same_dim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtup\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/pandas/core/indexing.py:924\u001B[0m, in \u001B[0;36m_LocationIndexer._getitem_tuple_same_dim\u001B[0;34m(self, tup)\u001B[0m\n\u001B[1;32m    921\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m com\u001B[38;5;241m.\u001B[39mis_null_slice(key):\n\u001B[1;32m    922\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m--> 924\u001B[0m retval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mretval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    925\u001B[0m \u001B[38;5;66;03m# We should never have retval.ndim < self.ndim, as that should\u001B[39;00m\n\u001B[1;32m    926\u001B[0m \u001B[38;5;66;03m#  be handled by the _getitem_lowerdim call above.\u001B[39;00m\n\u001B[1;32m    927\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m retval\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mndim\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/pandas/core/indexing.py:1301\u001B[0m, in \u001B[0;36m_LocIndexer._getitem_axis\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1298\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(key, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mndim\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m key\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   1299\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot index with multidimensional key\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1301\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_iterable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1303\u001B[0m \u001B[38;5;66;03m# nested tuple slicing\u001B[39;00m\n\u001B[1;32m   1304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_nested_tuple(key, labels):\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/pandas/core/indexing.py:1240\u001B[0m, in \u001B[0;36m_LocIndexer._getitem_iterable\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1238\u001B[0m \u001B[38;5;66;03m# A collection of keys\u001B[39;00m\n\u001B[1;32m   1239\u001B[0m keyarr, indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_listlike_indexer(key, axis)\n\u001B[0;32m-> 1240\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reindex_with_indexers\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1241\u001B[0m \u001B[43m    \u001B[49m\u001B[43m{\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m]\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_dups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m   1242\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/pandas/core/generic.py:5355\u001B[0m, in \u001B[0;36mNDFrame._reindex_with_indexers\u001B[0;34m(self, reindexers, fill_value, copy, allow_dups)\u001B[0m\n\u001B[1;32m   5352\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m ensure_platform_int(indexer)\n\u001B[1;32m   5354\u001B[0m \u001B[38;5;66;03m# TODO: speed up on homogeneous DataFrame objects (see _reindex_multi)\u001B[39;00m\n\u001B[0;32m-> 5355\u001B[0m new_data \u001B[38;5;241m=\u001B[39m \u001B[43mnew_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreindex_indexer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   5356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5358\u001B[0m \u001B[43m    \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbaxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5359\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5360\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_dups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_dups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5361\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5362\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5363\u001B[0m \u001B[38;5;66;03m# If we've made a copy once, no need to make another one\u001B[39;00m\n\u001B[1;32m   5364\u001B[0m copy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:743\u001B[0m, in \u001B[0;36mBaseBlockManager.reindex_indexer\u001B[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001B[0m\n\u001B[1;32m    740\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRequested axis not found in manager\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 743\u001B[0m     new_blocks, new_refs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_slice_take_blocks_ax0\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    744\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    745\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    746\u001B[0m \u001B[43m        \u001B[49m\u001B[43monly_slice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43monly_slice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    747\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_na_proxy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_na_proxy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    748\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    749\u001B[0m     parent \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m com\u001B[38;5;241m.\u001B[39mall_none(\u001B[38;5;241m*\u001B[39mnew_refs) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m    750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:912\u001B[0m, in \u001B[0;36mBaseBlockManager._slice_take_blocks_ax0\u001B[0;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy)\u001B[0m\n\u001B[1;32m    910\u001B[0m         refs\u001B[38;5;241m.\u001B[39mappend(weakref\u001B[38;5;241m.\u001B[39mref(blk))\n\u001B[1;32m    911\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 912\u001B[0m     nb \u001B[38;5;241m=\u001B[39m \u001B[43mblk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtake_nd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtaker\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_mgr_locs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmgr_locs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    913\u001B[0m     blocks\u001B[38;5;241m.\u001B[39mappend(nb)\n\u001B[1;32m    914\u001B[0m     refs\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/pandas/core/internals/blocks.py:874\u001B[0m, in \u001B[0;36mBlock.take_nd\u001B[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001B[0m\n\u001B[1;32m    871\u001B[0m values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalues\n\u001B[1;32m    873\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fill_value \u001B[38;5;129;01mis\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mno_default:\n\u001B[0;32m--> 874\u001B[0m     fill_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfill_value\u001B[49m\n\u001B[1;32m    875\u001B[0m     allow_fill \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    876\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/pandas/_libs/properties.pyx:36\u001B[0m, in \u001B[0;36mpandas._libs.properties.CachedProperty.__get__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/pandas/core/internals/blocks.py:204\u001B[0m, in \u001B[0;36mBlock.fill_value\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[1;32m    201\u001B[0m \u001B[38;5;129m@cache_readonly\u001B[39m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfill_value\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;66;03m# Used in reindex_indexer\u001B[39;00m\n\u001B[0;32m--> 204\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mna_value_for_dtype\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompat\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/pandas/core/dtypes/missing.py:676\u001B[0m, in \u001B[0;36mna_value_for_dtype\u001B[0;34m(dtype, compat)\u001B[0m\n\u001B[1;32m    674\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m needs_i8_conversion(dtype):\n\u001B[1;32m    675\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m dtype\u001B[38;5;241m.\u001B[39mtype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNaT\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mns\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 676\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[43mis_float_dtype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    677\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mnan\n\u001B[1;32m    678\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_integer_dtype(dtype):\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/pandas/core/dtypes/common.py:1246\u001B[0m, in \u001B[0;36mis_float_dtype\u001B[0;34m(arr_or_dtype)\u001B[0m\n\u001B[1;32m   1205\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1206\u001B[0m \u001B[38;5;124;03m    Check whether the provided array or dtype is of a numeric dtype.\u001B[39;00m\n\u001B[1;32m   1207\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1239\u001B[0m \u001B[38;5;124;03m    False\u001B[39;00m\n\u001B[1;32m   1240\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   1241\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _is_dtype_type(\n\u001B[1;32m   1242\u001B[0m         arr_or_dtype, classes_and_not_datetimelike(np\u001B[38;5;241m.\u001B[39mnumber, np\u001B[38;5;241m.\u001B[39mbool_)\n\u001B[1;32m   1243\u001B[0m     )\n\u001B[0;32m-> 1246\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_float_dtype\u001B[39m(arr_or_dtype) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[1;32m   1247\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1248\u001B[0m \u001B[38;5;124;03m    Check whether the provided array or dtype is of a float dtype.\u001B[39;00m\n\u001B[1;32m   1249\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1273\u001B[0m \u001B[38;5;124;03m    True\u001B[39;00m\n\u001B[1;32m   1274\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   1275\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _is_dtype_type(arr_or_dtype, classes(np\u001B[38;5;241m.\u001B[39mfloating))\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models.prectime import PrecTime\n",
    "from loss import WeightedCELoss\n",
    "\n",
    "cols = ['Inclination (deg)', 'Longitude (deg)', 'Eccentricity', 'Semimajor Axis (m)', 'RAAN (deg)', 'Argument of Periapsis (deg)', 'Vz (m/s)']\n",
    "classes = ['SS-CK', 'SS-EK', 'SS-HK', 'SS-NK', 'IK-CK', 'IK-EK', 'IK-HK', 'ID-NK', 'AD-NK']\n",
    "class_weights = torch.tensor([0.1, 0.1, 0.1, 0.1, 2, 2, 2, 2, 2]).cuda()\n",
    "trn_data = SPLID(train_datalist, ground_truth, cols, classes=classes)\n",
    "tst_data = SPLID(test_datalist, ground_truth, cols, classes=classes)\n",
    "\n",
    "trn_loader = data.DataLoader(trn_data, shuffle=True, batch_size=8)\n",
    "tst_loader = data.DataLoader(tst_data, shuffle=True, batch_size=8)\n",
    "\n",
    "lr = 1e-5\n",
    "n_epochs = 500\n",
    "best_tst_loss = 1_000_000.\n",
    "\n",
    "model = PrecTime(len(classes), n_win=24, l_win=92, c_in=len(cols), c_conv=128)\n",
    "model = model.cuda()\n",
    "criterion = WeightedCELoss(class_weights=class_weights)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "print('Start model training')\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/splid_trainer_{}'.format(timestamp))\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    print('EPOCH {}:'.format(epoch))\n",
    "    \n",
    "    running_loss = np.zeros(3)\n",
    "    last_loss = np.zeros(3)\n",
    "    model.train(True)\n",
    "    \n",
    "    for i, (x_batch, y_batch) in enumerate(trn_loader):\n",
    "        \n",
    "        x_batch = x_batch.cuda()\n",
    "        y_batch = y_batch.cuda()\n",
    "        # sched.step()\n",
    "        opt.zero_grad()\n",
    "        fine_out, coarse_out = model(x_batch)\n",
    "        tot_loss, fine_loss, coarse_loss = criterion(fine_out, coarse_out, y_batch)\n",
    "        tot_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += np.array([tot_loss.cpu().item(), fine_loss.cpu().item(), coarse_loss.cpu().item()])\n",
    "        if i % 50 == 49:\n",
    "            last_loss = running_loss / 50 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss[0]))\n",
    "            tb_x = epoch * len(trn_loader) + i + 1\n",
    "            writer.add_scalar('TotLoss/train', last_loss[0], tb_x)\n",
    "            writer.add_scalar('FineLoss/train', last_loss[1], tb_x)\n",
    "            writer.add_scalar('CoarseLoss/train', last_loss[2], tb_x)\n",
    "            running_loss = np.zeros(3)\n",
    "    \n",
    "    running_tst_loss = np.zeros(3)\n",
    "    model.eval()\n",
    "    \n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, (x_batch, y_batch) in enumerate(tst_loader):\n",
    "            x_batch = x_batch.cuda()\n",
    "            y_batch = y_batch.cuda()\n",
    "            fine_out, coarse_out = model(x_batch)\n",
    "            tot_tst_loss, fine_tst_loss, coarse_tst_loss = criterion(fine_out, coarse_out, y_batch)\n",
    "            running_tst_loss += np.array([tot_tst_loss.cpu(), fine_tst_loss.cpu(), coarse_tst_loss.cpu()])\n",
    "    \n",
    "    avg_tst_loss = running_tst_loss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(last_loss[0], avg_tst_loss[0]))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Total Loss',\n",
    "                    { 'Training' : last_loss[0], 'Validation' : avg_tst_loss[0] },\n",
    "                    epoch)\n",
    "    writer.add_scalars('Training vs. Validation Fine Loss',\n",
    "                    { 'Training' : last_loss[1], 'Validation' : avg_tst_loss[1] },\n",
    "                    epoch)\n",
    "    writer.add_scalars('Training vs. Validation Coarse Loss',\n",
    "                    { 'Training' : last_loss[2], 'Validation' : avg_tst_loss[2] },\n",
    "                    epoch)\n",
    "    writer.flush()\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_tst_loss[0] < best_tst_loss:\n",
    "        best_tst_loss = avg_tst_loss[0]\n",
    "        model_path = 'model_{}.pth'.format(timestamp)\n",
    "        torch.save(model.state_dict(), 'saved_models/' + model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x779d68f89890>]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGgCAYAAAAKKQXsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABZVklEQVR4nO3dd3hUVfoH8O+kTRJIIaRDKi10QgtVUJAistiRRQWxy66rro1dC64Fwd+6lkVwXRewYVkFyypIB6X3DgmEEBJCIKSXSWbm/P6YmZuZ1Ely79wp38/z5HnuzNzMvMlA7jvnvOc9GiGEABEREZGDeKkdABEREXkWJh9ERETkUEw+iIiIyKGYfBAREZFDMfkgIiIih2LyQURERA7F5IOIiIgciskHERERORSTDyIiInIoJh9ERETkUC1OPrZu3YqpU6ciNjYWGo0Gq1evlh6rqanBs88+i759+6Jdu3aIjY3FPffcg9zcXDljJiIiIhfm09JvKC8vR//+/TFnzhzccsstNo9VVFRg//79eOGFF9C/f38UFhbiT3/6E373u99h7969dj2/0WhEbm4ugoKCoNFoWhoeERERqUAIgdLSUsTGxsLLq5mxDdEGAMSqVauaPGf37t0CgMjKyrLrObOzswUAfvGLX/ziF7/45YJf2dnZzV7rWzzy0VLFxcXQaDQIDQ1t8HGdTgedTifdFuZNdrOzsxEcHKx0eERERCSDkpISxMXFISgoqNlzFU0+qqqq8Oyzz2LGjBmNJhILFizAyy+/XO/+4OBgJh9EREQuxp6SCcVWu9TU1OCOO+6AEAJLlixp9Lx58+ahuLhY+srOzlYqJCIiInICiox8WBKPrKwsbNy4sckRDK1WC61Wq0QYRERE5IRkTz4siUd6ejo2bdqEjh07yv0SRERE5MJanHyUlZUhIyNDup2ZmYmDBw8iLCwMMTExuO2227B//378+OOPMBgMyMvLAwCEhYXBz89PvsiJiIjIJWmEZXmJnTZv3oxrr7223v2zZs3C/PnzkZSU1OD3bdq0CWPHjm32+UtKShASEoLi4mIWnBIREbmIlly/WzzyMXbsWDSVr7QwlyEiIiIPw71diIiIyKGYfBAREZFDMfkgIiIih2LyQURERA7F5IOIiIgciskHERERORSTDyIiIg8hhMC/tp7Bgp9PqBqHorvaEhERkfPYcvoyXv/pJADgrrQExIUFqhIHRz6IiIg8xMHsIgDAoIQOiA0NUC0OJh9EREQe4EJhBd5enw4AmNwnGt5eGtViYfJBRETkAd4xJx4A0DNG3b3TmHwQERF5gKO5JdLxoIQOKkbC5IOIiMjtfbUnGycumpKPNY+Phr+vt6rxMPkgIiJyc898c1g6Tg5vr2IkJkw+iIiI3FhWQbl0/PCYLvDzUf/Sr34EREREpIiKaj3GvLkZAKD18cJzk1PUDciMyQcREZGbOnGxVDoe3ytKxUhsMfkgIiJyQ1fKdLh1yXbp9jwnGfUAmHwQERG5pZ+PXJSOH7wmGZ07qNNKvSFMPoiIiNzM0ZxivPDdMen27/rHqhhNfUw+iIiI3MySzWek43dnpKJPpxAVo6mPyQcREZEbeXdDOv5nNeUyLClMxWgaxuSDiIjITezLuoq31p2Wbn88Zygig/1VjKhhTD6IiIjcwJELxbh1yQ7pdu/YYFzTPULFiBrno3YARERE1Hp6gxHf7s+xaaHu5+OFT+9LUzGqpjH5ICIiclEHzhfioU/2Ib9UZ3P/P2ekokM7P5Wiah6TDyIiIhdiNAp8uTcbS7ecQVZBRb3Hv354OIYkOl+RqTUmH0RERC6gqsaAHw9fxFNfH2rw8UA/b/z02GgkhrdzcGQtx+SDiIjIyeWXVGHKe7/icp3pFYtnJvXA7BGJCPRzjcu6a0RJRETkofJLqzD8jY0wGIXN/TcNiMW01E4YntwR/r7eKkXXOkw+iIiInFT21QqMXrRJup0c3g7//P1A9IoNVjGqtmPyQURE5ISq9Ubcu3yPdHvRbf1wx+A4FSOSD5uMEREROaEV288hI78MAPC3ab3dJvEAmHwQERE5ndKqGrz20wkAQNfI9rgrLUHliOTF5IOIiMjJvPzDcQCAr7cGXz44DF5eGpUjkleLk4+tW7di6tSpiI2NhUajwerVq20e//bbbzFhwgR07NgRGo0GBw8elClUIiIi96c3GPHLsTwAwJ/GdUPH9lqVI5Jfi5OP8vJy9O/fH4sXL2708VGjRmHhwoVtDo6IiMjT/O3H4yip0iMkwBePjO2qdjiKaPFql8mTJ2Py5MmNPn733XcDAM6dO9fqoIiIiDxRZbUBn+86DwC4Y3BneLvZdIsFaz6IiIicxIHsQuiNAr7eGvzlhp5qh6MY1ft86HQ66HS17WJLSkpUjIaIiEgdBqPAk1+a9m2Z1CcGGo17jnoATjDysWDBAoSEhEhfcXHus46ZiIjIXvvPFyKvpAoAcMvATipHoyzVk4958+ahuLhY+srOzlY7JCIiIoc7lF0EABgQF4pre0SqG4zCVJ920Wq10GrdbxkRERGRvYxGgXc2pAMAxvd078QDaEXyUVZWhoyMDOl2ZmYmDh48iLCwMMTHx+Pq1as4f/48cnNzAQCnTp0CAERHRyM6OlqmsImIiNzHT0cvorRKDwAYlBCmcjTKa/G0y969e5GamorU1FQAwJNPPonU1FS8+OKLAIDvv/8eqampmDJlCgDgzjvvRGpqKpYuXSpj2ERERO5jd+ZVAEBKdBCGJbt/8tHikY+xY8dCCNHo47Nnz8bs2bPbEhMREZFHOZ5rWun58Jgubr3KxUL1glMiIiJPdu5KOfZmFQIAescGqxyNYzD5ICIiUtHf150GAHQI9EVSeDuVo3EMJh9EREQqOppTDAB44vru8PH2jMuyZ/yURERETqiqxoBzBeUAgEl9PGdFKJMPIiIilaw+kAMhTFMuEe09p+cVkw8iIiIVCCHwlrneo2/nUI9Y5WLB5IOIiEgFV8qqkV9q2lj1hSnuu4NtQ5h8EBERqSA9vxQAkNAxEN2iglSOxrGYfBAREalg9YEcAEC3SM9KPAAmH0RERA6XU1SJr/ZeAAD0imHyQURERAo7cqFYOr5rWIKKkaiDyQcREZGDZZjrPW5O7YTIYH+Vo3E8Jh9EREQOts+8l0vXyPYqR6IOJh9EREQO9FvGFWw6dRkAkw8iIiJygC2nL0vHI7uGqxiJeph8EBEROdDpS6Z6j+en9ER7rY/K0aiDyQcREZGDCCGw82wBAKBf51B1g1ERkw8iIiIHmf/9MVTVGAEA3Ty03gNg8kFEROQwm63qPTq081MxEnUx+SAiInIAg1Egr7gKAPC/x0apHI26mHwQERE5wM6zBdDpjdD6eCElOljtcFTF5IOIiEhhQgjM/PcuAEBksBbeXhqVI1IXkw8iIiKFXS7VScd3DolXMRLnwOSDiIhIYSfyTL09QgN98ejYLipHoz4mH0RERAoq0+kx6z+7AQAju4RDo/HsKReAyQcREZGifjmWJx337RyiYiTOg8kHERGRgk6Z26kDwO/TWO8BMPkgIiJSzPmCCnyw5SwA4OXf9Uawv6/KETkHJh9EREQKefOXU9JxtyjPbadeF5MPIiIiBQghcDy3WLo9JDFMxWicC5MPIiIiBbz+0wmcuVwOANjw5zHw9eYl14K/CSIiIpkZjQIfbsuUbieEBaoYjfNh8kFERCSzZ745LB2/eVs/+HDUwwZ/G0RERDLKyC/Ff/ddkG7fPjhOxWicE5MPIiIimRRVVGP8W1ul2/On9lIxGufV4uRj69atmDp1KmJjY6HRaLB69Wqbx4UQePHFFxETE4OAgACMHz8e6enpcsVLRETklAxGgb+sOiLdntArCrNHJqkYkfNqcfJRXl6O/v37Y/HixQ0+vmjRIrz77rtYunQpdu3ahXbt2mHixImoqqpqc7BERETOauGak/jpSG0r9bnXdlUxGufm09JvmDx5MiZPntzgY0IIvP3223j++ecxbdo0AMDHH3+MqKgorF69GnfeeWfboiUiInJCz/z3EL7aW1vn8el9aegfF6peQE5O1pqPzMxM5OXlYfz48dJ9ISEhSEtLw44dO+R8KSIiItXlFFViyGvrbRKPOSOTMLJrRxWjcn4tHvloSl6eabgpKirK5v6oqCjpsbp0Oh10Op10u6SkRM6QiIiIZFdQpsOr/zuBVQdybO6fPSIRL7LItFmyJh+tsWDBArz88stqh0FERGSXr/Zk2/TxsPjn71NxY79YFSJyPbJOu0RHRwMALl26ZHP/pUuXpMfqmjdvHoqLi6Wv7OxsOUMiIiKSxfHcEqS9vr5e4tE1sj32/HU8E48WkHXkIykpCdHR0diwYQMGDBgAwDSNsmvXLjzyyCMNfo9Wq4VWq5UzDCIiItkIIfCP9el4d4Nt24geUUF49eY+3DCuFVqcfJSVlSEjI0O6nZmZiYMHDyIsLAzx8fF4/PHH8eqrr6Jbt25ISkrCCy+8gNjYWNx0001yxk1ERKS4ymoD/vTFAfxyvHZEv1NoAD69Pw1J4e1UjMy1tTj52Lt3L6699lrp9pNPPgkAmDVrFpYvX45nnnkG5eXlePDBB1FUVIRRo0ZhzZo18Pf3ly9qIiIiB3jiy4M2icfnD6RhRJdwFSNyDxohhFA7CGslJSUICQlBcXExgoOD1Q6HiIg8kBAC963Yi40n8wEA13SPwLLZQ+DtpVE5MufVkuu36qtdiIiInM2y385JicedQ+Lwxq39VI7IvXBjOSIiIitZBeV49X/HAQBDE8Pw2s19VY7I/TD5ICIiMrtcqsPtS3fAaC5I+HDWYE61KIDJBxERkdniTRnILzV13f7mkREICfBVOSL3xOSDiIgIwIHzhVi+/RwA4PkpPTEooYO6AbkxJh9EREQA5n9/DAAQE+KPe4YnqhuMm2PyQUREHm/HmQIculAMAPjg7kHw8+HlUUn87RIRkUczGAX+9MUBAMDA+FD06xyqbkAegMkHERF5tCM5xVKR6UL283AIJh9EROSxagxGzP1sPwBgcp9odIsKUjkiz8Dkg4iIPNa29MvIKaoEANwxJE7laDwHkw8iIvJY68ybxo3tEYFre0SqHI3nYPJBREQe6fCFIqzcnQ0AuDm1k8rReBYmH0RE5JG+3Z8jHY/rGaViJJ6HyQcREXmkPeeuAgDeuXMA2mu5ybsjMfkgIiKPs3L3eRzLLQEADIxnG3VHY/JBREQeZ4V5D5dBCR0QFxaobjAeiMkHERF5lDKdHqculQIAlswcqHI0nonJBxEReZQFP52AEECn0ABEBvurHY5HYvJBREQeo7LagK/2mpbXXt+LK1zUwuSDiIg8xvGLxagxCADAX6f0VDkaz8Xkg4iIPMbHO7IAAON7RsLXm5dAtfA3T0REHiEjvwzfHcwFAKRyea2qmHwQEZFH2Hm2QDq+d2SieoEQkw8iIvIMRy4UAwAeHdsFgX7saKomJh9EROT2jueW4EvzKpe+nUJUjoaYfBARkdv7el+2dDyiS7iKkRDA5IOIiDzAsRzTPi6v39wXIYG+KkdDTD6IiOrIL63C5VKd2mGQTArKdNht3sF2UAJXuTgDJh9ERFZKq2ow9LUNmPzONtQYjGqHQzJ47tsjAIAAX28kR7RTORoCmHwQEdl47X8nAABXynQo1+lVjobaSgiB3ZmmUY8Hr0lmYzEnwXeBiMisTKfHF3tqCxOrOfLh8vJKqlBcWQNvLw0eGdtF7XDIjMkHERGAGoMRfV5aW+c+oVI0JJfnVx0FACSEBcLf11vlaMiCyQcRebyCMh26/fXnevfrOfLh0oxGgQ0n8wEA/Tqzt4czYfJBRB6txmDE418ebPQxcl3p+WXS8cLb+qkYCdXF5IOIPNq7G9KxLf2KdHtKvxhEBGkBANV6Tru4sr//cgoAMLJrR2h9OOXiTBRJPkpLS/H4448jISEBAQEBGDFiBPbs2aPESxERtdqpvFK8tzHD5r4XpvSCn3lFhN7IkQ9XVVCmwy/HLwEABieEqRwN1aVI8nH//fdj3bp1+OSTT3DkyBFMmDAB48ePR05OjhIvR0TUKrcv3W5ze+UDwxAd4g9fbw0ATru4sj3mpmIAd7B1RrInH5WVlfjmm2+waNEiXHPNNejatSvmz5+Prl27YsmSJXK/HBFRq2w+lY+Sqto+HrNHJGJYsukTsqUXBKddXNcPhy4CAO4aFo/QQD+Vo6G6ZN9TWK/Xw2AwwN/f3+b+gIAA/Prrr/XO1+l00Olq2xiXlJTIHRIRkY380irMXmY7Ffz8lJ7QaEwjHj7m5IMjH67pWG4x/nfElHwMTeqocjTUENlHPoKCgjB8+HC88soryM3NhcFgwKeffoodO3bg4sWL9c5fsGABQkJCpK+4uDi5QyIisrHr7FWb2788cY2UcACAn3nahTUfrmnt0TzpeEy3CBUjocYoUvPxySefQAiBTp06QavV4t1338WMGTPg5VX/5ebNm4fi4mLpKzs7u4FnJCKSx55zV/HHlQek26O7haN7VJDNOZx2cW2WTeTmT+3FHWydlOzTLgDQpUsXbNmyBeXl5SgpKUFMTAymT5+O5OTkeudqtVpotVolwiAiqmfb6cs2t9+fObDeOb6cdnFZ72/OwE7zyNaobuEqR0ONUbTPR7t27RATE4PCwkKsXbsW06ZNU/LliIiadOZyGd61Wlo7e0QigvzrfzL24WoXl/XBlrMAgEA/b3SJaK9yNNQYRUY+1q5dCyEEevTogYyMDDz99NNISUnBvffeq8TLERHZxdJ0yuLPE7o3eJ7U54N7u7iU4ooaFFfWAAC+fXSEVEBMzkeRkY/i4mLMnTsXKSkpuOeeezBq1CisXbsWvr6ceyMi9Ww07/MBAJ/fn9bgqAdgVfPBkQ+XIYTA6EUbAQDttT5IiQ5WOSJqiiIjH3fccQfuuOMOJZ6aiKhVPt2Zhaqa2mQiNb5Do+f6+rDmw9VkFVRIfVum9o9VORpqDvd2ISKP8Pzqo9LxlqfHIsCv8b0+fL1Y8+FqvthTu1Ly+Sk9VYyE7MHkg4jcXrlOb3M7rkNgk+e39zcNCucV65o8j5zD0ZxiLN1yBoCpo2k7rSKD+iQjJh9E5NaEEHj4033S7cfGdYOXV9OFiMOSTV0xf8u40uR55BysdyW+aUAnFSMhezH5ICK3lpFfJl2cvL00ePL6hle4WEuOaAcAKCjnyIezO5hdhIVrTgIAHh3bBYMTuYOtK2DyQURu7UJhpXQ8IC7Uru/R+pjqQXQ1rPlwdh9uOysdj+sZpWIk1BJMPojIbRmNAv/df0G6/eykFLu+T2te7aJjwalT++5gDv532LRn2IguHTEwPlTdgMhuTD6IyG2tO3FJujjdMzwBQ5PsG5L387Hs7WKEEGw05oyEEHjlxxPS7X/PGsymYi6EyQcRua2952p3r+3cIcDu77OMfACATs/RD2cjhMCEf2zFlTJTTc7DY7og0I8rXFwJkw8icltXyqql49Et2FrdUvMBsMupM1qy5QzS88sAAH06BeO5yfZNp5HzYPJBRG5pX9ZVrDqQAwB45aY+6Bljf7ttX28NLCP4LDp1LtvSL2PRmto9euZP7a1iNNRaTD6IyC3939rT0nG/TiEt+l6NRgNfL7ZYdzY/H7mIuz/aLd3+x/T+XFrroph8EJFb2nG2QDpO7Niuxd/vbW5EZjCy4NQZfLozC498tl+6/eykFDYUc2Gs0CEitxTs74OSKj3+ckMKQgJbvqO2j7cGqOHIh9qEEPjLqiNYubt275aHrknGw2OSubrFhTH5ICK389r/jks7nM5MS2jVc/h6mwaG9Rz5UI3RKDBnxR5sPnVZum/hrX0xfUi8ilGRHJh8EJFbEULgw22Z0u3WbjLmw51tVZVfWoU7/7UTZy+XS/d9N3ck+tvZpZacG5MPInIrhRU10rG9TcUa4sOaD9UUVVTj9qU7kFVQAQCICtZiw5/Hoj13q3UbfCeJyK2cv1ohHf971uBWP4+Pt2W1C5MPR3vq68NS4vHMpB54ZEwX1ne4GSYfROQ29AYjblr8GwBgcEIHBPu3vNDUwsdbIz0nOc7LPxzD+hOXAJhWtDwytovKEZESuNSWiNzG8Ysl0nFNG6dLLH0+WHDqOBn5ZVj22zkAwJ+v787Ew40x+SAit5F9tVI6njMysU3PJY18MPlwCCEEHlt5AACQlhSGP47rpnJEpCQmH0TkNi4UmuoEBsSFYlobG1BZCk457eIYuzKvSiNX3KvF/TH5ICK3UFJVgwU/nwQAjOoa3ubnY8Gp41RWG/BH86jHjKHxSI3voHJEpDQmH0TkFr7aU9sBs3OHgDY/nzTyYeTIh9I2nLyEy6U6AMADo5NUjoYcgckHEbmFM1bNqAYmtP2Ts6XDKft8KO+/+y4AAO4blYTkiPYqR0OOwOSDiNxCfkkVAGD64Dh0jwpq8/N5Sx1OmXwo6df0K1L79Kn9Y1WOhhyFyQcRubwrZTpsOJkPALixf4wsz+nLPh8O8ePhXABAXFgA+ncOUTkachQmH0Tk8l798bh0HB8WKMtz+pj7fLS1Xwg1TgiBbelXAACvTOvDLqYehMkHEbm81QdzpeNOoW0vNgXY4dQRvjuYi5yiSvh5eyEtqaPa4ZADMfkgIpcXGmhqo/732/tLS2TbigWnyvt893kAwODEDgjw81Y5GnIkJh9E5NKKKqpRZN7JdlKfaNmelwWnytLpDTiYXQQA+Nu0PuoGQw7H5IOIXNq4v2+RjtvJuOU6C06VdfhCMar1RoS390OXiHZqh0MOxuSDiFxWjcGIgvJqAECQjIkHwIJTpS346QQAYEhiGAtNPRCTDyJyWdlXK6TjT+5Pk/W5LQWnBnY4lV25Ti9Nudw2qLO6wZAqmHwQkcs6V2DqatozJhgD4kJlfe7ajeU48iG3IznFMAogKliLcT2j1A6HVMDkg4hcUmW1AXOW7wUAJIXL09vDGjeWU45l1CM1jhvIeSrZkw+DwYAXXngBSUlJCAgIQJcuXfDKK69ACP4HJiL5bEu/LB3HydRYzJovN5ZThE5vwHsb0gEAA+JD1Q2GVCNvhRaAhQsXYsmSJVixYgV69+6NvXv34t5770VISAgee+wxuV+OiDxU5pXajeTuHSH/TqiWkQ89C05l9cuxSyivNgAAxveMVDkaUovsycf27dsxbdo0TJkyBQCQmJiIlStXYvfu3XK/FBF5sJyiSgDAo2O7IDrEX/bnZ4dTZZy4WAIAuC4lEl0j274BILkm2addRowYgQ0bNuD06dMAgEOHDuHXX3/F5MmTGzxfp9OhpKTE5ouIqCn5JVX4eEcWAKBTB3naqdfFglNlpOeXAQCu6RauciSkJtlHPp577jmUlJQgJSUF3t7eMBgMeO211zBz5swGz1+wYAFefvllucMgIjf2/uYz0nFSR2UaVLHPhzIOnC8CAHSL4qiHJ5N95OOrr77CZ599hs8//xz79+/HihUr8H//939YsWJFg+fPmzcPxcXF0ld2drbcIRGRmzlzuUw6HpwYpshr+LLPh+y+O5iDK2U6AECPaCYfnkz2kY+nn34azz33HO68804AQN++fZGVlYUFCxZg1qxZ9c7XarXQarVyh0FEbuxicRUA4NP70uDno0zHAC61ld/PR/IAAGN7RCC8Pf/uezLZ/9dWVFTAy8v2ab29vWHkpwciksGGE5eQYa4biAtTpt4DqN1YjgWn8rGMWN07Uv7VSeRaZB/5mDp1Kl577TXEx8ejd+/eOHDgAN566y3MmTNH7pciIg/0yKf7pePYUOWSjwBf0xbvheYdc6lt9Aaj1JGWG8mR7MnHe++9hxdeeAGPPvoo8vPzERsbi4ceeggvvvii3C9FRB7GYBSoNo9EzB6RCF9v5Zo0W9q1H8kpRplOj/Yyb1znadLzy1BjEPD39UJsiHJJI7kG2f83BQUF4e2338bbb78t91MTkYe7VGKq9fDx0uCFG3sp+lpxYYEI8vdBaZUeecVV6BrZXtHXc3cvfXcMgGkfHi8v7mLr6bi3CxG5BCEExr65Wbrt7YALmNbHNPVSrWfdR1sYjQJHcooBAHNY70Fg8kFELuJCYaU05TKpT7RDXlNrXklTzaLTNrlYUoXKGgN8vTUOe+/IuTH5ICKXkGHV22PeDT0d8pqWZbw1TD7a5Kz5vYsPC1S0TodcB/8VEJHTMxoF/vrtEQDAxN5R6KTgKhdrfuYLJadd2uaHQ7kAgC4RrJshEyYfROT0tqZfRq65sVhSuOMuYL4+proSJh+tV1xRg6/3XQAA9DevICJi8kFETu/whWLp+LZBnR32upaRDx2Tj1Y7kVcCYW4Se8/wBHWDIafB5IOInFqNwYi31pl2yX56Yg+HLnn1Y8Fpm529bGosNqZ7BIL8fVWOhpwFkw8icmpLrXawdXSvDUtxJDeXaz1LsSnrPcgakw8icmqbT1+Wjod36ejQ167d34Wby7VGtd6Ij3dmAQCS2VKdrDD5ICKndTC7CPuyCgEAK+YMRbCDh+29NabkwyiYfLTGmmN5UrHuoIQOKkdDzoTJBxE5rQc/3isdq3HxkkY+jM6ZfHy1JxtDX1uPI1YFuc7keG4JACA2xB89Y4JVjoacCZMPInJK+aVVyC/VAQDGpUSqsrGbj7d55MMq+Th7uQyPfrZPurCq6ZlvDiO/VIdFa0+qHUqDMvJLAQAPj+2iciTkbLhNIxE5nRqDEUNf2yDd/mj2EFXi8NLUH/m444OduFKmw8mLpdj41FhV4gJM/TMsgvyd8095Rr6p2LQri02pDo58EJHTOZhdJB2P7xmlWhw+5mkXg1XycaXMNBpzoahSlZgs/rr6iHQcH+Z8xZyrDlzAuYIKAEC3qCCVoyFnw+SDiJxKTlElbl+6Q7r96k19VIvFq07y8dXebOmx5HB1L/i7M69Kx5Z+JM7kh0MXAZgatUUEaVWOhpyN8/2LJSKPJYTAm2tq6xfuG5WE6BB/1eLxqVNw+soPx6XHHN1zxFr21QqpHgYA9E7YBO30JVO9x0ezB6scCTkjJh9E5DQ+2ZmF1QdzpdsPj1G3UNHbauQjI78UpTq99JjGXA/iaEIITPjHVpv7nG01TrlOjwuFpmmp3rEhKkdDzojJBxE5hd2ZV/Hid8ek2x/PGar6cL0l+aiqMWD8W3Uu+CqNNuQWV6GyxmBzX42TjXx8uO0sACC8vRZh7fxUjoacEZMPIlLdxeJK3PFBbZ3HmO4RGNk1XMWITCxNxiyNzgCgnZ83AKBGpa6nqw/kAACC/X3w2LhuAJyrA2uNwYj3NmYAAPp15qgHNYzJBxGpqlynx4g3Nkq3A3y98e9Zg6VRBzV5e5n+RO4yF3emRAfhpam9AQB6FfZ7ySoox5trTwEApvSLga/5d+RMIx9ZBeVSge588++KqC4mH0RuqrLagENWS1adUW5RJXq/tBbW3cvXPn6NtKGb2ixNxix+nxYPXx/19nvZln5FOr45tTN8zL8ntUZhGmLZxbZvpxDEdwxUORpyVs7xP5yIZDdn+R5MW/wbVmw/p3YoDTp3pdxmxAMAFt3az6kuWD5Woy8D4kIxMy0BPl6WC75jRxsuFlfi+dVHAQCzRyRiaFIYfL0tq3GcY+TDaBR48JN9AIAklZcik3Nj8kHkhnR6A3acLQAAvPT9sWbOdrziihqM/b/NNve9cUtf3DEkTp2AGnF9L1ODMz8fL7w9fQC8vTRWF3zHjjYst0oix/WMBABphMhZaj62nymQjrmRHDXFOXvyElGbLDYX/FkIIVRbGtqQe5fvtrk9Z2QSpjtZ4gEAqfEdsPuv46CrMSIuzDQiY2nopdMbmvpWWeWXVOGDLZYVJH4Y0cVUjGuZFnKWmo+jubUb3P0+LV7FSMjZMfkgcjPnrpTj3TrJh8Eo6tUvqOX1n05g//ki6fbobuH4yw0pTpUcWYsMsm1yFuBr+rNZWe245GPet7Wt1L//wyipGNfXPAXkLH0+tqVfBgA8Mb6709TtkHPivw4iN3PK3FnSmrNcnDLyS/GvrWel2/07h+DjOUOlwklXEGheauuo5OPIhWJsOJkPAOge1R6xoQHSY8408rH2WB5+yzBNu3SP4kZy1DTX+R9PRM2qqjHgiS8P1rvfGS5O+SVV9Rp1ffXwcKcd8WiMlHzUKJ986PQGTP3nrwBMS5DXPn6NzeM+TlTz8ePhi9LxCCfo0ULOjckHkRtZufs8KsyfyOeMTJLud4aL02e7ztvcXn7vEGh9vFWKpvX8fU0xF1bUoNyq3brcDEaBOz7YKd1+bFy3eomaM/X52HvO1Avl/ZkDERLgq3I05OyYfBC5kfT8Mum4f1wILNeqGpWXYp64WIJ3NqRLtx8Z2wVje0SqGFHrRQXX1oDsPne1iTPbZv2JS1KfllsGdsIjY+vvcyP1+VB5Wm3+98dwsbgKAJAaH6pqLOQamHwQuYniyhp8bh5d6BbZHlP7xdYWJKo88vHXVbUFk8H+PvjjdV1VjKZt/Hy8MLZHBADgwtUKRV5jW/plPGTulwGYCjgbYqn5UHNXWyGEtAw4LiwAMSEBTX8DEZh8ELmN+Vb9PP42rQ+8vDRWFyf1kg8hhM3qlo/vS0Ogn2svtLNMK+j08l/0j+eW4O6Papci//yn0dIy37r8nKDmw7rWY/WjI1WLg1wLkw8iN7HKvOEYAPTuFAygtkOnmtMuD39a+wm+Z0ww+nVy/c3GLBd9uZOPymoD7vpol3R76V2D0DMmuNHzfVSu+SiurMEfVx4AYGqn3rG9ursQk+tg8kHkBoQQaK81jSY8OykFwf6mT+Zqd8A0GAXWHrsk3V716Ah4OcGGcW1laTRWLWPyUWMwYvxbW3C1vBoA8OpNfTCpT3ST3xNkfp/PKzT905zvD+VKx648lUaOx+SDyA3MWb4HZeaVF/eOTJTutyQfcl4kWyK/tEo67hrZXlop4uosq3SqZRpxqNYbce+yPcgpqgQAPD+lJ+4altDs90UEmUYa9EaBc1fKZYnFXkdzivGCea+ZWwZ2woTeTSdKRNaYfBC5OJ3egE2nTJ0l/by9bC7wAQ7sSdFQXMMX1G4c9+l9aQ6PQSlyj3y88fNJ/Jph2rH2Lzek4P7RyXZ9X8d2ftKx9UonpdUYjHjMPN0CALcN7Oyw1yb3IHvykZiYCI1GU+9r7ty5cr8UEQHIvlopHb84tZfNYwG+6iUfR3NKpONgfx9Eh/g3cbZrkTP5+PnIRfznt0wAwIyh8XjAzsQDALy8NEhLCgMAlOlq2hyLvd7bkI6z5pGWv9yQwqZi1GKyl5zv2bMHBkPtH7qjR4/i+uuvx+233y73SxERgOXbTReuXjHB9Ybqa1uBK9cMqzEXCmvrEOz9JO8qtDJtLneppAov/3AcgKk/xktTe7W442uYefSjrMox7/GGE5ds9g66cyg3kKOWkz35iIiIsLn9xhtvoEuXLhgzZozcL0Xk8TLyS/HpTlNvj6jg+isNLNMuJQ66MFlU6414f9MZAEB4ey0evMa9kg/LKpO27pnz919OIa+kCr7eGrw/c2CramLamQuNSxXstmpx4mIJ7luxV7p96KUJUnEzUUsoWvNRXV2NTz/9FHPmzHG5/RuIXMGx3Nqpjcl9Yuo9HmFe+njkQnG9x5T03cEcaYO7mWnxblNoamHpLGpoQ/Kx/3whvtlvWh792s19W92cyzIFVKNXdkVTVkE5Jr+zTbr9r7sHsY06tZqiycfq1atRVFSE2bNnN3qOTqdDSUmJzRcR2SfTPO8+NDEMtw+uX/Q3xFwPkOXgpZiHrZKdrpHut8OpHCMfz31zGAajQP+4UNw+qPUFm75SLMqtaDqVV4oxb26Wbi+8tS9Xt1CbKJp8fPTRR5g8eTJiY2MbPWfBggUICQmRvuLi4pQMicht5BVX4e31pv1SrusZ2eDoomX79fySqnqPKelKmU46Ht8zyqGv7QjeXm1ra37kQjFOXyqDlwb48O5BbRoZlvZ3UaiXy6/pVzDx7drdiN+fORDTh7DOg9pGseQjKysL69evx/3339/kefPmzUNxcbH0lZ2drVRIRG7l811Z0nFSeLsGz/FXoBlWc4oravDz0TwAwIf3DJbqTtyJr7ltfWumXQxGgT+s3A8AGNk1HJHBbVsFZBmFMSgw8rEt/bJNx9WPZg3GDX3rT+8RtZRiGywsW7YMkZGRmDJlSpPnabVaaLVsyUvUUmetmkqN6NKxwXP8fJRpA96U1386IR3HhbnnJmPelg37WpF8HMkpRlaBaRrsr1N6tjkWy/49co98bDqZj3uX75Fuf/PICAxK6CDra5DnUiT5MBqNWLZsGWbNmgUfH9feQIrIWZ29bEo+/n3PYKnNdl1SMaID9/748XBty+24Dg1viObqpJqPVlzwP9tpGrGa2DsKKdGN79tifyyWREi+93hfViEe/KR2Vcuax0fLEiuRhSLTLuvXr8f58+cxZ84cJZ6eyOP959dMHL9oKs7u0kRBp2UDNLnagNvDsrLlyeu7S8tA3Y13K4s8j+eW4Ot9FwAAY7pHyhKLr8w7F5+5XIaZ/96JGoOARgNseXosEw+SnSJ/GSZMmAAh1NvimcjdLTM3FgOAuA6NT20osQFaUyqrDSgwb4w2a0SiQ15TDa2t+diabmqDHxcWgNvasMLFmmUKSI5pl+KKGtzz0W5U1RjRKTQAXz08HJ1C3XPqjNTFvV2IXIwQApdLTatJFt3WT1rt0BBHJx/3Lt8tHbtzD4jWXvD3nisEANyVliC9N21Vmwi17T02GAVe/vGYtLndR7MHM/EgxTD5IHIxWQUVqKoxwttLg5sGdGryXMu0i94oYGxjN87mGI0CO89eVfQ1nEXtChP7f6cHzhdi/YlLACBr4aYllpo2vr+L1p7Et+amZ4t/P5BTLaQoJh9ELmb8W1sAAN4aTbOfnn2tHle67iO/tLa3xzt3DlD0tdRmWWHSktUuX+4xtRGIDwtEaryMyYclwWzD+5t+qRQfbDkLAHjommRM6cfltKQsJh9ELuRKmU664N0ysOlRD6D2UzHQtlbg9rBsJNe5QwCmNTMi4+q8W9Fb42iuqevrc5NTpO+XQ1sLToUQeOjTfQCA5PB2eHZSimyxETWGyQeRCzlt3i8FAF69qU+z51uWYQJt3wStOSfyTLF1bqIA1l1Iy1vtvOBX6404mmNandS3U4issUj1J618f7/Zn4Ozl8vh663Bf2YPgZeMiRFRY5h8ELmQd8zt1Mf3jGqy0NTCUSMfheXVeGH1UQBAx/bu3zSwpdMub/x8EgAQ1s5P9uSsLQWnV8p0+OuqIwCAaQM6IbGRTrlEcmPyQeQijuYUY1emqaCzW5R9m7V5eWlg2TZEyY3HLD1HAKBPrLyf7J1RSwtOfzluajd/V1q87Dt8+7Rhqe0Ph3Kh0xsRHeyPZyb2kDUuoqYw+SByEdY7xc5owcZerVmZ0VLZVrvmzhmVqNjrOAtLzYY9nWPLdHpcKDQtX50zKkn2WKRRmBYWnGZfrcDLPxwHAMwemdjmPWaIWoLJB5GLOJVnGl14YHQS4jva37bcuw2twO1lubjeNSweWh/320iuLstogz0J3SlzLUxUsBahgX6yx+LbipU3APDVXtPqmw6Bvrgl1b0LhMn5MPkgcgFnL5dhxQ7TniA9Wth/wbcNm6DZ66T5Auuue7nU1ZKaj2W/mbrRtvR9s1drGp7p9AYs334OAPDXKb046kEOx+SDyAWs3H1eOh4YH9qi7/WWqQNmY3acKZCaZ3X2lOTDy76pjpKqGvx05CIAYFLvaEVi8W3Fst+3fjmN0io9fLw0GNsjQpG4iJrC5IPIBRzLNU25XJcSieQI+4pNLaQLpUIjH+uOX5KO+8e5f7EpYL2xXNO/04z8MhgFEKT1wYyhcYrEUttkzP739xfze/bkhO4I94DVSeR8mHwQObmLxZXYfqYAgGmn2JZSuuajoNzU2XRoUpjHjHz4ettX85GRXwYA6BcXIvsqFwvLFFCNnSMf2zOuIPNKOby9NLhrWIIiMRE1h8kHkZN75r+HpWN7l9haa0lxZGucN690udeNd7Gty3rko6kdvC3JR7fIIMVi8W1hw7P5PxwDAKQlhSHY3303/yPnxuSDyIkJIXDwfBEA4Ia+0a1aSVJbHKlMzYdlmW1cmGeMegD2NW+rqNbja/OKkh7RyiUftct+m08+8oqrcPqSKSF6xY4OuURKYfJB5MQOZBehVKeHn7cX/jF9QKueQ8lpl3fWp+NKWTUAD0s+rLrLNnbRX38iH4UVNQCACb2iFIulJR1OVx0w7Vrbp1MwurSwdohITkw+iJyU0Shwy/vbAZg+Obe2f4aSTcb+tfWMdBwS4DlD+P5WuwXr9IYGzzljnnK5fVBnRVvO21twWlRRjXc2nAYAjOwarlg8RPZg8kHkpM5eKZOO7x7e+sJAb4X6fAghoNObPm3/Z/ZgWZ/b2fl4e0kjDpU1jSQfl831Hq2o02lRLF72FZzuPHsVVTVGeGmAR8d0VTQmouYw+SByUu9tzAAADErogDsGt36ZZu2wvLzJR0F5NfRGAY0GGN3N83pF+PuaRqIqqxtLPsoBAF0jlU0+fO0c+dhzzrQv0PQh8QgJ9JxRKnJOTD6InNDRnGJ8dzAXgGkJa1u0ZB+SlsgtMrVUj2ivlS6AniTAknw0MPKx59xVnDBvttc9SrliU8C+lTcXCivw0a+mTqstbVJHpATP+4tB5AK2pV+Rjn8/1P5N5BqiVM3HzH/vAgBEh3hma27LyMfOs1frPWYp7OwW2V7x3ieWkS2g8ffYksj6eGkwQaFOq0QtweSDyMmcyivFwjUnAQAv3NirzatI7O3G2RJXynQordIDAAbGd5DteV2JpbV6XnFlvcfOmus9HrwmWfE4rFfeNPYeH8wuAgA8NznFowqDyXkx+SByMit2nJOOx3Rv+6oEJZqMZV4pl46fGN/yrqvu4P7RpsQit6iq3mOW34/S9R6Abc+RhqbWKqsNUgv8/nGhisdDZA8mH0ROZF9WIT7fZdpE7q5h8egqQ2fMluzAai9Lvcew5DCPLV60TLtU17ngl+v0uFRiajmfHO7Y5KOholNLMuvrrUHvWGV21iVqKSYfRE7k+dVHpeN5k3vK8pw+rdj1tDkXCk3JR2xogGzP6Wr8zL0+qvW2v9e1x/IAAOHt/RySmHlbj3w08B7/lmGqH7qxXywC/XwUj4fIHkw+yOPoDUbkl9YfKlfbvG+PSCskXryxF9pp5blQtKT9tj2ullfjzbWnAADxHtTVtC5LoWfdqY4lm02N1xy1/Fij0aC9+d+KpQ7HQm8wYn9WIQDg/tFJDomHyB5MPshj/HzkIm5fuh1d//ozhr62waY7p9rWHL2IlbtN0y1DEjtgzij5LhRy13ysN9cPAMB1KZGyPKcr8jMXelonHxXVeqSbO5s+PbGHw2Ip05mSjs92nre5//tDuSivNiBI64OUaE65kPPgGJybKqqoxr6sQmReKUdplR5+Pl64Z3gCghy0i2VRRTW2nynA0ZxiTOkXg96xIQ553bpMm3tdwEvfH6v32Os/ncSdQ+NV39lz48lLePjT/dLtN27tJ+vz+zTyCb21MgtMxZTje0ahX+dQWZ7TFUnTLlYjSpbpqGB/H1WmpPJKbFferNh+DgAwID7UZnqGSG1MPtyIEAJrj13CI5/tQ0O9hjLyy1q9OVlLvLXuNN7dkC7d/uFwLrY9c53ir1tXTlElHvx4L47lljR6Tm5RJYKj1Uk+ynR6vL3uNP5tbv4EAO/OSJV9wy/LJ3SdXp7kw7KL7bDktjU/c3WWxmrWNR8XCk2/G6V7e9T10DXJ+GDrWXQI9JPuq6w2SP/2X5ray6HxEDWHyYebqKjWY/oHO3Ekp7jRc1YdyMGCW/pKVfpyyyuuwjWLNtWr/s++WomD2UUY4MBlfhXVekx5dxuKzLuKWhuWHCY1htLVKLPNfHPWHb+EBz7ea3Pfwlv74nf9Y2V/Lcv7rWtkD5KW0BuM+PHwRQCOv8A6G98Gpl1+PmIqNnV0LUzH9qako8Kq1fvB7CLojQLRwf7cwZacDpMPN5CRX4bxb22x69wrZTpFLhrnrpTjur9vRmNlBcdzSxyWfGRfrcDoRZvq3f/c5BQ8MDoZ3l4aXPf3zTh7uVy20QB7CCGwO/Mqnv7vYZw3jx5YvHhjrzbt39KUAD9T8lElw8/6wdaz0nGvGM+uIfDzMU1jWEY+iiqq8d/9FwCYpjkcKcC8iqWiurbg9P3N5r2BEjtAo+GUCzkXJh8ubl9WIW5dsr3Bx/58fXcMiA9F/7hQjFiwEWU6PaoU+qT/wdaz9RKPh8Yk43B2MXacLbD5o6ikgjIdblr8m819gxI64N0ZqehkNQdv2Z6+SobRAHtkFZTjya8OYZ955YG1ZfcOwbU9lCvctGz/LsfPuivTNGIU3l6L+I6ePfJRt5D30IViabrzrmGt34W4NQLNo1uWkY/C8mqpRf9EtlMnJ8Tkw4UVlOlw34o99e5fPXdkvVGGdlpvc/Ih/8V24ZqT0koNi11/GYeoYH88981h7Dhb4LCL/D83ZaCgvFq6PXtEIv5yQ0+pONBC6yNvHURT9mVdxa1LdtS7PybEHyvmDFV84zGtr3yJVpa52PSfv09t83O5Ou86e+ZYamHG94yUlr46iq+P7c62B7Jrk1wlpvKI2orJhwt76JN9NjUNvWODsfLBYQ2u3giQ8QJUl6WvAQAEaX2w8y/jpB4VliH/ika2HZfTmqMXsey3c9LtWwd2xvNTetrsfWHh72tJPpSNK/tqRYOJx1t39MdNAzrBywErECwXQstKjNYq1+mRVWC6wCaFt2tzXK6u7p45lt+vGrUwPlIspmTaMsJ2+6DODo+FyB5MPlzUD4dysddqCH9Kvxi8e2dqo8vp/KXkQ75P+kII/OmLg9JtX28NDr40wSaGAF/HJR/Wy1X7dw7BG7f2bTDxACB1eiyrUm46KPNKOa79v802993QNxpv3NrPoct7LRu/WRqYtdaDn5gKZEMCfBEZpG1zXK7OcsE3muda9pwzTUl17uD4JbY+VolQuU6PxZtMHwgGJXjmpn/k/BRpMpaTk4O77roLHTt2REBAAPr27Yu9e/c2/41kl/zSKvxx5QHpdv+4UPxzRuOJB1A79F4p48jHhcJKfH8oV7r95UPD68Vg2W59ubnfgFLeWV+7tNdLA6x6dKS0GqEhUcGmi+fqgzmKxFOu0+Mu85bzFs9OSsH7Mwc5vK9IYrjpk3hhRU2ra28MRoHfMgoAADcNiGUBIyCNWukNRuzOvCqNNvRSYf8USy8Xg1Fg86nL0v3XenATOHJuso98FBYWYuTIkbj22mvx888/IyIiAunp6ejQgRm4HIQQeNvqQgsAn9w3tNmLQZh5j4m6qyza4herTpfTBsQ2uLV6alztfZXVBmkaRk6XS3X4x/rT0u1vHx3Z7HRGY+2o5bJozUnkFNVOcyy9ayAm9YlR5LWaY11/UFFtaNX+Hpb+FVofL7w4tbdssbmy2pEPYNOpfACm38+wpI4Oj8Xbq7bmY/95UxI0qXc0ooL9HR4LkT1kTz4WLlyIuLg4LFu2TLovKYl7Cshlw4l8addTAPjP7MF2fZIOb2/6pP+fXzNxnwytu49cKMYrPx6Xbj83OaXB8/p0qv0UeKVMhzgF+h98titLOr51YGe7lvRO7huDD7dlNtgHpK1yiiqxYkdtTG/c0le1xAMw7f3h661BjUG0usvpuxtMyzaTI9qzU6aZl6a2zuK8uRbmmUkpDqnjqcvHqvjVknxM7BPl8DiI7CX7tMv333+PwYMH4/bbb0dkZCRSU1Px4Ycfyv0yHmtreu2Q6h+v64rrUuz7A2Np9az1kectt47j8fHdEBPS8Dy3RqNBrHnq5arVKhS57D9fKI0E+Xl7Yf7v7OvkaOkEWVwpf/Ix88Od0nFyeDvcMlD9oj+pIZa+5fu75JdU4Rtz/4ohiRzBtLCe6rCMKKq10Z4l+Th1qRQHzhcBQIMjkUTOQvbk4+zZs1iyZAm6deuGtWvX4pFHHsFjjz2GFStWNHi+TqdDSUmJzRc17JOdWfjY6hN1S5pSTRtgWm6XX6prcxwGo8CW07XJxy2pTV9c/f2UW2mzzmrq5+mJPezeuybI31xwqtPDKNOGawBQWlWDcwW1U1trHr+m3jJfNUitwFsx8nH4Qm3X3MfGdZMtJldnvVuwpbOwasmHt+1oS3JEO4/ecZicn+zTLkajEYMHD8brr78OAEhNTcXRo0exdOlSzJo1q975CxYswMsvvyx3GG7pP1Z7gHz76IgWTWFYai3kWFq6fPs57DY3m1r8+4HNNpuyNPSSu6fGsdxim2W+v0+Lt/t7rUeAqg1G+Hu1vRbFaBS4+f3ahm+v39zXKRIPoOFW4PYQQmDR2pMAgJtTO0nTdwR416mzCvTzRnKEOkuQLTUfFn/7XR8WBZNTk/0vY0xMDHr1sh367tmzJ86fP9/g+fPmzUNxcbH0lZ2dLXdIbmH/edMOtYBpeqFPC3eJtSQANQbR5q3VV1itXOkZ03yDLKUaej319WHpeOUDw6TeIvaw/D7kjOtwTjEyzNupazQtS4aU5tfKnW03ncrH6Uumn6l/Z3V2JnZWPnUu+LcP6tzkCitlY6lNNDQaYGBCqCpxENlL9pGPkSNH4tSpUzb3nT59GgkJDbcb1mq10Gr5aao59y6r7WR64MXrW/yJ2uaTvt7YplUnluRlZNeOSLZjw6ra5EPeaZczl8uk49QW7qXh662BRgMIYYmr7ctfz+TXxvPspIYLcNVi6YBZY2hZ4vlreoF0fJtCe8+4qjq5B+bIUMjdWtbTLkkd27VqRRORI8mepj/xxBPYuXMnXn/9dWRkZODzzz/Hv/71L8ydO1ful/IYxZU1UmFkVLC2RZ/wLayTj7YkAYs3ZUhLSN+5074W21ppV1X5Rj5eWH1U2tBrx7zrWrxTr0ajkbaar5Zh5CO3qBJ//voQAKBvpxA8dE1ym59TTq2ZdjmYXYT//Gaa6vvH9P4Obxnu7OqOfMSGOr65mIXeKqmc0k+9lVVE9pI9+RgyZAhWrVqFlStXok+fPnjllVfw9ttvY+bMmXK/lEcwGIXNjrUrHxjWqufx8faSCuTaMs3w5traUS175/8tiU+VTCMfVTUGfLKztvC2sZU29sYlx7SL9fLnib2jnG6+3bcVidbiTRnS8Ygu4bLH5OqslxxHBGlVm3IBYFP/9eT13VWLg8heinyUufHGG3HjjTcq8dQe51ReKS6bV6j0igm2a5qjMVofL1RUG1o9AlFs1ROjJase/GUe+Ui/VDu9sei2fq1+Hq2vN1CllyWui8VV0vG0AZ3a/Hxyk/pACPumXX7LuCKtJHrt5j5sVtUA6zqLG/qou3NsSIAvfnvuOrTz83a6xJeoIRxHdXLHrfbjeGZSjzY9l7+vNyqqDa0egVh3wnQxig3xb9GnKzlHGMp1ekz9568AgB5RQS1ablyXXxuWn1orLK+W+mC8dUd/RRqptZWl8ZXBjpoPIYTUvt/P2wu3cXOyBnl5abBizlAcyy3Gw9d0UTscdFJx2oeopZh8OLGM/DI8Za4jmD0iEWN7tG2fBikJaMUn/avl1VIsUSEt+xQsZ8HprszaAsi05LA2PZfWsrNtG/uPvPzDMem4W2Tzq3/UYKlHbG7kQwiBcW9tkRrCvTtjgM3KILI1pnsExnSPUDsMIpfjHE0IqEE/HbkoHcuxWVVbkgBLXw8AuKGFrcLl7PNhPeXyxPi2zW3LFddPR/Ok4xQ7lh6rwVIc2dQyayEEnvvmCM5eNi3pvrZHhKpt4YnIfXHkw0kJIbDjTO2n/BtlqGBvy8U2q8B0QfL20rR4SaFlhKGtHU5P5ZViwc+mhlePj++GDu382vR8luXKbVntYjQKWGbYVz4wTNWiw6ZYFmY0lnzUGIx47psj0vQRALx5e39HhEZEHojJh5P697ZM7DhrSj4+uz9NlnX70jRDC0c+qvVG6aI/d2yXFm8s5i/TCMNXe2sb0PW3Y/O45shRi7J06xnp+wc78b4nTY18FFVU409fHLRpmf/pfWnsZkpEimHy4aTe25guHffpJE9nydbWfHy5p3YZaZfIlq+2qa2taFvykW5u4pXQMRBjZZhnb2stit5gxKI1pqXHnUIDnHbUA7AqOK2TfGRfrcDsZbtxxjzVAgBfPDgMw5Idvy08EXkOJh9OqLSqBiVVegDAvMkpCAloe/dNoHbJa0tXu1h2yQSAsd1bXvQqR8Hp/w5fxFbzJ/O37ugvy3LCttZ8WC+v/fME5+6t0FDBaUZ+mU0PGQBYMnMgEw8iUhyTDycjhMDoRZuk2/fJ2LK5tSMfWebtwt+bkYqQwJYnQnIUdlqvKOkVI+9IUGtrPrLMu9cmR7TDLQOdezmqd51pl5N5JZj09jabc5beNZAFpkTkEEw+nExucRWKzM28RnbtCB8Zh/JbkwTsy7qKfVmFAICk8Nbt2NnW2gqDUUi/k4W39m3TvjQNx9W6EZn55oQowQn7etTlbVVw+sOhXKmPh8Wy2UNwbUrblnITEdmLyYeT+dqqqPLt6fbtnWKv1qw6mf/9cem4ayvqPVr7utZe+fE4qg1GaDTAbYPk29ysLbUoV8urpR1su0U55/Jaa5Yi4XXHL9kUlgLAd3NHylLAS0RkLyYfTuTM5TK8vd5UaDqpdzQiguRdbdDSkQ8hhHSBffL67i3evM2iLatdynV6LN9+DgAwNDGsxSttmtKWDqfHcoul41kjEuUKSTGWaZe6icfuv45DZBBbpxORYzH5cCLbrfp63JQq//4g/i1captbXIXKGgO8NMCDbdiltS2dRE/m1baXf/WmPq2OoSHSbrutSIqO5ZrimtIvxiXaWlfXec+9NMDxv01qdUJJRNQWzrs20MNUVhvwwuqjAIApfWMwSYGNqiwjH1V2TDMIIfC790x7qHTqENCmi5Rltc7JvFIIOzc2A0xLWe/5aDcAU7dNuac3agtwW5YUGYwCb5j7nvSWofOsIxzMLrK5/cMfRzHxICLVMPlwEh/9elY6nqjQDpmWJKDIanfaxlwu1aHAvL/H5DaugIjrUFuQeSSnuIkzbW08mY/yalNioMTyz9ZOu3y5p7Yup0+sPCtvlHapRCcdL7y1L3q7SNxE5J6YfDiJXVZ7p4xTaNVBx/amduTWLbQbc9q8h0qgnzfmTU5p0+tat0E/b162a4995wul47uHJ7Qphoa0tuB0b1btezW8i2v0xBjdLRwAcF1KJKYPiVc5GiLydKz5cAJf783GtvQrAIDVc0einVaZt6VLRO1qlYpqfaMt241GgWf+a9rBdlTXcFkaek3pF4P/Hb6Iy6W65k8GcO5KOT7YYhoNWnhrX1nay9fV2v4jmVdM3UAX/36gU3c1tfb3O/rjp8MXcesg5+5HQkSewTX+cjq5qhoD1h2/hMnvbEPf+Wvxyc6sFn2/Zd8UAEiJVm7Z5qCE2r1HLhRWNnre9jMFyDV37xyYIM9+Je3NyUNFtX31Ff/aVjsNNTgxTJYY6mpN/xGjUUgdX5MjWtf3RA2RQf6YPTIJQf7ydMslImoLjny0gRACG07k46FP99nsmfHSd0dxV1q8XSMGm0/l46q5tmLFnKGKFwF27hCAC4WVKNfpGz3n0IUi6fiuYfJMd/i14EIvhMC645cAALNHJNqM2MjJrxVNxv6y6oh03Nqma0REno4jH23wzf4c3P/x3nqbdRkFMPCVdc1+f7lOj9nL9gAwbUw2RobN0prT3Kf9qhoD3l5/GoBpX5n2Mk0BtWT7+v/8dk6anvnjdV1lef2GtGbaxTI9NjA+lKtFiIhaiclHK/3n10w89fWhRh8vrKhBQVnT9Q0rd9fuFvv0xB6yxdYUv2YuuB/9mokagymZGiTTlIvpde0fZfhsl2naqmM7P3RUcFv3lk67VOuNyCsxTUctnjlQsbiIiNwdk49WOJpTjL/9eNzmvoHxodj81FgcenGCdJ+lO2hjz/Hq/04AAH7XP1aRpmINaW4ztZ1nTY3OukS0kzf58LZv5OPrvdk4a97efd2TY2R7/QZjMv8uDmUXwWhsvv/IuYJyGIwC7bU+iA5mV1AiotZi8tEKj31huynXO3cOwLePjkRieDuEBPpKyxqzGllWWq03Yu7n+6XbSiwjbUxTIxC7zhZI0wpvT0+VZZVL3ddtKvnQG4xYuMZUfDs0KQxhVkt0ldDLqkFYcWXzvU/mLDdNkXWJaCfr74aIyNMw+WihHWcKpE/mALBk5kBMG2A7ahFn3uU0u5Hk4/3NGdJ27K/c1AdDFFrN0ZCmRj5e/M60S2tooC96xijTTbSphl5vrTuNK2Wm4ttFt/aT9fUbEt5eK9W0FFZUN3lucUWNtELo+l5RisdGROTOmHy0QEGZDjM+3CndvmtYPCb3rd/907LF+nsbM6Cvc7FddeCCtHkcYJpycaTG6hz2ny/EqUulAIB/3DEAPjL3r2hu5GPX2QK8v/kMAGDW8AQkOmglSWigaelpYTNdXy2/GwCYe61yRbBERJ6AyUcLnMqrvQCFt9fi1Zv6Nnie9dJQ611Efzyciye+rC1S3f/C9VLLc0exrPCwTgIqqw2Y/sEOAECvmGBcq0CH1aZGXKpqDJj5713S7T9c1032129MoJ+5ALeZ/V1OmTe4uy4lklMuRERtxOTDTpdLdfi91QXyxam9Gj13TI/aJbOnL5VBpzfgiS8P4g+f19aKfDRrsOI1DQ1pqObj5R+OSStcnp6kzKobvyamXf7+yynozQWfn9w3FBFByq1wqcvHvNV8TRMFp6VVNXjBPCXVXebN7YiIPBGTDzt9dzBHOn5gdFKT0yW+3l6Ye20XAMDCNSfR4/k1WHWg9vtfu7kPxvVUp26gdidXUxKw6WQ+vjBvlHbnkDhc20OZfWX8vBte4rt0yxl8uC0TADCiS0eM7BKuyOs3xtfbNIpRd3rM2q/mIlyA9R5ERHJgh1M7nb1SW2SaltT8ZmLRIQEN3v/Z/WkY2dWxF1hr1iMQ645fwgMf75UeU7KWoW6HUyEEXvzumNSKvmM7Pyy/dyi8vBw7pWHZm6WmieTjaK5pJ95RXcNlXX5MROSpmHzY4WB2ET7fZWoINrlPNMb1bH50ILzOlMrghA7496zBCA10/FSLNcvIx3sbM2zu//GPo6RVOkqwJB8nLpbgjZ9PYumWMzaP//LENdI5juRjHvmwTDs1ZPsZU++Tib056kFEJAcmH3ZYvKn2Qv3wmC52FRyO6xmFWwd2RniQH2YMiXfY6o3mNPQB/6uHhqNPpxBFX9e6yZh14uHtpcHmp8Yq2sm0KZaRD72x4ZGPn49clDaS69s51EFRERG5NyYfdthqtWLF3oJDPx8v/P2O/kqF1GpGYfsJf9WjI5Aar/xUQkP52s2pnfDqTX3QTqb9Y1pDmnbRNzzyseZYnnTcT+EEjYjIUzD5aMbF4kqpTuHHP45CgJ9rbyZ278hEZF+twLieUbhlYCeHbY7Wv3MoescG41huCZ6f0hM3pXZCuEqjHdZ8zDUmNY2MfBzMLgJg2nHY0fUoRETuislHMya/s0067hUT3MSZriGhYzt8NHuIw183wM8b/3tstMNftzm+PpaRj/rJx6oDF6ROtAM45UJEJBsutW2CTm9AkbnzZWigLz/5uiFf83uqb6DPx+JNptqU7lHtERLo2GZwRETujMlHEyyfegFg9aMjVYyElOIjLbW1TT7yS6qkXYk/vGeww+MiInJnTD6a8PzqowCAAXGhTrNaheTVWJ+PBz7ZBwDoHRuMhI5874mI5CR78jF//nxoNBqbr5SUFLlfRnFnL5dhd+ZVAEBsqL/K0ZBSGupwmnmlHIfMhabTh8SpERYRkVtTpOC0d+/eWL9+fe2L+LheXetpq11MZ6YlqBgJKanu3i6mzqumEa/EjoG4exjfeyIiuSmSFfj4+CA6OlqJp3aY9Eum+f6JvaNUbYdOyvL1MS+1Na922XzqMraZ93KZNSKRO9gSESlAkZqP9PR0xMbGIjk5GTNnzsT58+cbPVen06GkpMTmS20FZTr8fd1pAEA/LrF0a75elg6nAvklVXj4U1Oth5+3F2YMjVczNCIityV78pGWlobly5djzZo1WLJkCTIzMzF69GiUlpY2eP6CBQsQEhIifcXFqT/Hvnz7OenYHXp7UOOsC06f+eaw1FBu9dyRDmvARkTkaWSfdpk8ebJ03K9fP6SlpSEhIQFfffUV7rvvvnrnz5s3D08++aR0u6SkRPUE5GRebaLEKRf3ZtlY7rNdtaNzj4ztgl6xTDqJiJSieCVoaGgounfvjoyMjAYf12q10GrVb7Nt7UJhJQDgP7MHq7LTKjmOoU5zsaGJYXhmYg+VoiEi8gyKX1nLyspw5swZxMTEKP1Ssth8Kh8nLprqTuLD2N/B3Vn397hpQCy+eng4i0yJiBQm+8jHU089halTpyIhIQG5ubl46aWX4O3tjRkzZsj9Uop4+Yfj0nF8WKCKkZAj3D08AacvleLGfrG4oa9rJMhERK5O9uTjwoULmDFjBgoKChAREYFRo0Zh586diIiIkPulZGc0CmReKQcAPD2xB6dcPEBkkD8+uJvt04mIHEn25OOLL76Q+ykd5lJpFQDA20uDB69JVjkaIiIi98SP9lbuXbYHABAd7C8twSQiIiJ58QprdrlUJy2xHdGlo8rREBERuS8mH2bp+bW9Pf5yQ08VIyEiInJvTD7Mdp017WB7XUokOrTzUzkaIiIi98XkA0BWQTne2ZAOAOgW1V7laIiIiNwbkw+YdjK1mNa/k4qREBERuT8mHwBOXTLVezw8hnt6EBERKc3jk4+sgnJ8bt5UjIkHERGR8jw++Vi65Yx0nJYUpmIkREREnsHjk4/DF4oBAPePSkJUsL/K0RAREbk/j04+Np3Mx7Fc0w629wxPVDcYIiIiD+HRycdL3x8DAMSE+CMuLEDlaIiIiDyDxyYfecVVOH+1AgDw7oxUaDQalSMiIiLyDB6ZfOgNRkx+ZysAICU6CEMSWWhKRETkKB6ZfBy6UIzCihoAwIyh8SpHQ0RE5Fk8LvkoqqjGrUu2AwCu6R6BWSMS1Q2IiIjIw3hc8vHhtrPS8c2psSpGQkRE5Jk8Kvn4dv8FLN5kaio2vmckbhrAfVyIiIgczaOSj493ZEnHr9/SlytciIiIVOAxycflUh0OXSgCAKx/cgwig9jNlIiISA0+agfgKFpfL/xtWh+cyS9D18j2aodDRETksTwm+Qj298XdwxLUDoOIiMjjecy0CxERETkHJh9ERETkUEw+iIiIyKGYfBAREZFDMfkgIiIih2LyQURERA7F5IOIiIgciskHERERORSTDyIiInIoJh9ERETkUEw+iIiIyKGYfBAREZFDMfkgIiIih3K6XW2FEACAkpISlSMhIiIie1mu25breFOcLvkoLS0FAMTFxakcCREREbVUaWkpQkJCmjxHI+xJURzIaDQiNzcXQUFB0Gg0sjxnSUkJ4uLikJ2djeDgYFmek+TD98e58f1xfnyPnJunvD9CCJSWliI2NhZeXk1XdTjdyIeXlxc6d+6syHMHBwe79Rvv6vj+ODe+P86P75Fz84T3p7kRDwsWnBIREZFDMfkgIiIih/KI5EOr1eKll16CVqtVOxRqAN8f58b3x/nxPXJufH/qc7qCUyIiInJvHjHyQURERM6DyQcRERE5FJMPIiIicigmH0RERORQbp98LF68GImJifD390daWhp2796tdkgeYf78+dBoNDZfKSkp0uNVVVWYO3cuOnbsiPbt2+PWW2/FpUuXbJ7j/PnzmDJlCgIDAxEZGYmnn34aer3e0T+KW9i6dSumTp2K2NhYaDQarF692uZxIQRefPFFxMTEICAgAOPHj0d6errNOVevXsXMmTMRHByM0NBQ3HfffSgrK7M55/Dhwxg9ejT8/f0RFxeHRYsWKf2juY3m3qPZs2fX+z81adIkm3P4HilnwYIFGDJkCIKCghAZGYmbbroJp06dsjlHrr9rmzdvxsCBA6HVatG1a1csX75c6R/P4dw6+fjyyy/x5JNP4qWXXsL+/fvRv39/TJw4Efn5+WqH5hF69+6NixcvSl+//vqr9NgTTzyBH374AV9//TW2bNmC3Nxc3HLLLdLjBoMBU6ZMQXV1NbZv344VK1Zg+fLlePHFF9X4UVxeeXk5+vfvj8WLFzf4+KJFi/Duu+9i6dKl2LVrF9q1a4eJEyeiqqpKOmfmzJk4duwY1q1bhx9//BFbt27Fgw8+KD1eUlKCCRMmICEhAfv27cObb76J+fPn41//+pfiP587aO49AoBJkybZ/J9auXKlzeN8j5SzZcsWzJ07Fzt37sS6detQU1ODCRMmoLy8XDpHjr9rmZmZmDJlCq699locPHgQjz/+OO6//36sXbvWoT+v4oQbGzp0qJg7d65022AwiNjYWLFgwQIVo/IML730kujfv3+DjxUVFQlfX1/x9ddfS/edOHFCABA7duwQQgjx008/CS8vL5GXlyeds2TJEhEcHCx0Op2isbs7AGLVqlXSbaPRKKKjo8Wbb74p3VdUVCS0Wq1YuXKlEEKI48ePCwBiz5490jk///yz0Gg0IicnRwghxPvvvy86dOhg8/48++yzokePHgr/RO6n7nskhBCzZs0S06ZNa/R7+B45Vn5+vgAgtmzZIoSQ7+/aM888I3r37m3zWtOnTxcTJ05U+kdyKLcd+aiursa+ffswfvx46T4vLy+MHz8eO3bsUDEyz5Geno7Y2FgkJydj5syZOH/+PABg3759qKmpsXlvUlJSEB8fL703O3bsQN++fREVFSWdM3HiRJSUlODYsWOO/UHcXGZmJvLy8mzej5CQEKSlpdm8H6GhoRg8eLB0zvjx4+Hl5YVdu3ZJ51xzzTXw8/OTzpk4cSJOnTqFwsJCB/007m3z5s2IjIxEjx498Mgjj6CgoEB6jO+RYxUXFwMAwsLCAMj3d23Hjh02z2E5x92uW26bfFy5cgUGg8HmTQaAqKgo5OXlqRSV50hLS8Py5cuxZs0aLFmyBJmZmRg9ejRKS0uRl5cHPz8/hIaG2nyP9XuTl5fX4HtneYzkY/l9NvV/JS8vD5GRkTaP+/j4ICwsjO+Zg0yaNAkff/wxNmzYgIULF2LLli2YPHkyDAYDAL5HjmQ0GvH4449j5MiR6NOnDwDI9netsXNKSkpQWVmpxI+jCqfb1Zbcw+TJk6Xjfv36IS0tDQkJCfjqq68QEBCgYmRErunOO++Ujvv27Yt+/fqhS5cu2Lx5M8aNG6diZJ5n7ty5OHr0qE0dG7WM2458hIeHw9vbu16l8aVLlxAdHa1SVJ4rNDQU3bt3R0ZGBqKjo1FdXY2ioiKbc6zfm+jo6AbfO8tjJB/L77Op/yvR0dH1CrX1ej2uXr3K90wlycnJCA8PR0ZGBgC+R47yhz/8AT/++CM2bdqEzp07S/fL9XetsXOCg4Pd6oOb2yYffn5+GDRoEDZs2CDdZzQasWHDBgwfPlzFyDxTWVkZzpw5g5iYGAwaNAi+vr42782pU6dw/vx56b0ZPnw4jhw5YvPHdN26dQgODkavXr0cHr87S0pKQnR0tM37UVJSgl27dtm8H0VFRdi3b590zsaNG2E0GpGWliads3XrVtTU1EjnrFu3Dj169ECHDh0c9NN4jgsXLqCgoAAxMTEA+B4pTQiBP/zhD1i1ahU2btyIpKQkm8fl+rs2fPhwm+ewnON21y21K16V9MUXXwitViuWL18ujh8/Lh588EERGhpqU2lMyvjzn/8sNm/eLDIzM8Vvv/0mxo8fL8LDw0V+fr4QQoiHH35YxMfHi40bN4q9e/eK4cOHi+HDh0vfr9frRZ8+fcSECRPEwYMHxZo1a0RERISYN2+eWj+SSystLRUHDhwQBw4cEADEW2+9JQ4cOCCysrKEEEK88cYbIjQ0VHz33Xfi8OHDYtq0aSIpKUlUVlZKzzFp0iSRmpoqdu3aJX799VfRrVs3MWPGDOnxoqIiERUVJe6++25x9OhR8cUXX4jAwEDxwQcfOPzndUVNvUelpaXiqaeeEjt27BCZmZli/fr1YuDAgaJbt26iqqpKeg6+R8p55JFHREhIiNi8ebO4ePGi9FVRUSGdI8fftbNnz4rAwEDx9NNPixMnTojFixcLb29vsWbNGof+vEpz6+RDCCHee+89ER8fL/z8/MTQoUPFzp071Q7JI0yfPl3ExMQIPz8/0alTJzF9+nSRkZEhPV5ZWSkeffRR0aFDBxEYGChuvvlmcfHiRZvnOHfunJg8ebIICAgQ4eHh4s9//rOoqalx9I/iFjZt2iQA1PuaNWuWEMK03PaFF14QUVFRQqvVinHjxolTp07ZPEdBQYGYMWOGaN++vQgODhb33nuvKC0ttTnn0KFDYtSoUUKr1YpOnTqJN954w1E/ostr6j2qqKgQEyZMEBEREcLX11ckJCSIBx54oN4HKb5HymnovQEgli1bJp0j19+1TZs2iQEDBgg/Pz+RnJxs8xruQiOEEI4ebSEiIiLP5bY1H0REROScmHwQERGRQzH5ICIiIodi8kFEREQOxeSDiIiIHIrJBxERETkUkw8iIiJyKCYfRERE5FBMPoiIiMihmHwQERGRQzH5ICIiIodi8kFEREQO9f9C3QoEczhwzQAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('../dataset/train_v2/1004.csv')\n",
    "\n",
    "plt.plot(df['Vz (m/s)'].rolling(12).std())\n",
    "plt.figure()\n",
    "plt.plot(df['Vz (m/s)'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T04:08:10.858665Z",
     "start_time": "2024-03-09T04:08:10.681657Z"
    }
   },
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
