{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T22:37:24.460672217Z",
     "start_time": "2024-02-19T22:37:24.441269163Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fastcore.basics import Path, AttrDict\n",
    "from dataset import SPLID\n",
    "import torch\n",
    "\n",
    "import os\n",
    "config = AttrDict(\n",
    "    challenge_data_dir = Path('~/Projects/splid-comp/dataset').expanduser(),\n",
    "    valid_ratio = 0.1,\n",
    "    kernel_size = 5,\n",
    "    tolerance= 6, # Default evaluation tolerance\n",
    ")\n",
    "\n",
    "# Define the directory paths\n",
    "train_data_dir = config.challenge_data_dir / \"train_v2\"\n",
    "\n",
    "# Load the ground truth data\n",
    "ground_truth = config.challenge_data_dir / 'train_labels_v2.csv'\n",
    "\n",
    "datalist = []\n",
    "\n",
    "# Searching for training data within the dataset folder\n",
    "for file in os.listdir(train_data_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        datalist.append(os.path.join(train_data_dir, file))\n",
    "\n",
    "# Sort the training data and labels\n",
    "datalist = sorted(datalist, key=lambda i: int(os.path.splitext(os.path.basename(i))[0]))\n",
    "    \n",
    "\n",
    "train_datalist, test_datalist = train_test_split(datalist, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T23:29:12.679711769Z",
     "start_time": "2024-02-19T22:37:44.799833380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1520 files...\n",
      "Loaded file 0 of 1520\n",
      "Loaded file 50 of 1520\n",
      "Loaded file 100 of 1520\n",
      "Loaded file 150 of 1520\n",
      "Loaded file 200 of 1520\n",
      "Loaded file 250 of 1520\n",
      "Loaded file 300 of 1520\n",
      "Loaded file 350 of 1520\n",
      "Loaded file 400 of 1520\n",
      "Loaded file 450 of 1520\n",
      "Loaded file 500 of 1520\n",
      "Loaded file 550 of 1520\n",
      "Loaded file 600 of 1520\n",
      "Loaded file 650 of 1520\n",
      "Loaded file 700 of 1520\n",
      "Loaded file 750 of 1520\n",
      "Loaded file 800 of 1520\n",
      "Loaded file 850 of 1520\n",
      "Loaded file 900 of 1520\n",
      "Loaded file 950 of 1520\n",
      "Loaded file 1000 of 1520\n",
      "Loaded file 1050 of 1520\n",
      "Loaded file 1100 of 1520\n",
      "Loaded file 1150 of 1520\n",
      "Loaded file 1200 of 1520\n",
      "Loaded file 1250 of 1520\n",
      "Loaded file 1300 of 1520\n",
      "Loaded file 1350 of 1520\n",
      "Loaded file 1400 of 1520\n",
      "Loaded file 1450 of 1520\n",
      "Loaded file 1500 of 1520\n",
      "Joining dataframes...\n",
      "Done!\n",
      "Loading 380 files...\n",
      "Loaded file 0 of 380\n",
      "Loaded file 50 of 380\n",
      "Loaded file 100 of 380\n",
      "Loaded file 150 of 380\n",
      "Loaded file 200 of 380\n",
      "Loaded file 250 of 380\n",
      "Loaded file 300 of 380\n",
      "Loaded file 350 of 380\n",
      "Joining dataframes...\n",
      "Done!\n",
      "Start model training\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 6.900020370483398\n",
      "  batch 100 loss: 6.7368394947052\n",
      "  batch 150 loss: 6.235457315444946\n",
      "LOSS train 6.235457315444946 valid 5.875463648846275\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 5.656241235733032\n",
      "  batch 100 loss: 5.2400080108642575\n",
      "  batch 150 loss: 5.071345615386963\n",
      "LOSS train 5.071345615386963 valid 4.967586529882331\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 4.929323329925537\n",
      "  batch 100 loss: 4.736621203422547\n",
      "  batch 150 loss: 4.667303218841552\n",
      "LOSS train 4.667303218841552 valid 4.490513261995818\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 4.464949398040772\n",
      "  batch 100 loss: 4.259348998069763\n",
      "  batch 150 loss: 4.19341881275177\n",
      "LOSS train 4.19341881275177 valid 3.921198581394396\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 3.9902046394348143\n",
      "  batch 100 loss: 3.752190742492676\n",
      "  batch 150 loss: 3.775519137382507\n",
      "LOSS train 3.775519137382507 valid 3.5012419600235787\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 3.523346939086914\n",
      "  batch 100 loss: 3.407092547416687\n",
      "  batch 150 loss: 3.3345985269546508\n",
      "LOSS train 3.3345985269546508 valid 3.311008378079063\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 3.1874298095703124\n",
      "  batch 100 loss: 3.3193352317810056\n",
      "  batch 150 loss: 3.3034433221817014\n",
      "LOSS train 3.3034433221817014 valid 3.1191741228103638\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 3.0576376152038574\n",
      "  batch 100 loss: 3.1969401741027834\n",
      "  batch 150 loss: 3.101672306060791\n",
      "LOSS train 3.101672306060791 valid 2.9772128933354427\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 3.0324625730514527\n",
      "  batch 100 loss: 2.8926169300079345\n",
      "  batch 150 loss: 3.1843713784217833\n",
      "LOSS train 3.1843713784217833 valid 2.8949476668709204\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 2.9220424604415896\n",
      "  batch 100 loss: 2.9742599606513975\n",
      "  batch 150 loss: 2.8526300382614136\n",
      "LOSS train 2.8526300382614136 valid 2.8005303364051017\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 2.826131706237793\n",
      "  batch 100 loss: 2.9810062408447267\n",
      "  batch 150 loss: 2.760761466026306\n",
      "LOSS train 2.760761466026306 valid 2.7799121737480164\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 2.6640052342414857\n",
      "  batch 100 loss: 2.8839336228370667\n",
      "  batch 150 loss: 2.8703114986419678\n",
      "LOSS train 2.8703114986419678 valid 2.7544484452197424\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 2.6639180612564086\n",
      "  batch 100 loss: 2.777121925354004\n",
      "  batch 150 loss: 2.711536993980408\n",
      "LOSS train 2.711536993980408 valid 2.6854955836346277\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 2.7740024876594545\n",
      "  batch 100 loss: 2.6396539926528932\n",
      "  batch 150 loss: 2.7576988673210145\n",
      "LOSS train 2.7576988673210145 valid 2.647020154877713\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 2.6225945425033568\n",
      "  batch 100 loss: 2.7376302814483644\n",
      "  batch 150 loss: 2.5628603219985964\n",
      "LOSS train 2.5628603219985964 valid 2.6528073643383228\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 2.6580907845497133\n",
      "  batch 100 loss: 2.5367860412597656\n",
      "  batch 150 loss: 2.565572636127472\n",
      "LOSS train 2.565572636127472 valid 2.554363821682177\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 2.4379043316841127\n",
      "  batch 100 loss: 2.5995572352409364\n",
      "  batch 150 loss: 2.654955470561981\n",
      "LOSS train 2.654955470561981 valid 2.5205879838843095\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 2.5286670923233032\n",
      "  batch 100 loss: 2.5178046822547913\n",
      "  batch 150 loss: 2.4010438895225525\n",
      "LOSS train 2.4010438895225525 valid 2.5008249125982585\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 2.564590139389038\n",
      "  batch 100 loss: 2.4716115260124205\n",
      "  batch 150 loss: 2.415607576370239\n",
      "LOSS train 2.415607576370239 valid 2.4605226861803153\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 2.4351547145843506\n",
      "  batch 100 loss: 2.393001458644867\n",
      "  batch 150 loss: 2.4648116660118102\n",
      "LOSS train 2.4648116660118102 valid 2.4665798074320744\n",
      "EPOCH 21:\n",
      "  batch 50 loss: 2.406338105201721\n",
      "  batch 100 loss: 2.3992863082885743\n",
      "  batch 150 loss: 2.440931758880615\n",
      "LOSS train 2.440931758880615 valid 2.363168710156491\n",
      "EPOCH 22:\n",
      "  batch 50 loss: 2.3949943828582763\n",
      "  batch 100 loss: 2.4604236841201783\n",
      "  batch 150 loss: 2.3347963523864745\n",
      "LOSS train 2.3347963523864745 valid 2.318053091827192\n",
      "EPOCH 23:\n",
      "  batch 50 loss: 2.4165739941596986\n",
      "  batch 100 loss: 2.262806680202484\n",
      "  batch 150 loss: 2.3379493498802186\n",
      "LOSS train 2.3379493498802186 valid 2.4964252741713273\n",
      "EPOCH 24:\n",
      "  batch 50 loss: 2.397849588394165\n",
      "  batch 100 loss: 2.38895277261734\n",
      "  batch 150 loss: 2.2374470257759094\n",
      "LOSS train 2.2374470257759094 valid 2.255449009569068\n",
      "EPOCH 25:\n",
      "  batch 50 loss: 2.1567338585853575\n",
      "  batch 100 loss: 2.359024670124054\n",
      "  batch 150 loss: 2.2525905394554138\n",
      "LOSS train 2.2525905394554138 valid 2.238719723726574\n",
      "EPOCH 26:\n",
      "  batch 50 loss: 2.2464632773399353\n",
      "  batch 100 loss: 2.396157217025757\n",
      "  batch 150 loss: 2.1793055629730222\n",
      "LOSS train 2.1793055629730222 valid 2.1962000884507833\n",
      "EPOCH 27:\n",
      "  batch 50 loss: 2.2893025159835814\n",
      "  batch 100 loss: 2.15076327085495\n",
      "  batch 150 loss: 2.241565225124359\n",
      "LOSS train 2.241565225124359 valid 2.225479427136873\n",
      "EPOCH 28:\n",
      "  batch 50 loss: 2.137034862041473\n",
      "  batch 100 loss: 2.1905744814872743\n",
      "  batch 150 loss: 2.1549216675758363\n",
      "LOSS train 2.1549216675758363 valid 2.149882558145021\n",
      "EPOCH 29:\n",
      "  batch 50 loss: 2.3239432644844054\n",
      "  batch 100 loss: 2.047176167964935\n",
      "  batch 150 loss: 2.0476319766044617\n",
      "LOSS train 2.0476319766044617 valid 2.102077587654716\n",
      "EPOCH 30:\n",
      "  batch 50 loss: 2.081390178203583\n",
      "  batch 100 loss: 2.160246832370758\n",
      "  batch 150 loss: 2.1612865471839906\n",
      "LOSS train 2.1612865471839906 valid 2.1068601953355888\n",
      "EPOCH 31:\n",
      "  batch 50 loss: 2.110510880947113\n",
      "  batch 100 loss: 2.0488205265998842\n",
      "  batch 150 loss: 2.120663549900055\n",
      "LOSS train 2.120663549900055 valid 2.094743201607152\n",
      "EPOCH 32:\n",
      "  batch 50 loss: 2.132900478839874\n",
      "  batch 100 loss: 2.0907294392585754\n",
      "  batch 150 loss: 2.044437267780304\n",
      "LOSS train 2.044437267780304 valid 2.0301979780197144\n",
      "EPOCH 33:\n",
      "  batch 50 loss: 2.0955657982826232\n",
      "  batch 100 loss: 2.0192020201683043\n",
      "  batch 150 loss: 2.1026186513900758\n",
      "LOSS train 2.1026186513900758 valid 2.061496154258126\n",
      "EPOCH 34:\n",
      "  batch 50 loss: 1.969654619693756\n",
      "  batch 100 loss: 2.1253886079788207\n",
      "  batch 150 loss: 2.043934121131897\n",
      "LOSS train 2.043934121131897 valid 2.069683921964545\n",
      "EPOCH 35:\n",
      "  batch 50 loss: 2.1282893371582032\n",
      "  batch 100 loss: 1.95105783700943\n",
      "  batch 150 loss: 2.034862711429596\n",
      "LOSS train 2.034862711429596 valid 2.133708202525189\n",
      "EPOCH 36:\n",
      "  batch 50 loss: 1.9483107399940491\n",
      "  batch 100 loss: 2.0991284680366515\n",
      "  batch 150 loss: 2.000331389904022\n",
      "LOSS train 2.000331389904022 valid 2.0001803260100517\n",
      "EPOCH 37:\n",
      "  batch 50 loss: 1.9291444039344787\n",
      "  batch 100 loss: 2.0058153200149538\n",
      "  batch 150 loss: 1.915763943195343\n",
      "LOSS train 1.915763943195343 valid 2.013189362852197\n",
      "EPOCH 38:\n",
      "  batch 50 loss: 2.008127508163452\n",
      "  batch 100 loss: 2.014120898246765\n",
      "  batch 150 loss: 1.9367379450798035\n",
      "LOSS train 1.9367379450798035 valid 1.994675796282919\n",
      "EPOCH 39:\n",
      "  batch 50 loss: 1.9204577660560609\n",
      "  batch 100 loss: 2.01465030670166\n",
      "  batch 150 loss: 1.9000758171081542\n",
      "LOSS train 1.9000758171081542 valid 1.940876000805905\n",
      "EPOCH 40:\n",
      "  batch 50 loss: 2.0233559799194336\n",
      "  batch 100 loss: 1.7636994159221648\n",
      "  batch 150 loss: 1.9521788811683656\n",
      "LOSS train 1.9521788811683656 valid 1.9027911581491168\n",
      "EPOCH 41:\n",
      "  batch 50 loss: 1.8947953271865845\n",
      "  batch 100 loss: 2.0096570253372192\n",
      "  batch 150 loss: 1.8500494241714478\n",
      "LOSS train 1.8500494241714478 valid 1.892468643815894\n",
      "EPOCH 42:\n",
      "  batch 50 loss: 1.9049231815338135\n",
      "  batch 100 loss: 1.9297573685646057\n",
      "  batch 150 loss: 1.8385672068595886\n",
      "LOSS train 1.8385672068595886 valid 1.888036963186766\n",
      "EPOCH 43:\n",
      "  batch 50 loss: 1.8682387948036194\n",
      "  batch 100 loss: 2.0450458550453186\n",
      "  batch 150 loss: 1.8066328692436218\n",
      "LOSS train 1.8066328692436218 valid 1.8621463963859959\n",
      "EPOCH 44:\n",
      "  batch 50 loss: 1.905397403240204\n",
      "  batch 100 loss: 1.8311014080047607\n",
      "  batch 150 loss: 1.8186632704734802\n",
      "LOSS train 1.8186632704734802 valid 1.876766628340671\n",
      "EPOCH 45:\n",
      "  batch 50 loss: 1.932094280719757\n",
      "  batch 100 loss: 1.8583581161499023\n",
      "  batch 150 loss: 1.9296267318725586\n",
      "LOSS train 1.9296267318725586 valid 1.8456731971941496\n",
      "EPOCH 46:\n",
      "  batch 50 loss: 1.837166826725006\n",
      "  batch 100 loss: 1.7898161172866822\n",
      "  batch 150 loss: 1.835862205028534\n",
      "LOSS train 1.835862205028534 valid 1.851009034796765\n",
      "EPOCH 47:\n",
      "  batch 50 loss: 1.8077117133140563\n",
      "  batch 100 loss: 1.834153950214386\n",
      "  batch 150 loss: 1.886259753704071\n",
      "LOSS train 1.886259753704071 valid 1.8323437979346828\n",
      "EPOCH 48:\n",
      "  batch 50 loss: 1.8924679803848266\n",
      "  batch 100 loss: 1.7701750373840333\n",
      "  batch 150 loss: 1.8509013295173644\n",
      "LOSS train 1.8509013295173644 valid 1.8323404616431187\n",
      "EPOCH 49:\n",
      "  batch 50 loss: 1.6874576091766358\n",
      "  batch 100 loss: 1.8765483951568604\n",
      "  batch 150 loss: 1.8013663983345032\n",
      "LOSS train 1.8013663983345032 valid 1.8367676907464077\n",
      "EPOCH 50:\n",
      "  batch 50 loss: 1.888808672428131\n",
      "  batch 100 loss: 1.7717364025115967\n",
      "  batch 150 loss: 1.7368861627578736\n",
      "LOSS train 1.7368861627578736 valid 1.807353116964039\n",
      "EPOCH 51:\n",
      "  batch 50 loss: 1.7829173481464387\n",
      "  batch 100 loss: 1.833118052482605\n",
      "  batch 150 loss: 1.7218744003772735\n",
      "LOSS train 1.7218744003772735 valid 1.78957526307357\n",
      "EPOCH 52:\n",
      "  batch 50 loss: 1.7610789442062378\n",
      "  batch 100 loss: 1.785975887775421\n",
      "  batch 150 loss: 1.7225478744506837\n",
      "LOSS train 1.7225478744506837 valid 1.8313183376663609\n",
      "EPOCH 53:\n",
      "  batch 50 loss: 1.7454431414604188\n",
      "  batch 100 loss: 1.8098869848251342\n",
      "  batch 150 loss: 1.7758164930343627\n",
      "LOSS train 1.7758164930343627 valid 1.82718163258151\n",
      "EPOCH 54:\n",
      "  batch 50 loss: 1.7612210941314697\n",
      "  batch 100 loss: 1.7319407331943513\n",
      "  batch 150 loss: 1.797507495880127\n",
      "LOSS train 1.797507495880127 valid 1.7677056005126552\n",
      "EPOCH 55:\n",
      "  batch 50 loss: 1.669472050666809\n",
      "  batch 100 loss: 1.8083624172210693\n",
      "  batch 150 loss: 1.6615996384620666\n",
      "LOSS train 1.6615996384620666 valid 1.730776874642623\n",
      "EPOCH 56:\n",
      "  batch 50 loss: 1.6533532059192657\n",
      "  batch 100 loss: 1.7368652963638305\n",
      "  batch 150 loss: 1.7648283863067626\n",
      "LOSS train 1.7648283863067626 valid 1.7618485406825417\n",
      "EPOCH 57:\n",
      "  batch 50 loss: 1.76111843585968\n",
      "  batch 100 loss: 1.738066086769104\n",
      "  batch 150 loss: 1.6133824133872985\n",
      "LOSS train 1.6133824133872985 valid 1.7194695566829883\n",
      "EPOCH 58:\n",
      "  batch 50 loss: 1.744436593055725\n",
      "  batch 100 loss: 1.6278789401054383\n",
      "  batch 150 loss: 1.6229939424991608\n",
      "LOSS train 1.6229939424991608 valid 1.7713350935986167\n",
      "EPOCH 59:\n",
      "  batch 50 loss: 1.7890482711791993\n",
      "  batch 100 loss: 1.5497564959526062\n",
      "  batch 150 loss: 1.7815905356407165\n",
      "LOSS train 1.7815905356407165 valid 1.7366632662321393\n",
      "EPOCH 60:\n",
      "  batch 50 loss: 1.6623233485221862\n",
      "  batch 100 loss: 1.6746730041503906\n",
      "  batch 150 loss: 1.6434450495243071\n",
      "LOSS train 1.6434450495243071 valid 1.7645353580776013\n",
      "EPOCH 61:\n",
      "  batch 50 loss: 1.6111849343776703\n",
      "  batch 100 loss: 1.650156786441803\n",
      "  batch 150 loss: 1.6815365421772004\n",
      "LOSS train 1.6815365421772004 valid 1.6664667600079586\n",
      "EPOCH 62:\n",
      "  batch 50 loss: 1.630735410451889\n",
      "  batch 100 loss: 1.6647168147563933\n",
      "  batch 150 loss: 1.7249124264717102\n",
      "LOSS train 1.7249124264717102 valid 1.6641426996180886\n",
      "EPOCH 63:\n",
      "  batch 50 loss: 1.656029372215271\n",
      "  batch 100 loss: 1.6603248882293702\n",
      "  batch 150 loss: 1.6046729147434236\n",
      "LOSS train 1.6046729147434236 valid 1.7482210617316396\n",
      "EPOCH 64:\n",
      "  batch 50 loss: 1.6419781517982484\n",
      "  batch 100 loss: 1.5528560400009155\n",
      "  batch 150 loss: 1.6682348108291627\n",
      "LOSS train 1.6682348108291627 valid 1.7915780826618797\n",
      "EPOCH 65:\n",
      "  batch 50 loss: 1.6796127581596374\n",
      "  batch 100 loss: 1.6125080537796022\n",
      "  batch 150 loss: 1.644130401611328\n",
      "LOSS train 1.644130401611328 valid 1.7009407219133879\n",
      "EPOCH 66:\n",
      "  batch 50 loss: 1.6008361744880677\n",
      "  batch 100 loss: 1.6603087961673737\n",
      "  batch 150 loss: 1.6808803129196166\n",
      "LOSS train 1.6808803129196166 valid 1.7236686097948175\n",
      "EPOCH 67:\n",
      "  batch 50 loss: 1.6836286199092865\n",
      "  batch 100 loss: 1.6219632363319396\n",
      "  batch 150 loss: 1.6736184561252594\n",
      "LOSS train 1.6736184561252594 valid 1.7030120664521267\n",
      "EPOCH 68:\n",
      "  batch 50 loss: 1.6864580988883973\n",
      "  batch 100 loss: 1.661495840549469\n",
      "  batch 150 loss: 1.4318824315071106\n",
      "LOSS train 1.4318824315071106 valid 1.6902168863698055\n",
      "EPOCH 69:\n",
      "  batch 50 loss: 1.5139927768707275\n",
      "  batch 100 loss: 1.5253093552589416\n",
      "  batch 150 loss: 1.6500650668144226\n",
      "LOSS train 1.6500650668144226 valid 1.6260040449468713\n",
      "EPOCH 70:\n",
      "  batch 50 loss: 1.4201566660404206\n",
      "  batch 100 loss: 1.6242773723602295\n",
      "  batch 150 loss: 1.6174819433689118\n",
      "LOSS train 1.6174819433689118 valid 1.6471445858478546\n",
      "EPOCH 71:\n",
      "  batch 50 loss: 1.5347351586818696\n",
      "  batch 100 loss: 1.5884009575843812\n",
      "  batch 150 loss: 1.5183835017681122\n",
      "LOSS train 1.5183835017681122 valid 1.614885665868458\n",
      "EPOCH 72:\n",
      "  batch 50 loss: 1.5263119864463806\n",
      "  batch 100 loss: 1.6009259498119355\n",
      "  batch 150 loss: 1.5818025875091553\n",
      "LOSS train 1.5818025875091553 valid 1.622972784858001\n",
      "EPOCH 73:\n",
      "  batch 50 loss: 1.5275478661060333\n",
      "  batch 100 loss: 1.575050630569458\n",
      "  batch 150 loss: 1.5583788061141968\n",
      "LOSS train 1.5583788061141968 valid 1.6584347298270778\n",
      "EPOCH 74:\n",
      "  batch 50 loss: 1.4524547672271728\n",
      "  batch 100 loss: 1.5801227068901063\n",
      "  batch 150 loss: 1.5068051207065583\n",
      "LOSS train 1.5068051207065583 valid 1.620183433357038\n",
      "EPOCH 75:\n",
      "  batch 50 loss: 1.5262459480762482\n",
      "  batch 100 loss: 1.519667879343033\n",
      "  batch 150 loss: 1.4676478791236878\n",
      "LOSS train 1.4676478791236878 valid 1.6033306294365932\n",
      "EPOCH 76:\n",
      "  batch 50 loss: 1.5427217662334443\n",
      "  batch 100 loss: 1.5810667479038238\n",
      "  batch 150 loss: 1.4530914199352265\n",
      "LOSS train 1.4530914199352265 valid 1.5955266842716618\n",
      "EPOCH 77:\n",
      "  batch 50 loss: 1.6807205200195312\n",
      "  batch 100 loss: 1.5573913180828094\n",
      "  batch 150 loss: 1.4247139835357665\n",
      "LOSS train 1.4247139835357665 valid 1.6398104821380817\n",
      "EPOCH 78:\n",
      "  batch 50 loss: 1.4362621772289277\n",
      "  batch 100 loss: 1.5808506739139556\n",
      "  batch 150 loss: 1.5233541917800903\n",
      "LOSS train 1.5233541917800903 valid 1.6367455658159757\n",
      "EPOCH 79:\n",
      "  batch 50 loss: 1.5619224524497985\n",
      "  batch 100 loss: 1.5142683637142182\n",
      "  batch 150 loss: 1.5011499917507172\n",
      "LOSS train 1.5011499917507172 valid 1.5790447875073081\n",
      "EPOCH 80:\n",
      "  batch 50 loss: 1.4738944864273071\n",
      "  batch 100 loss: 1.4447930598258971\n",
      "  batch 150 loss: 1.5725507426261902\n",
      "LOSS train 1.5725507426261902 valid 1.6148148304537724\n",
      "EPOCH 81:\n",
      "  batch 50 loss: 1.5815946090221404\n",
      "  batch 100 loss: 1.5318242931365966\n",
      "  batch 150 loss: 1.4747512900829316\n",
      "LOSS train 1.4747512900829316 valid 1.558416706951041\n",
      "EPOCH 82:\n",
      "  batch 50 loss: 1.4849102115631103\n",
      "  batch 100 loss: 1.544534970521927\n",
      "  batch 150 loss: 1.4714916336536408\n",
      "LOSS train 1.4714916336536408 valid 1.673513230524565\n",
      "EPOCH 83:\n",
      "  batch 50 loss: 1.5470481884479523\n",
      "  batch 100 loss: 1.5176450419425964\n",
      "  batch 150 loss: 1.4209595036506653\n",
      "LOSS train 1.4209595036506653 valid 1.5711063679895902\n",
      "EPOCH 84:\n",
      "  batch 50 loss: 1.4288235104084015\n",
      "  batch 100 loss: 1.3904305458068849\n",
      "  batch 150 loss: 1.4819605958461761\n",
      "LOSS train 1.4819605958461761 valid 1.5780328716102399\n",
      "EPOCH 85:\n",
      "  batch 50 loss: 1.502471271753311\n",
      "  batch 100 loss: 1.4385300886631012\n",
      "  batch 150 loss: 1.4712178659439088\n",
      "LOSS train 1.4712178659439088 valid 1.5445459798762673\n",
      "EPOCH 86:\n",
      "  batch 50 loss: 1.4176354455947875\n",
      "  batch 100 loss: 1.498143471479416\n",
      "  batch 150 loss: 1.4604661655426026\n",
      "LOSS train 1.4604661655426026 valid 1.5661044826633053\n",
      "EPOCH 87:\n",
      "  batch 50 loss: 1.4143112206459045\n",
      "  batch 100 loss: 1.47556156873703\n",
      "  batch 150 loss: 1.4251933157444001\n",
      "LOSS train 1.4251933157444001 valid 1.5813842902058048\n",
      "EPOCH 88:\n",
      "  batch 50 loss: 1.3248334693908692\n",
      "  batch 100 loss: 1.4752755379676818\n",
      "  batch 150 loss: 1.4921281480789184\n",
      "LOSS train 1.4921281480789184 valid 1.5327019079735404\n",
      "EPOCH 89:\n",
      "  batch 50 loss: 1.3846595799922943\n",
      "  batch 100 loss: 1.5252720093727112\n",
      "  batch 150 loss: 1.3894243371486663\n",
      "LOSS train 1.3894243371486663 valid 1.6110063336397473\n",
      "EPOCH 90:\n",
      "  batch 50 loss: 1.39715083360672\n",
      "  batch 100 loss: 1.3180938458442688\n",
      "  batch 150 loss: 1.4718389046192168\n",
      "LOSS train 1.4718389046192168 valid 1.5081736665023\n",
      "EPOCH 91:\n",
      "  batch 50 loss: 1.4074843096733094\n",
      "  batch 100 loss: 1.4501631367206573\n",
      "  batch 150 loss: 1.4146235406398773\n",
      "LOSS train 1.4146235406398773 valid 1.5621130654686375\n",
      "EPOCH 92:\n",
      "  batch 50 loss: 1.4016182374954225\n",
      "  batch 100 loss: 1.3960938382148742\n",
      "  batch 150 loss: 1.4899269795417787\n",
      "LOSS train 1.4899269795417787 valid 1.5048253395055469\n",
      "EPOCH 93:\n",
      "  batch 50 loss: 1.3849704456329346\n",
      "  batch 100 loss: 1.4099705588817597\n",
      "  batch 150 loss: 1.4287846958637238\n",
      "LOSS train 1.4287846958637238 valid 1.5090861430293636\n",
      "EPOCH 94:\n",
      "  batch 50 loss: 1.394135445356369\n",
      "  batch 100 loss: 1.4079428255558013\n",
      "  batch 150 loss: 1.409422723054886\n",
      "LOSS train 1.409422723054886 valid 1.465663298180229\n",
      "EPOCH 95:\n",
      "  batch 50 loss: 1.3180388295650483\n",
      "  batch 100 loss: 1.3253927505016327\n",
      "  batch 150 loss: 1.3971907114982605\n",
      "LOSS train 1.3971907114982605 valid 1.4907720575207157\n",
      "EPOCH 96:\n",
      "  batch 50 loss: 1.4158272874355315\n",
      "  batch 100 loss: 1.4093616580963135\n",
      "  batch 150 loss: 1.3041936111450196\n",
      "LOSS train 1.3041936111450196 valid 1.5026534999671735\n",
      "EPOCH 97:\n",
      "  batch 50 loss: 1.315137597322464\n",
      "  batch 100 loss: 1.472214115858078\n",
      "  batch 150 loss: 1.3334499990940094\n",
      "LOSS train 1.3334499990940094 valid 1.4945906369309676\n",
      "EPOCH 98:\n",
      "  batch 50 loss: 1.3646520793437957\n",
      "  batch 100 loss: 1.3508136177062988\n",
      "  batch 150 loss: 1.3438158452510833\n",
      "LOSS train 1.3438158452510833 valid 1.4880666403393996\n",
      "EPOCH 99:\n",
      "  batch 50 loss: 1.3341877901554107\n",
      "  batch 100 loss: 1.4131563258171083\n",
      "  batch 150 loss: 1.4053329300880433\n",
      "LOSS train 1.4053329300880433 valid 1.5588656836434414\n",
      "EPOCH 100:\n",
      "  batch 50 loss: 1.3802638268470764\n",
      "  batch 100 loss: 1.390805253982544\n",
      "  batch 150 loss: 1.2275054168701172\n",
      "LOSS train 1.2275054168701172 valid 1.5078723665915037\n",
      "EPOCH 101:\n",
      "  batch 50 loss: 1.3464214503765106\n",
      "  batch 100 loss: 1.3769518887996675\n",
      "  batch 150 loss: 1.3230522382259369\n",
      "LOSS train 1.3230522382259369 valid 1.5179689259905564\n",
      "EPOCH 102:\n",
      "  batch 50 loss: 1.4273546195030213\n",
      "  batch 100 loss: 1.3116144514083863\n",
      "  batch 150 loss: 1.4015933465957642\n",
      "LOSS train 1.4015933465957642 valid 1.4408623780074872\n",
      "EPOCH 103:\n",
      "  batch 50 loss: 1.228486454486847\n",
      "  batch 100 loss: 1.3061066055297852\n",
      "  batch 150 loss: 1.2961411666870117\n",
      "LOSS train 1.2961411666870117 valid 1.4228475846742328\n",
      "EPOCH 104:\n",
      "  batch 50 loss: 1.214695543050766\n",
      "  batch 100 loss: 1.3013274002075195\n",
      "  batch 150 loss: 1.3947030305862427\n",
      "LOSS train 1.3947030305862427 valid 1.5922191017552425\n",
      "EPOCH 105:\n",
      "  batch 50 loss: 1.442571839094162\n",
      "  batch 100 loss: 1.2641458535194396\n",
      "  batch 150 loss: 1.3714261531829834\n",
      "LOSS train 1.3714261531829834 valid 1.458615911634345\n",
      "EPOCH 106:\n",
      "  batch 50 loss: 1.3280889666080475\n",
      "  batch 100 loss: 1.3477533292770385\n",
      "  batch 150 loss: 1.2726197493076326\n",
      "LOSS train 1.2726197493076326 valid 1.4447750621720363\n",
      "EPOCH 107:\n",
      "  batch 50 loss: 1.3671569406986237\n",
      "  batch 100 loss: 1.2419580698013306\n",
      "  batch 150 loss: 1.276803525686264\n",
      "LOSS train 1.276803525686264 valid 1.4389956781738682\n",
      "EPOCH 108:\n",
      "  batch 50 loss: 1.2767781507968903\n",
      "  batch 100 loss: 1.3057028400897979\n",
      "  batch 150 loss: 1.224424617290497\n",
      "LOSS train 1.224424617290497 valid 1.6391661371055402\n",
      "EPOCH 109:\n",
      "  batch 50 loss: 1.4180818939208983\n",
      "  batch 100 loss: 1.2901176822185516\n",
      "  batch 150 loss: 1.3058447813987732\n",
      "LOSS train 1.3058447813987732 valid 1.408981751454504\n",
      "EPOCH 110:\n",
      "  batch 50 loss: 1.2284058570861816\n",
      "  batch 100 loss: 1.3686126029491426\n",
      "  batch 150 loss: 1.2687556278705596\n",
      "LOSS train 1.2687556278705596 valid 1.4148778758550946\n",
      "EPOCH 111:\n",
      "  batch 50 loss: 1.2392237842082978\n",
      "  batch 100 loss: 1.2611328399181365\n",
      "  batch 150 loss: 1.3181052434444427\n",
      "LOSS train 1.3181052434444427 valid 1.4189680077527698\n",
      "EPOCH 112:\n",
      "  batch 50 loss: 1.1697528076171875\n",
      "  batch 100 loss: 1.326690936088562\n",
      "  batch 150 loss: 1.2719227266311646\n",
      "LOSS train 1.2719227266311646 valid 1.483445550266065\n",
      "EPOCH 113:\n",
      "  batch 50 loss: 1.3102400720119476\n",
      "  batch 100 loss: 1.2059178411960603\n",
      "  batch 150 loss: 1.277474020719528\n",
      "LOSS train 1.277474020719528 valid 1.3836456003941988\n",
      "EPOCH 114:\n",
      "  batch 50 loss: 1.2367188656330108\n",
      "  batch 100 loss: 1.2960650444030761\n",
      "  batch 150 loss: 1.2102770364284516\n",
      "LOSS train 1.2102770364284516 valid 1.418776716056623\n",
      "EPOCH 115:\n",
      "  batch 50 loss: 1.239681007862091\n",
      "  batch 100 loss: 1.402069435119629\n",
      "  batch 150 loss: 1.2692155718803406\n",
      "LOSS train 1.2692155718803406 valid 1.4660844128382833\n",
      "EPOCH 116:\n",
      "  batch 50 loss: 1.176796132326126\n",
      "  batch 100 loss: 1.2338267982006073\n",
      "  batch 150 loss: 1.2903871428966522\n",
      "LOSS train 1.2903871428966522 valid 1.4637864420288487\n",
      "EPOCH 117:\n",
      "  batch 50 loss: 1.223776286840439\n",
      "  batch 100 loss: 1.3304231202602386\n",
      "  batch 150 loss: 1.2777181303501128\n",
      "LOSS train 1.2777181303501128 valid 1.4194835675390143\n",
      "EPOCH 118:\n",
      "  batch 50 loss: 1.1915537011623383\n",
      "  batch 100 loss: 1.1822171533107757\n",
      "  batch 150 loss: 1.1461500477790834\n",
      "LOSS train 1.1461500477790834 valid 1.3830805326762952\n",
      "EPOCH 119:\n",
      "  batch 50 loss: 1.2402208125591279\n",
      "  batch 100 loss: 1.2402272057533263\n",
      "  batch 150 loss: 1.2023045492172242\n",
      "LOSS train 1.2023045492172242 valid 1.3874272500213825\n",
      "EPOCH 120:\n",
      "  batch 50 loss: 1.2128279554843902\n",
      "  batch 100 loss: 1.3266004228591919\n",
      "  batch 150 loss: 1.1404208660125732\n",
      "LOSS train 1.1404208660125732 valid 1.4230137875205593\n",
      "EPOCH 121:\n",
      "  batch 50 loss: 1.2611390447616577\n",
      "  batch 100 loss: 1.2284698188304901\n",
      "  batch 150 loss: 1.2336609065532684\n",
      "LOSS train 1.2336609065532684 valid 1.3506354956250441\n",
      "EPOCH 122:\n",
      "  batch 50 loss: 1.221379008293152\n",
      "  batch 100 loss: 1.3081763756275178\n",
      "  batch 150 loss: 1.1840559101104737\n",
      "LOSS train 1.1840559101104737 valid 1.4163669739898883\n",
      "EPOCH 123:\n",
      "  batch 50 loss: 1.251583482027054\n",
      "  batch 100 loss: 1.2100239038467406\n",
      "  batch 150 loss: 1.225788152217865\n",
      "LOSS train 1.225788152217865 valid 1.373447640946037\n",
      "EPOCH 124:\n",
      "  batch 50 loss: 1.1501711237430572\n",
      "  batch 100 loss: 1.178558912873268\n",
      "  batch 150 loss: 1.1938987648487092\n",
      "LOSS train 1.1938987648487092 valid 1.3479872722374766\n",
      "EPOCH 125:\n",
      "  batch 50 loss: 1.1460387313365936\n",
      "  batch 100 loss: 1.2421329379081727\n",
      "  batch 150 loss: 1.1912652742862702\n",
      "LOSS train 1.1912652742862702 valid 1.3890968372947292\n",
      "EPOCH 126:\n",
      "  batch 50 loss: 1.1836396384239196\n",
      "  batch 100 loss: 1.2463589799404144\n",
      "  batch 150 loss: 1.1781151902675628\n",
      "LOSS train 1.1781151902675628 valid 1.37592707809649\n",
      "EPOCH 127:\n",
      "  batch 50 loss: 1.1156032013893127\n",
      "  batch 100 loss: 1.275688214302063\n",
      "  batch 150 loss: 1.1719788491725922\n",
      "LOSS train 1.1719788491725922 valid 1.4150980052195097\n",
      "EPOCH 128:\n",
      "  batch 50 loss: 1.165225703716278\n",
      "  batch 100 loss: 1.1277390265464782\n",
      "  batch 150 loss: 1.2170684790611268\n",
      "LOSS train 1.2170684790611268 valid 1.3553374095966941\n",
      "EPOCH 129:\n",
      "  batch 50 loss: 1.285884211063385\n",
      "  batch 100 loss: 1.1931211173534393\n",
      "  batch 150 loss: 1.149013308286667\n",
      "LOSS train 1.149013308286667 valid 1.4172140093226182\n",
      "EPOCH 130:\n",
      "  batch 50 loss: 1.145757532119751\n",
      "  batch 100 loss: 1.176503562927246\n",
      "  batch 150 loss: 1.171789335012436\n",
      "LOSS train 1.171789335012436 valid 1.4037362337112427\n",
      "EPOCH 131:\n",
      "  batch 50 loss: 1.1006202912330627\n",
      "  batch 100 loss: 1.1012100636959077\n",
      "  batch 150 loss: 1.1628544926643372\n",
      "LOSS train 1.1628544926643372 valid 1.3350884632060402\n",
      "EPOCH 132:\n",
      "  batch 50 loss: 0.9962944030761719\n",
      "  batch 100 loss: 1.1935636281967164\n",
      "  batch 150 loss: 1.2287431454658508\n",
      "LOSS train 1.2287431454658508 valid 1.3546755023692783\n",
      "EPOCH 133:\n",
      "  batch 50 loss: 1.1424509263038636\n",
      "  batch 100 loss: 1.1699018478393555\n",
      "  batch 150 loss: 1.0837980473041535\n",
      "LOSS train 1.0837980473041535 valid 1.3870685398578644\n",
      "EPOCH 134:\n",
      "  batch 50 loss: 1.0861043524742127\n",
      "  batch 100 loss: 1.1418158090114594\n",
      "  batch 150 loss: 1.234267419576645\n",
      "LOSS train 1.234267419576645 valid 1.3487176675545542\n",
      "EPOCH 135:\n",
      "  batch 50 loss: 1.2188007128238678\n",
      "  batch 100 loss: 1.2097104781866073\n",
      "  batch 150 loss: 1.1478322732448578\n",
      "LOSS train 1.1478322732448578 valid 1.3455094663720382\n",
      "EPOCH 136:\n",
      "  batch 50 loss: 1.1324529361724853\n",
      "  batch 100 loss: 1.2056421142816545\n",
      "  batch 150 loss: 1.1902045154571532\n",
      "LOSS train 1.1902045154571532 valid 1.3475026127539182\n",
      "EPOCH 137:\n",
      "  batch 50 loss: 1.1757751667499543\n",
      "  batch 100 loss: 1.1534483098983765\n",
      "  batch 150 loss: 1.0764367687702179\n",
      "LOSS train 1.0764367687702179 valid 1.3469101394477643\n",
      "EPOCH 138:\n",
      "  batch 50 loss: 1.0854553472995758\n",
      "  batch 100 loss: 1.1777466809749604\n",
      "  batch 150 loss: 1.1595479774475097\n",
      "LOSS train 1.1595479774475097 valid 1.3359443537498776\n",
      "EPOCH 139:\n",
      "  batch 50 loss: 1.1140927982330322\n",
      "  batch 100 loss: 1.0519907343387604\n",
      "  batch 150 loss: 1.1207260656356812\n",
      "LOSS train 1.1207260656356812 valid 1.3949100939851058\n",
      "EPOCH 140:\n",
      "  batch 50 loss: 1.116576501727104\n",
      "  batch 100 loss: 1.0712458181381226\n",
      "  batch 150 loss: 1.160012196302414\n",
      "LOSS train 1.160012196302414 valid 1.3411895789598163\n",
      "EPOCH 141:\n",
      "  batch 50 loss: 1.1442218852043151\n",
      "  batch 100 loss: 1.1631844139099121\n",
      "  batch 150 loss: 1.0471819961071014\n",
      "LOSS train 1.0471819961071014 valid 1.3125158579725968\n",
      "EPOCH 142:\n",
      "  batch 50 loss: 1.0995225489139557\n",
      "  batch 100 loss: 1.1382045900821687\n",
      "  batch 150 loss: 1.0696801537275313\n",
      "LOSS train 1.0696801537275313 valid 1.337630431903036\n",
      "EPOCH 143:\n",
      "  batch 50 loss: 1.088252350091934\n",
      "  batch 100 loss: 1.0585714960098267\n",
      "  batch 150 loss: 1.1020150381326674\n",
      "LOSS train 1.1020150381326674 valid 1.336909565486406\n",
      "EPOCH 144:\n",
      "  batch 50 loss: 1.0763441836833954\n",
      "  batch 100 loss: 1.0932764303684235\n",
      "  batch 150 loss: 1.0818549048900605\n",
      "LOSS train 1.0818549048900605 valid 1.3806627910388143\n",
      "EPOCH 145:\n",
      "  batch 50 loss: 1.149881410598755\n",
      "  batch 100 loss: 1.0334620451927186\n",
      "  batch 150 loss: 1.1076590359210967\n",
      "LOSS train 1.1076590359210967 valid 1.3125082332836955\n",
      "EPOCH 146:\n",
      "  batch 50 loss: 1.0221379560232162\n",
      "  batch 100 loss: 1.1522704076766968\n",
      "  batch 150 loss: 1.1666877591609954\n",
      "LOSS train 1.1666877591609954 valid 1.3008954007374613\n",
      "EPOCH 147:\n",
      "  batch 50 loss: 1.061885962486267\n",
      "  batch 100 loss: 1.1013481938838958\n",
      "  batch 150 loss: 1.0866313469409943\n",
      "LOSS train 1.0866313469409943 valid 1.3516327770132768\n",
      "EPOCH 148:\n",
      "  batch 50 loss: 1.0522011518478394\n",
      "  batch 100 loss: 1.0453260666131974\n",
      "  batch 150 loss: 1.135364261865616\n",
      "LOSS train 1.135364261865616 valid 1.3300361005883468\n",
      "EPOCH 149:\n",
      "  batch 50 loss: 1.1013814449310302\n",
      "  batch 100 loss: 1.0354664701223373\n",
      "  batch 150 loss: 0.9607332837581635\n",
      "LOSS train 0.9607332837581635 valid 1.3502626607292576\n",
      "EPOCH 150:\n",
      "  batch 50 loss: 0.9844412624835968\n",
      "  batch 100 loss: 1.1208723127841949\n",
      "  batch 150 loss: 1.1207924604415893\n",
      "LOSS train 1.1207924604415893 valid 1.3096437250313007\n",
      "EPOCH 151:\n",
      "  batch 50 loss: 1.0114440035820007\n",
      "  batch 100 loss: 1.0594351661205292\n",
      "  batch 150 loss: 1.1658162665367127\n",
      "LOSS train 1.1658162665367127 valid 1.336471134110501\n",
      "EPOCH 152:\n",
      "  batch 50 loss: 1.0608988988399506\n",
      "  batch 100 loss: 1.0560979330539704\n",
      "  batch 150 loss: 1.0789566814899445\n",
      "LOSS train 1.0789566814899445 valid 1.2973584673906629\n",
      "EPOCH 153:\n",
      "  batch 50 loss: 1.0481596499681474\n",
      "  batch 100 loss: 1.0595168149471283\n",
      "  batch 150 loss: 1.073873609304428\n",
      "LOSS train 1.073873609304428 valid 1.314235541381334\n",
      "EPOCH 154:\n",
      "  batch 50 loss: 0.9888377815485001\n",
      "  batch 100 loss: 1.1020761501789094\n",
      "  batch 150 loss: 1.0412260591983795\n",
      "LOSS train 1.0412260591983795 valid 1.2583661299002797\n",
      "EPOCH 155:\n",
      "  batch 50 loss: 1.039927368760109\n",
      "  batch 100 loss: 1.1162265145778656\n",
      "  batch 150 loss: 1.0018792247772217\n",
      "LOSS train 1.0018792247772217 valid 1.3407788111975318\n",
      "EPOCH 156:\n",
      "  batch 50 loss: 1.023371490240097\n",
      "  batch 100 loss: 1.0732007145881652\n",
      "  batch 150 loss: 1.0073644179105758\n",
      "LOSS train 1.0073644179105758 valid 1.2650830486887379\n",
      "EPOCH 157:\n",
      "  batch 50 loss: 1.0940219891071319\n",
      "  batch 100 loss: 1.039481121301651\n",
      "  batch 150 loss: 1.002861145734787\n",
      "LOSS train 1.002861145734787 valid 1.3237625642826683\n",
      "EPOCH 158:\n",
      "  batch 50 loss: 1.0337489986419677\n",
      "  batch 100 loss: 1.0862307858467102\n",
      "  batch 150 loss: 1.1165688633918762\n",
      "LOSS train 1.1165688633918762 valid 1.3129118806437443\n",
      "EPOCH 159:\n",
      "  batch 50 loss: 1.036842799782753\n",
      "  batch 100 loss: 1.076612251996994\n",
      "  batch 150 loss: 0.9780991631746292\n",
      "LOSS train 0.9780991631746292 valid 1.2876148976777728\n",
      "EPOCH 160:\n",
      "  batch 50 loss: 0.9976845401525497\n",
      "  batch 100 loss: 1.0089627194404602\n",
      "  batch 150 loss: 1.0608893072605132\n",
      "LOSS train 1.0608893072605132 valid 1.307796481408571\n",
      "EPOCH 161:\n",
      "  batch 50 loss: 1.0612700939178468\n",
      "  batch 100 loss: 1.0641143786907197\n",
      "  batch 150 loss: 0.9622029161453247\n",
      "LOSS train 0.9622029161453247 valid 1.2714729089485972\n",
      "EPOCH 162:\n",
      "  batch 50 loss: 1.0405344253778457\n",
      "  batch 100 loss: 0.9391188693046569\n",
      "  batch 150 loss: 1.1024207079410553\n",
      "LOSS train 1.1024207079410553 valid 1.2248709162599163\n",
      "EPOCH 163:\n",
      "  batch 50 loss: 1.015499016046524\n",
      "  batch 100 loss: 1.0131992971897126\n",
      "  batch 150 loss: 0.9741610372066498\n",
      "LOSS train 0.9741610372066498 valid 1.2550642678612156\n",
      "EPOCH 164:\n",
      "  batch 50 loss: 1.0292866891622543\n",
      "  batch 100 loss: 0.9370375490188598\n",
      "  batch 150 loss: 1.0995271247625351\n",
      "LOSS train 1.0995271247625351 valid 1.294734347023462\n",
      "EPOCH 165:\n",
      "  batch 50 loss: 0.9601243889331817\n",
      "  batch 100 loss: 1.052286170721054\n",
      "  batch 150 loss: 1.0110200172662736\n",
      "LOSS train 1.0110200172662736 valid 1.2573498895293789\n",
      "EPOCH 166:\n",
      "  batch 50 loss: 0.9423112452030182\n",
      "  batch 100 loss: 0.9604474544525147\n",
      "  batch 150 loss: 1.0672505116462707\n",
      "LOSS train 1.0672505116462707 valid 1.2642353459408409\n",
      "EPOCH 167:\n",
      "  batch 50 loss: 1.0191340494155883\n",
      "  batch 100 loss: 0.9469862359762192\n",
      "  batch 150 loss: 0.989692153930664\n",
      "LOSS train 0.989692153930664 valid 1.2670388080571826\n",
      "EPOCH 168:\n",
      "  batch 50 loss: 0.9457809031009674\n",
      "  batch 100 loss: 0.9928131967782974\n",
      "  batch 150 loss: 1.0211365878582002\n",
      "LOSS train 1.0211365878582002 valid 1.2969851038957898\n",
      "EPOCH 169:\n",
      "  batch 50 loss: 0.9392711198329926\n",
      "  batch 100 loss: 1.0158907836675644\n",
      "  batch 150 loss: 0.9974152410030365\n",
      "LOSS train 0.9974152410030365 valid 1.258858887772811\n",
      "EPOCH 170:\n",
      "  batch 50 loss: 1.0170663392543793\n",
      "  batch 100 loss: 1.0603256571292876\n",
      "  batch 150 loss: 0.9319850212335586\n",
      "LOSS train 0.9319850212335586 valid 1.2361612892464588\n",
      "EPOCH 171:\n",
      "  batch 50 loss: 1.0620823043584824\n",
      "  batch 100 loss: 0.9700260484218597\n",
      "  batch 150 loss: 0.9419708186388016\n",
      "LOSS train 0.9419708186388016 valid 1.2467000390353955\n",
      "EPOCH 172:\n",
      "  batch 50 loss: 0.9151928931474685\n",
      "  batch 100 loss: 0.9778750294446945\n",
      "  batch 150 loss: 1.0034307545423509\n",
      "LOSS train 1.0034307545423509 valid 1.2829951957652443\n",
      "EPOCH 173:\n",
      "  batch 50 loss: 0.9567295843362809\n",
      "  batch 100 loss: 1.0561110466718673\n",
      "  batch 150 loss: 0.9762150889635086\n",
      "LOSS train 0.9762150889635086 valid 1.2797762202589136\n",
      "EPOCH 174:\n",
      "  batch 50 loss: 0.9363217914104461\n",
      "  batch 100 loss: 0.9956181359291076\n",
      "  batch 150 loss: 1.0318378853797912\n",
      "LOSS train 1.0318378853797912 valid 1.2620826865497388\n",
      "EPOCH 175:\n",
      "  batch 50 loss: 0.9749333322048187\n",
      "  batch 100 loss: 0.8957939374446869\n",
      "  batch 150 loss: 0.9000323188304901\n",
      "LOSS train 0.9000323188304901 valid 1.2169397975269116\n",
      "EPOCH 176:\n",
      "  batch 50 loss: 0.8762189257144928\n",
      "  batch 100 loss: 0.9564143520593643\n",
      "  batch 150 loss: 1.0016859024763107\n",
      "LOSS train 1.0016859024763107 valid 1.2437644773407985\n",
      "EPOCH 177:\n",
      "  batch 50 loss: 0.9464898639917374\n",
      "  batch 100 loss: 0.9517536830902099\n",
      "  batch 150 loss: 0.9622141247987748\n",
      "LOSS train 0.9622141247987748 valid 1.298010035565025\n",
      "EPOCH 178:\n",
      "  batch 50 loss: 0.9665178149938584\n",
      "  batch 100 loss: 0.9193382942676545\n",
      "  batch 150 loss: 0.9828753310441971\n",
      "LOSS train 0.9828753310441971 valid 1.2757702893332432\n",
      "EPOCH 179:\n",
      "  batch 50 loss: 0.9330817192792893\n",
      "  batch 100 loss: 0.9630729103088379\n",
      "  batch 150 loss: 0.9457677495479584\n",
      "LOSS train 0.9457677495479584 valid 1.2354821664722342\n",
      "EPOCH 180:\n",
      "  batch 50 loss: 0.9617229878902436\n",
      "  batch 100 loss: 0.9495097225904465\n",
      "  batch 150 loss: 0.9379468500614166\n",
      "LOSS train 0.9379468500614166 valid 1.2464000912089097\n",
      "EPOCH 181:\n",
      "  batch 50 loss: 0.93202172935009\n",
      "  batch 100 loss: 0.9031899249553681\n",
      "  batch 150 loss: 0.9543864047527313\n",
      "LOSS train 0.9543864047527313 valid 1.2041617757395695\n",
      "EPOCH 182:\n",
      "  batch 50 loss: 1.0957495379447937\n",
      "  batch 100 loss: 0.9407398235797882\n",
      "  batch 150 loss: 0.9368793380260467\n",
      "LOSS train 0.9368793380260467 valid 1.2087870679403607\n",
      "EPOCH 183:\n",
      "  batch 50 loss: 0.8415994685888291\n",
      "  batch 100 loss: 0.9276456040143967\n",
      "  batch 150 loss: 0.9989888262748718\n",
      "LOSS train 0.9989888262748718 valid 1.2144313470313424\n",
      "EPOCH 184:\n",
      "  batch 50 loss: 1.0013602596521378\n",
      "  batch 100 loss: 0.9201057422161102\n",
      "  batch 150 loss: 0.8703047823905945\n",
      "LOSS train 0.8703047823905945 valid 1.2175107065000033\n",
      "EPOCH 185:\n",
      "  batch 50 loss: 0.8884588944911956\n",
      "  batch 100 loss: 0.9336188703775405\n",
      "  batch 150 loss: 0.9655051153898239\n",
      "LOSS train 0.9655051153898239 valid 1.2151090918402923\n",
      "EPOCH 186:\n",
      "  batch 50 loss: 0.9577656984329224\n",
      "  batch 100 loss: 0.8898363548517227\n",
      "  batch 150 loss: 1.0794883048534394\n",
      "LOSS train 1.0794883048534394 valid 1.2950248247698735\n",
      "EPOCH 187:\n",
      "  batch 50 loss: 0.9709169781208038\n",
      "  batch 100 loss: 0.9093713974952697\n",
      "  batch 150 loss: 1.0095347011089324\n",
      "LOSS train 1.0095347011089324 valid 1.2717994678961604\n",
      "EPOCH 188:\n",
      "  batch 50 loss: 0.8974647605419159\n",
      "  batch 100 loss: 0.976620420217514\n",
      "  batch 150 loss: 0.9175230967998504\n",
      "LOSS train 0.9175230967998504 valid 1.2495047944156747\n",
      "EPOCH 189:\n",
      "  batch 50 loss: 0.8475336593389511\n",
      "  batch 100 loss: 0.9498557817935943\n",
      "  batch 150 loss: 0.9175752502679825\n",
      "LOSS train 0.9175752502679825 valid 1.2484542507874339\n",
      "EPOCH 190:\n",
      "  batch 50 loss: 0.8499011844396591\n",
      "  batch 100 loss: 0.8895424425601959\n",
      "  batch 150 loss: 0.9710841012001038\n",
      "LOSS train 0.9710841012001038 valid 1.2686978351128728\n",
      "EPOCH 191:\n",
      "  batch 50 loss: 0.8910545086860657\n",
      "  batch 100 loss: 0.8446287024021149\n",
      "  batch 150 loss: 0.981584821343422\n",
      "LOSS train 0.981584821343422 valid 1.250051979955874\n",
      "EPOCH 192:\n",
      "  batch 50 loss: 0.8763770878314971\n",
      "  batch 100 loss: 0.9582480466365815\n",
      "  batch 150 loss: 0.8576160776615143\n",
      "LOSS train 0.8576160776615143 valid 1.1735243577706187\n",
      "EPOCH 193:\n",
      "  batch 50 loss: 0.8924130260944366\n",
      "  batch 100 loss: 0.873119592666626\n",
      "  batch 150 loss: 0.8585710626840591\n",
      "LOSS train 0.8585710626840591 valid 1.1609511422483545\n",
      "EPOCH 194:\n",
      "  batch 50 loss: 0.8251709043979645\n",
      "  batch 100 loss: 0.8613643312454223\n",
      "  batch 150 loss: 0.8789362919330597\n",
      "LOSS train 0.8789362919330597 valid 1.2253076057685048\n",
      "EPOCH 195:\n",
      "  batch 50 loss: 0.8785651469230652\n",
      "  batch 100 loss: 0.8687243753671646\n",
      "  batch 150 loss: 0.8621445924043656\n",
      "LOSS train 0.8621445924043656 valid 1.2462524105059474\n",
      "EPOCH 196:\n",
      "  batch 50 loss: 0.908840708732605\n",
      "  batch 100 loss: 0.8786638516187668\n",
      "  batch 150 loss: 0.9940256756544114\n",
      "LOSS train 0.9940256756544114 valid 1.3957472686704837\n",
      "EPOCH 197:\n",
      "  batch 50 loss: 0.939189435839653\n",
      "  batch 100 loss: 0.9073981004953384\n",
      "  batch 150 loss: 0.9463846015930176\n",
      "LOSS train 0.9463846015930176 valid 1.2367173205865056\n",
      "EPOCH 198:\n",
      "  batch 50 loss: 0.8584810388088226\n",
      "  batch 100 loss: 0.9385362547636033\n",
      "  batch 150 loss: 0.9645518136024475\n",
      "LOSS train 0.9645518136024475 valid 1.2186614871025085\n",
      "EPOCH 199:\n",
      "  batch 50 loss: 0.8415126717090606\n",
      "  batch 100 loss: 0.8388531225919723\n",
      "  batch 150 loss: 0.9060806250572204\n",
      "LOSS train 0.9060806250572204 valid 1.1730145035605681\n",
      "EPOCH 200:\n",
      "  batch 50 loss: 0.8915748780965805\n",
      "  batch 100 loss: 0.8961466360092163\n",
      "  batch 150 loss: 0.9184447348117828\n",
      "LOSS train 0.9184447348117828 valid 1.2038483572633643\n",
      "EPOCH 201:\n",
      "  batch 50 loss: 0.8461149394512176\n",
      "  batch 100 loss: 0.842242106795311\n",
      "  batch 150 loss: 0.8823310816287995\n",
      "LOSS train 0.8823310816287995 valid 1.1863583318497006\n",
      "EPOCH 202:\n",
      "  batch 50 loss: 0.8378718537092209\n",
      "  batch 100 loss: 0.8972288423776626\n",
      "  batch 150 loss: 0.8322696423530579\n",
      "LOSS train 0.8322696423530579 valid 1.2248461254333194\n",
      "EPOCH 203:\n",
      "  batch 50 loss: 0.881325570344925\n",
      "  batch 100 loss: 0.8633754014968872\n",
      "  batch 150 loss: 0.8156972497701644\n",
      "LOSS train 0.8156972497701644 valid 1.247914063303094\n",
      "EPOCH 204:\n",
      "  batch 50 loss: 0.824532385468483\n",
      "  batch 100 loss: 0.9779042416810989\n",
      "  batch 150 loss: 0.9134470266103745\n",
      "LOSS train 0.9134470266103745 valid 1.2355875498370121\n",
      "EPOCH 205:\n",
      "  batch 50 loss: 0.8730755466222763\n",
      "  batch 100 loss: 0.9651967483758926\n",
      "  batch 150 loss: 0.8961595940589905\n",
      "LOSS train 0.8961595940589905 valid 1.2585069301881289\n",
      "EPOCH 206:\n",
      "  batch 50 loss: 0.8995037102699279\n",
      "  batch 100 loss: 0.914302659034729\n",
      "  batch 150 loss: 0.8902100622653961\n",
      "LOSS train 0.8902100622653961 valid 1.2331253439188004\n",
      "EPOCH 207:\n",
      "  batch 50 loss: 0.7584689950942993\n",
      "  batch 100 loss: 0.9111844557523727\n",
      "  batch 150 loss: 0.8866047096252442\n",
      "LOSS train 0.8866047096252442 valid 1.2123655629785437\n",
      "EPOCH 208:\n",
      "  batch 50 loss: 0.8798647952079773\n",
      "  batch 100 loss: 0.7972249919176102\n",
      "  batch 150 loss: 0.8656676828861236\n",
      "LOSS train 0.8656676828861236 valid 1.236312477996475\n",
      "EPOCH 209:\n",
      "  batch 50 loss: 0.90478136241436\n",
      "  batch 100 loss: 0.7608346122503281\n",
      "  batch 150 loss: 0.888979224562645\n",
      "LOSS train 0.888979224562645 valid 1.2442810198194103\n",
      "EPOCH 210:\n",
      "  batch 50 loss: 0.8283633548021316\n",
      "  batch 100 loss: 0.8005411636829376\n",
      "  batch 150 loss: 0.8329893058538437\n",
      "LOSS train 0.8329893058538437 valid 1.1907234262478978\n",
      "EPOCH 211:\n",
      "  batch 50 loss: 0.8354537665843964\n",
      "  batch 100 loss: 0.7833296418190002\n",
      "  batch 150 loss: 0.926656186580658\n",
      "LOSS train 0.926656186580658 valid 1.2062852359131764\n",
      "EPOCH 212:\n",
      "  batch 50 loss: 0.8125795757770539\n",
      "  batch 100 loss: 0.8531719905138015\n",
      "  batch 150 loss: 0.8533528137207032\n",
      "LOSS train 0.8533528137207032 valid 1.20844698187552\n",
      "EPOCH 213:\n",
      "  batch 50 loss: 0.8634614032506943\n",
      "  batch 100 loss: 0.8360168480873108\n",
      "  batch 150 loss: 0.8024191230535507\n",
      "LOSS train 0.8024191230535507 valid 1.1960272616461705\n",
      "EPOCH 214:\n",
      "  batch 50 loss: 0.7928785163164139\n",
      "  batch 100 loss: 0.7674523192644119\n",
      "  batch 150 loss: 0.8612951451539993\n",
      "LOSS train 0.8612951451539993 valid 1.2095499870024229\n",
      "EPOCH 215:\n",
      "  batch 50 loss: 0.854459987282753\n",
      "  batch 100 loss: 0.8220149147510528\n",
      "  batch 150 loss: 0.8147446274757385\n",
      "LOSS train 0.8147446274757385 valid 1.206015208834096\n",
      "EPOCH 216:\n",
      "  batch 50 loss: 0.8988228714466096\n",
      "  batch 100 loss: 0.8215645408630371\n",
      "  batch 150 loss: 0.8215606939792633\n",
      "LOSS train 0.8215606939792633 valid 1.202933738890447\n",
      "EPOCH 217:\n",
      "  batch 50 loss: 0.7743068897724151\n",
      "  batch 100 loss: 0.915964161157608\n",
      "  batch 150 loss: 0.7427738809585571\n",
      "LOSS train 0.7427738809585571 valid 1.1961548446040404\n",
      "EPOCH 218:\n",
      "  batch 50 loss: 0.7922962081432342\n",
      "  batch 100 loss: 0.821117548942566\n",
      "  batch 150 loss: 0.757070883512497\n",
      "LOSS train 0.757070883512497 valid 1.245246539774694\n",
      "EPOCH 219:\n",
      "  batch 50 loss: 0.8005871015787125\n",
      "  batch 100 loss: 0.8189183610677719\n",
      "  batch 150 loss: 0.962385030388832\n",
      "LOSS train 0.962385030388832 valid 1.1800890533547652\n",
      "EPOCH 220:\n",
      "  batch 50 loss: 0.8091339302062989\n",
      "  batch 100 loss: 0.8032244604825973\n",
      "  batch 150 loss: 0.8112592035531998\n",
      "LOSS train 0.8112592035531998 valid 1.2375540513741343\n",
      "EPOCH 221:\n",
      "  batch 50 loss: 0.8102321410179139\n",
      "  batch 100 loss: 0.7951366806030273\n",
      "  batch 150 loss: 0.7706560778617859\n",
      "LOSS train 0.7706560778617859 valid 1.2239310866908024\n",
      "EPOCH 222:\n",
      "  batch 50 loss: 0.80362140417099\n",
      "  batch 100 loss: 0.754071578681469\n",
      "  batch 150 loss: 0.8187202090024948\n",
      "LOSS train 0.8187202090024948 valid 1.2534692930547815\n",
      "EPOCH 223:\n",
      "  batch 50 loss: 0.8047932469844818\n",
      "  batch 100 loss: 0.8499773818254471\n",
      "  batch 150 loss: 0.8303767865896226\n",
      "LOSS train 0.8303767865896226 valid 1.191635143600012\n",
      "EPOCH 224:\n",
      "  batch 50 loss: 0.7568407016992569\n",
      "  batch 100 loss: 0.8270874381065368\n",
      "  batch 150 loss: 0.7767815434932709\n",
      "LOSS train 0.7767815434932709 valid 1.180089134919016\n",
      "EPOCH 225:\n",
      "  batch 50 loss: 0.8920212215185166\n",
      "  batch 100 loss: 0.7950490528345108\n",
      "  batch 150 loss: 0.7431688934564591\n",
      "LOSS train 0.7431688934564591 valid 1.2154842136721862\n",
      "EPOCH 226:\n",
      "  batch 50 loss: 0.8057346171140671\n",
      "  batch 100 loss: 0.8140181082487107\n",
      "  batch 150 loss: 0.7482619857788086\n",
      "LOSS train 0.7482619857788086 valid 1.2183626505889391\n",
      "EPOCH 227:\n",
      "  batch 50 loss: 0.795832233428955\n",
      "  batch 100 loss: 0.8183986055850982\n",
      "  batch 150 loss: 0.7694972735643387\n",
      "LOSS train 0.7694972735643387 valid 1.1781928319680064\n",
      "EPOCH 228:\n",
      "  batch 50 loss: 0.7529905432462692\n",
      "  batch 100 loss: 0.7907141733169556\n",
      "  batch 150 loss: 0.7865987133979797\n",
      "LOSS train 0.7865987133979797 valid 1.244483426997536\n",
      "EPOCH 229:\n",
      "  batch 50 loss: 0.7308111953735351\n",
      "  batch 100 loss: 0.811318365931511\n",
      "  batch 150 loss: 0.9224564242362976\n",
      "LOSS train 0.9224564242362976 valid 1.2458481020049046\n",
      "EPOCH 230:\n",
      "  batch 50 loss: 0.8421682414412498\n",
      "  batch 100 loss: 0.8357626175880433\n",
      "  batch 150 loss: 0.7906634086370468\n",
      "LOSS train 0.7906634086370468 valid 1.1815540037657086\n",
      "EPOCH 231:\n",
      "  batch 50 loss: 0.8494730716943741\n",
      "  batch 100 loss: 0.7896312075853348\n",
      "  batch 150 loss: 0.7966064018011093\n",
      "LOSS train 0.7966064018011093 valid 1.244313381220165\n",
      "EPOCH 232:\n",
      "  batch 50 loss: 0.7507153052091599\n",
      "  batch 100 loss: 0.8075978487730027\n",
      "  batch 150 loss: 0.7415337443351746\n",
      "LOSS train 0.7415337443351746 valid 1.1877313005296808\n",
      "EPOCH 233:\n",
      "  batch 50 loss: 0.7594688332080841\n",
      "  batch 100 loss: 0.8504755574464798\n",
      "  batch 150 loss: 0.8036434769630432\n",
      "LOSS train 0.8036434769630432 valid 1.2281285328300375\n",
      "EPOCH 234:\n",
      "  batch 50 loss: 0.7695707857608796\n",
      "  batch 100 loss: 0.7226347607374192\n",
      "  batch 150 loss: 0.7942368960380555\n",
      "LOSS train 0.7942368960380555 valid 1.2054065512983423\n",
      "EPOCH 235:\n",
      "  batch 50 loss: 0.7324085533618927\n",
      "  batch 100 loss: 0.7322596287727356\n",
      "  batch 150 loss: 0.7210133242607116\n",
      "LOSS train 0.7210133242607116 valid 1.1721462944620533\n",
      "EPOCH 236:\n",
      "  batch 50 loss: 0.7078467327356338\n",
      "  batch 100 loss: 0.8130141890048981\n",
      "  batch 150 loss: 0.7614067023992539\n",
      "LOSS train 0.7614067023992539 valid 1.2204379878546063\n",
      "EPOCH 237:\n",
      "  batch 50 loss: 0.7358704310655594\n",
      "  batch 100 loss: 0.7474654084444046\n",
      "  batch 150 loss: 0.7844848960638047\n",
      "LOSS train 0.7844848960638047 valid 1.1802487985083931\n",
      "EPOCH 238:\n",
      "  batch 50 loss: 0.7026805645227432\n",
      "  batch 100 loss: 0.8450354945659637\n",
      "  batch 150 loss: 0.748132073879242\n",
      "LOSS train 0.748132073879242 valid 1.1927364107809568\n",
      "EPOCH 239:\n",
      "  batch 50 loss: 0.7889131009578705\n",
      "  batch 100 loss: 0.7132920628786087\n",
      "  batch 150 loss: 0.7587944084405899\n",
      "LOSS train 0.7587944084405899 valid 1.1854937272636514\n",
      "EPOCH 240:\n",
      "  batch 50 loss: 0.70627674639225\n",
      "  batch 100 loss: 0.8260089033842086\n",
      "  batch 150 loss: 0.6977731031179428\n",
      "LOSS train 0.6977731031179428 valid 1.1718967258930206\n",
      "EPOCH 241:\n",
      "  batch 50 loss: 0.8363626584410667\n",
      "  batch 100 loss: 0.8288661736249924\n",
      "  batch 150 loss: 0.8081503480672836\n",
      "LOSS train 0.8081503480672836 valid 1.1448644117305153\n",
      "EPOCH 242:\n",
      "  batch 50 loss: 0.709197255373001\n",
      "  batch 100 loss: 0.7868730574846268\n",
      "  batch 150 loss: 0.7393183648586273\n",
      "LOSS train 0.7393183648586273 valid 1.1734011102663844\n",
      "EPOCH 243:\n",
      "  batch 50 loss: 0.85022500872612\n",
      "  batch 100 loss: 0.8114243507385254\n",
      "  batch 150 loss: 0.6920562994480133\n",
      "LOSS train 0.6920562994480133 valid 1.116294698495614\n",
      "EPOCH 244:\n",
      "  batch 50 loss: 0.7278031599521637\n",
      "  batch 100 loss: 0.669431961774826\n",
      "  batch 150 loss: 0.8828362900018693\n",
      "LOSS train 0.8828362900018693 valid 1.0935120919817372\n",
      "EPOCH 245:\n",
      "  batch 50 loss: 0.8075745850801468\n",
      "  batch 100 loss: 0.7427630198001861\n",
      "  batch 150 loss: 0.7210545015335083\n",
      "LOSS train 0.7210545015335083 valid 1.1072801500558853\n",
      "EPOCH 246:\n",
      "  batch 50 loss: 0.7804706418514251\n",
      "  batch 100 loss: 0.674349056482315\n",
      "  batch 150 loss: 0.7476104676723481\n",
      "LOSS train 0.7476104676723481 valid 1.1247997432947159\n",
      "EPOCH 247:\n",
      "  batch 50 loss: 0.6585742884874344\n",
      "  batch 100 loss: 0.7232425796985626\n",
      "  batch 150 loss: 0.7763664758205414\n",
      "LOSS train 0.7763664758205414 valid 1.1464795744732807\n",
      "EPOCH 248:\n",
      "  batch 50 loss: 0.7285527926683426\n",
      "  batch 100 loss: 0.7063789981603622\n",
      "  batch 150 loss: 0.7573267340660095\n",
      "LOSS train 0.7573267340660095 valid 1.12152078590895\n",
      "EPOCH 249:\n",
      "  batch 50 loss: 0.6804442620277404\n",
      "  batch 100 loss: 0.7225012332201004\n",
      "  batch 150 loss: 0.7409630545973778\n",
      "LOSS train 0.7409630545973778 valid 1.1555197333034717\n",
      "EPOCH 250:\n",
      "  batch 50 loss: 0.6771079230308533\n",
      "  batch 100 loss: 0.7072973394393921\n",
      "  batch 150 loss: 0.7460980683565139\n",
      "LOSS train 0.7460980683565139 valid 1.2541580035498268\n",
      "EPOCH 251:\n",
      "  batch 50 loss: 0.7500780737400055\n",
      "  batch 100 loss: 0.725284321308136\n",
      "  batch 150 loss: 0.7646676051616669\n",
      "LOSS train 0.7646676051616669 valid 1.1238902611167807\n",
      "EPOCH 252:\n",
      "  batch 50 loss: 0.675735542178154\n",
      "  batch 100 loss: 0.7433686864376068\n",
      "  batch 150 loss: 0.7440414410829544\n",
      "LOSS train 0.7440414410829544 valid 1.1689478582457493\n",
      "EPOCH 253:\n",
      "  batch 50 loss: 0.6664556214213371\n",
      "  batch 100 loss: 0.709545618891716\n",
      "  batch 150 loss: 0.7856778168678283\n",
      "LOSS train 0.7856778168678283 valid 1.157910815979305\n",
      "EPOCH 254:\n",
      "  batch 50 loss: 0.6825600785017013\n",
      "  batch 100 loss: 0.7575734096765518\n",
      "  batch 150 loss: 0.7404346805810929\n",
      "LOSS train 0.7404346805810929 valid 1.1066471024563438\n",
      "EPOCH 255:\n",
      "  batch 50 loss: 0.6725193643569947\n",
      "  batch 100 loss: 0.7102015417814255\n",
      "  batch 150 loss: 0.6920476359128952\n",
      "LOSS train 0.6920476359128952 valid 1.1516739757437455\n",
      "EPOCH 256:\n",
      "  batch 50 loss: 0.7705464953184128\n",
      "  batch 100 loss: 0.6323798018693924\n",
      "  batch 150 loss: 0.7474617677927017\n",
      "LOSS train 0.7474617677927017 valid 1.1448338949366619\n",
      "EPOCH 257:\n",
      "  batch 50 loss: 0.6613331067562104\n",
      "  batch 100 loss: 0.7279178649187088\n",
      "  batch 150 loss: 0.7513900223374367\n",
      "LOSS train 0.7513900223374367 valid 1.1223618866581666\n",
      "EPOCH 258:\n",
      "  batch 50 loss: 0.6818253409862518\n",
      "  batch 100 loss: 0.6402431744337082\n",
      "  batch 150 loss: 0.6505633819103241\n",
      "LOSS train 0.6505633819103241 valid 1.1124768970828307\n",
      "EPOCH 259:\n",
      "  batch 50 loss: 0.6260674107074737\n",
      "  batch 100 loss: 0.7670056927204132\n",
      "  batch 150 loss: 0.7128760588169097\n",
      "LOSS train 0.7128760588169097 valid 1.1250232167934116\n",
      "EPOCH 260:\n",
      "  batch 50 loss: 0.707231325507164\n",
      "  batch 100 loss: 0.6463426667451858\n",
      "  batch 150 loss: 0.7098242712020874\n",
      "LOSS train 0.7098242712020874 valid 1.1499854138022976\n",
      "EPOCH 261:\n",
      "  batch 50 loss: 0.7351542407274246\n",
      "  batch 100 loss: 0.6228654330968857\n",
      "  batch 150 loss: 0.6621999824047089\n",
      "LOSS train 0.6621999824047089 valid 1.1244854534927167\n",
      "EPOCH 262:\n",
      "  batch 50 loss: 0.6777386537194252\n",
      "  batch 100 loss: 0.6861980131268501\n",
      "  batch 150 loss: 0.7403898590803146\n",
      "LOSS train 0.7403898590803146 valid 1.1426033228635788\n",
      "EPOCH 263:\n",
      "  batch 50 loss: 0.6282997924089432\n",
      "  batch 100 loss: 0.7072260314226151\n",
      "  batch 150 loss: 0.6675752675533295\n",
      "LOSS train 0.6675752675533295 valid 1.3087990480033975\n",
      "EPOCH 264:\n",
      "  batch 50 loss: 0.7668409106135369\n",
      "  batch 100 loss: 0.7761917972564697\n",
      "  batch 150 loss: 0.7212205439805984\n",
      "LOSS train 0.7212205439805984 valid 1.149314676460467\n",
      "EPOCH 265:\n",
      "  batch 50 loss: 0.6879407119750977\n",
      "  batch 100 loss: 0.7151821976900101\n",
      "  batch 150 loss: 0.693339616060257\n",
      "LOSS train 0.693339616060257 valid 1.3272689988738613\n",
      "EPOCH 266:\n",
      "  batch 50 loss: 0.715595832169056\n",
      "  batch 100 loss: 0.7519542092084884\n",
      "  batch 150 loss: 0.6840341973304749\n",
      "LOSS train 0.6840341973304749 valid 1.223582522649514\n",
      "EPOCH 267:\n",
      "  batch 50 loss: 0.7034851467609405\n",
      "  batch 100 loss: 0.7182835721969605\n",
      "  batch 150 loss: 0.6267504066228866\n",
      "LOSS train 0.6267504066228866 valid 1.1798696023853201\n",
      "EPOCH 268:\n",
      "  batch 50 loss: 0.6446128106117248\n",
      "  batch 100 loss: 0.7174746304750442\n",
      "  batch 150 loss: 0.7917466920614242\n",
      "LOSS train 0.7917466920614242 valid 1.1603641596279646\n",
      "EPOCH 269:\n",
      "  batch 50 loss: 0.8733558535575867\n",
      "  batch 100 loss: 0.6629592370986939\n",
      "  batch 150 loss: 0.6621433341503143\n",
      "LOSS train 0.6621433341503143 valid 1.1899282344077762\n",
      "EPOCH 270:\n",
      "  batch 50 loss: 0.6127588397264481\n",
      "  batch 100 loss: 0.6766882681846619\n",
      "  batch 150 loss: 0.7427061522006988\n",
      "LOSS train 0.7427061522006988 valid 1.1429874810733294\n",
      "EPOCH 271:\n",
      "  batch 50 loss: 0.6776358005404473\n",
      "  batch 100 loss: 0.7078064078092575\n",
      "  batch 150 loss: 0.6863658732175827\n",
      "LOSS train 0.6863658732175827 valid 1.1499591846215098\n",
      "EPOCH 272:\n",
      "  batch 50 loss: 0.6503204339742661\n",
      "  batch 100 loss: 0.6219868046045304\n",
      "  batch 150 loss: 0.6838300976157189\n",
      "LOSS train 0.6838300976157189 valid 1.137076166115309\n",
      "EPOCH 273:\n",
      "  batch 50 loss: 0.6369631853699684\n",
      "  batch 100 loss: 0.7226856821775436\n",
      "  batch 150 loss: 0.6385769957304\n",
      "LOSS train 0.6385769957304 valid 1.1145117369137312\n",
      "EPOCH 274:\n",
      "  batch 50 loss: 0.6472709798812866\n",
      "  batch 100 loss: 0.654507651925087\n",
      "  batch 150 loss: 0.7739179927110672\n",
      "LOSS train 0.7739179927110672 valid 1.1954567401032699\n",
      "EPOCH 275:\n",
      "  batch 50 loss: 0.6582597428560257\n",
      "  batch 100 loss: 0.6837774610519409\n",
      "  batch 150 loss: 0.6480233532190323\n",
      "LOSS train 0.6480233532190323 valid 1.2211957624868344\n",
      "EPOCH 276:\n",
      "  batch 50 loss: 0.699113989174366\n",
      "  batch 100 loss: 0.6608333468437195\n",
      "  batch 150 loss: 0.5610807567834855\n",
      "LOSS train 0.5610807567834855 valid 1.1811328757750361\n",
      "EPOCH 277:\n",
      "  batch 50 loss: 0.6265154400467873\n",
      "  batch 100 loss: 0.6359453117847442\n",
      "  batch 150 loss: 0.7047029650211334\n",
      "LOSS train 0.7047029650211334 valid 1.1063732810710605\n",
      "EPOCH 278:\n",
      "  batch 50 loss: 0.6242561811208724\n",
      "  batch 100 loss: 0.6250637817382813\n",
      "  batch 150 loss: 0.6694835916161537\n",
      "LOSS train 0.6694835916161537 valid 1.1227257667403472\n",
      "EPOCH 279:\n",
      "  batch 50 loss: 0.6420521318912507\n",
      "  batch 100 loss: 0.6610620939731597\n",
      "  batch 150 loss: 0.6517616450786591\n",
      "LOSS train 0.6517616450786591 valid 1.1650266953204806\n",
      "EPOCH 280:\n",
      "  batch 50 loss: 0.6339104068279267\n",
      "  batch 100 loss: 0.6160397374629974\n",
      "  batch 150 loss: 0.6907430157065392\n",
      "LOSS train 0.6907430157065392 valid 1.1316707855776738\n",
      "EPOCH 281:\n",
      "  batch 50 loss: 0.6036252582073212\n",
      "  batch 100 loss: 0.6503079810738563\n",
      "  batch 150 loss: 0.6744780683517456\n",
      "LOSS train 0.6744780683517456 valid 1.1041022390127182\n",
      "EPOCH 282:\n",
      "  batch 50 loss: 0.6354196757078171\n",
      "  batch 100 loss: 0.6352983522415161\n",
      "  batch 150 loss: 0.6533870327472687\n",
      "LOSS train 0.6533870327472687 valid 1.1662533393031673\n",
      "EPOCH 283:\n",
      "  batch 50 loss: 0.6486036741733551\n",
      "  batch 100 loss: 0.5837218770384789\n",
      "  batch 150 loss: 0.6664027863740921\n",
      "LOSS train 0.6664027863740921 valid 1.2242400544254404\n",
      "EPOCH 284:\n",
      "  batch 50 loss: 0.6388868647813797\n",
      "  batch 100 loss: 0.6608135414123535\n",
      "  batch 150 loss: 0.6364598435163498\n",
      "LOSS train 0.6364598435163498 valid 1.1824420431726856\n",
      "EPOCH 285:\n",
      "  batch 50 loss: 0.6050856322050094\n",
      "  batch 100 loss: 0.7341144627332687\n",
      "  batch 150 loss: 0.6643504363298416\n",
      "LOSS train 0.6643504363298416 valid 1.1244072718055624\n",
      "EPOCH 286:\n",
      "  batch 50 loss: 0.6425469893217087\n",
      "  batch 100 loss: 0.6955355429649352\n",
      "  batch 150 loss: 0.6584333795309066\n",
      "LOSS train 0.6584333795309066 valid 1.0966341562177007\n",
      "EPOCH 287:\n",
      "  batch 50 loss: 0.5977909603714943\n",
      "  batch 100 loss: 0.6278036230802536\n",
      "  batch 150 loss: 0.7056670206785202\n",
      "LOSS train 0.7056670206785202 valid 1.1071978301594132\n",
      "EPOCH 288:\n",
      "  batch 50 loss: 0.7018946182727813\n",
      "  batch 100 loss: 0.7091565611958504\n",
      "  batch 150 loss: 0.6723866951465607\n",
      "LOSS train 0.6723866951465607 valid 1.100366089689104\n",
      "EPOCH 289:\n",
      "  batch 50 loss: 0.6380777794122696\n",
      "  batch 100 loss: 0.7106670618057251\n",
      "  batch 150 loss: 0.6547440981864929\n",
      "LOSS train 0.6547440981864929 valid 1.1318094385297675\n",
      "EPOCH 290:\n",
      "  batch 50 loss: 0.6021216541528702\n",
      "  batch 100 loss: 0.6644263654947281\n",
      "  batch 150 loss: 0.5944396924972534\n",
      "LOSS train 0.5944396924972534 valid 1.1088580277405287\n",
      "EPOCH 291:\n",
      "  batch 50 loss: 0.6790436327457428\n",
      "  batch 100 loss: 0.5893350884318351\n",
      "  batch 150 loss: 0.6511843079328536\n",
      "LOSS train 0.6511843079328536 valid 1.1092508364664881\n",
      "EPOCH 292:\n",
      "  batch 50 loss: 0.6130799973011016\n",
      "  batch 100 loss: 0.5570103433728218\n",
      "  batch 150 loss: 0.6156692212820053\n",
      "LOSS train 0.6156692212820053 valid 1.1559139497970279\n",
      "EPOCH 293:\n",
      "  batch 50 loss: 0.6042022711038589\n",
      "  batch 100 loss: 0.610395324230194\n",
      "  batch 150 loss: 0.650983761548996\n",
      "LOSS train 0.650983761548996 valid 1.1215061341461383\n",
      "EPOCH 294:\n",
      "  batch 50 loss: 0.550913000702858\n",
      "  batch 100 loss: 0.6363119029998779\n",
      "  batch 150 loss: 0.6398189666867257\n",
      "LOSS train 0.6398189666867257 valid 1.14448192794072\n",
      "EPOCH 295:\n",
      "  batch 50 loss: 0.6199425035715103\n",
      "  batch 100 loss: 0.6334997725486755\n",
      "  batch 150 loss: 0.5206214648485183\n",
      "LOSS train 0.5206214648485183 valid 1.1285733168846683\n",
      "EPOCH 296:\n",
      "  batch 50 loss: 0.5874418649077415\n",
      "  batch 100 loss: 0.6556350499391556\n",
      "  batch 150 loss: 0.5681445273756981\n",
      "LOSS train 0.5681445273756981 valid 1.1194785637290854\n",
      "EPOCH 297:\n",
      "  batch 50 loss: 0.6181505966186523\n",
      "  batch 100 loss: 0.5625950145721436\n",
      "  batch 150 loss: 0.6793984824419022\n",
      "LOSS train 0.6793984824419022 valid 1.1716928568325544\n",
      "EPOCH 298:\n",
      "  batch 50 loss: 0.5296153378486633\n",
      "  batch 100 loss: 0.6210936480760574\n",
      "  batch 150 loss: 0.6969333004951477\n",
      "LOSS train 0.6969333004951477 valid 1.1516540634788965\n",
      "EPOCH 299:\n",
      "  batch 50 loss: 0.564223843216896\n",
      "  batch 100 loss: 0.6574160423874855\n",
      "  batch 150 loss: 0.6231993567943573\n",
      "LOSS train 0.6231993567943573 valid 1.1130096402607466\n",
      "EPOCH 300:\n",
      "  batch 50 loss: 0.5808260220289231\n",
      "  batch 100 loss: 0.6539644593000412\n",
      "  batch 150 loss: 0.631871400475502\n",
      "LOSS train 0.631871400475502 valid 1.1716223672816628\n",
      "EPOCH 301:\n",
      "  batch 50 loss: 0.7617262727022172\n",
      "  batch 100 loss: 0.6730635124444961\n",
      "  batch 150 loss: 0.580799509882927\n",
      "LOSS train 0.580799509882927 valid 1.1291880944841786\n",
      "EPOCH 302:\n",
      "  batch 50 loss: 0.5977278393507004\n",
      "  batch 100 loss: 0.636822330057621\n",
      "  batch 150 loss: 0.5837505260109901\n",
      "LOSS train 0.5837505260109901 valid 1.1556631193349236\n",
      "EPOCH 303:\n",
      "  batch 50 loss: 0.6406115439534187\n",
      "  batch 100 loss: 0.577179174721241\n",
      "  batch 150 loss: 0.6088486126065255\n",
      "LOSS train 0.6088486126065255 valid 1.158958259381746\n",
      "EPOCH 304:\n",
      "  batch 50 loss: 0.5331630539894104\n",
      "  batch 100 loss: 0.6651720869541168\n",
      "  batch 150 loss: 0.6172139900922775\n",
      "LOSS train 0.6172139900922775 valid 1.1805727230875116\n",
      "EPOCH 305:\n",
      "  batch 50 loss: 0.5139508524537086\n",
      "  batch 100 loss: 0.6484522208571434\n",
      "  batch 150 loss: 0.600721073448658\n",
      "LOSS train 0.600721073448658 valid 1.159006710115232\n",
      "EPOCH 306:\n",
      "  batch 50 loss: 0.5620331346988678\n",
      "  batch 100 loss: 0.5749188902974128\n",
      "  batch 150 loss: 0.5722529694437981\n",
      "LOSS train 0.5722529694437981 valid 1.2101883111815703\n",
      "EPOCH 307:\n",
      "  batch 50 loss: 0.5511570471525192\n",
      "  batch 100 loss: 0.5887694150209427\n",
      "  batch 150 loss: 0.5840660563111305\n",
      "LOSS train 0.5840660563111305 valid 1.1972815825750953\n",
      "EPOCH 308:\n",
      "  batch 50 loss: 0.5883053731918335\n",
      "  batch 100 loss: 0.5596500161290169\n",
      "  batch 150 loss: 0.6305869460105896\n",
      "LOSS train 0.6305869460105896 valid 1.2102131855330969\n",
      "EPOCH 309:\n",
      "  batch 50 loss: 0.543669713139534\n",
      "  batch 100 loss: 0.5606920078396798\n",
      "  batch 150 loss: 0.6129984754323959\n",
      "LOSS train 0.6129984754323959 valid 1.119846443596639\n",
      "EPOCH 310:\n",
      "  batch 50 loss: 0.5818030440807342\n",
      "  batch 100 loss: 0.6391544091701508\n",
      "  batch 150 loss: 0.5558271208405494\n",
      "LOSS train 0.5558271208405494 valid 1.130314172098511\n",
      "EPOCH 311:\n",
      "  batch 50 loss: 0.5364900454878807\n",
      "  batch 100 loss: 0.5216591060161591\n",
      "  batch 150 loss: 0.5956810969114303\n",
      "LOSS train 0.5956810969114303 valid 1.1076430419557972\n",
      "EPOCH 312:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 38\u001B[0m\n\u001B[1;32m     35\u001B[0m last_loss \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m     36\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 38\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrn_loader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mx_batch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    672\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    673\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 674\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    676\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Projects/splid-comp/trial_models/dataset.py:72\u001B[0m, in \u001B[0;36mSPLID.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m     71\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mObjectID\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mids[idx]]\n\u001B[0;32m---> 72\u001B[0m     series \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcol_transformer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m[:, :\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns)]\n\u001B[1;32m     73\u001B[0m     labels \u001B[38;5;241m=\u001B[39m data[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEW\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNS\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[1;32m     75\u001B[0m     series \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(series, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:157\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 157\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    159\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    160\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    161\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    162\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    163\u001B[0m         )\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:827\u001B[0m, in \u001B[0;36mColumnTransformer.transform\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    823\u001B[0m     \u001B[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001B[39;00m\n\u001B[1;32m    824\u001B[0m     \u001B[38;5;66;03m# check that n_features_in_ is consistent\u001B[39;00m\n\u001B[1;32m    825\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_n_features(X, reset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m--> 827\u001B[0m Xs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    828\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    829\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    830\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_transform_one\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    831\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfitted\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    832\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumn_as_strings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_dataframe_and_transform_dataframe\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    833\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    834\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_output(Xs)\n\u001B[1;32m    836\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m Xs:\n\u001B[1;32m    837\u001B[0m     \u001B[38;5;66;03m# All transformers are None\u001B[39;00m\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:681\u001B[0m, in \u001B[0;36mColumnTransformer._fit_transform\u001B[0;34m(self, X, y, func, fitted, column_as_strings)\u001B[0m\n\u001B[1;32m    675\u001B[0m transformers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter(\n\u001B[1;32m    677\u001B[0m         fitted\u001B[38;5;241m=\u001B[39mfitted, replace_strings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, column_as_strings\u001B[38;5;241m=\u001B[39mcolumn_as_strings\n\u001B[1;32m    678\u001B[0m     )\n\u001B[1;32m    679\u001B[0m )\n\u001B[1;32m    680\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 681\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    682\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    683\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtransformer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrans\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfitted\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrans\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    684\u001B[0m \u001B[43m            \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_safe_indexing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    685\u001B[0m \u001B[43m            \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    686\u001B[0m \u001B[43m            \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    687\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmessage_clsname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mColumnTransformer\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    688\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_log_message\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtransformers\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    689\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    690\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrans\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtransformers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    691\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    692\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    693\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected 2D array, got 1D array instead\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e):\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     60\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     61\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     62\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     64\u001B[0m )\n\u001B[0;32m---> 65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/joblib/parallel.py:1863\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1861\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[1;32m   1862\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 1863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n\u001B[1;32m   1865\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[1;32m   1866\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[1;32m   1867\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[1;32m   1868\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[1;32m   1869\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[1;32m   1870\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/joblib/parallel.py:1792\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1790\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1791\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m-> 1792\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1793\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1794\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    125\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[0;32m--> 127\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/pipeline.py:940\u001B[0m, in \u001B[0;36m_transform_one\u001B[0;34m(transformer, X, y, weight, **fit_params)\u001B[0m\n\u001B[1;32m    939\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_transform_one\u001B[39m(transformer, X, y, weight, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params):\n\u001B[0;32m--> 940\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mtransformer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    941\u001B[0m     \u001B[38;5;66;03m# if we have a weight for this transformer, multiply output\u001B[39;00m\n\u001B[1;32m    942\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:157\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 157\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    159\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    160\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    161\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    162\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    163\u001B[0m         )\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:1003\u001B[0m, in \u001B[0;36mStandardScaler.transform\u001B[0;34m(self, X, copy)\u001B[0m\n\u001B[1;32m    988\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    989\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Perform standardization by centering and scaling.\u001B[39;00m\n\u001B[1;32m    990\u001B[0m \n\u001B[1;32m    991\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1001\u001B[0m \u001B[38;5;124;03m        Transformed array.\u001B[39;00m\n\u001B[1;32m   1002\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1003\u001B[0m     \u001B[43mcheck_is_fitted\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1005\u001B[0m     copy \u001B[38;5;241m=\u001B[39m copy \u001B[38;5;28;01mif\u001B[39;00m copy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy\n\u001B[1;32m   1006\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_data(\n\u001B[1;32m   1007\u001B[0m         X,\n\u001B[1;32m   1008\u001B[0m         reset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1012\u001B[0m         force_all_finite\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow-nan\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1013\u001B[0m     )\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/utils/validation.py:1460\u001B[0m, in \u001B[0;36mcheck_is_fitted\u001B[0;34m(estimator, attributes, msg, all_or_any)\u001B[0m\n\u001B[1;32m   1457\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(estimator, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   1458\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m is not an estimator instance.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (estimator))\n\u001B[0;32m-> 1460\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43m_is_fitted\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattributes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_or_any\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1461\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NotFittedError(msg \u001B[38;5;241m%\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(estimator)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m})\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/utils/validation.py:1398\u001B[0m, in \u001B[0;36m_is_fitted\u001B[0;34m(estimator, attributes, all_or_any)\u001B[0m\n\u001B[1;32m   1395\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(estimator, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__sklearn_is_fitted__\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   1396\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m estimator\u001B[38;5;241m.\u001B[39m__sklearn_is_fitted__()\n\u001B[0;32m-> 1398\u001B[0m fitted_attrs \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\n\u001B[1;32m   1399\u001B[0m \u001B[43m    \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mvars\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mendswith\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m_\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstartswith\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m__\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1400\u001B[0m \u001B[43m\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m   1401\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(fitted_attrs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/Projects/splid-comp/venv/lib/python3.11/site-packages/sklearn/utils/validation.py:1399\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1395\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(estimator, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__sklearn_is_fitted__\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   1396\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m estimator\u001B[38;5;241m.\u001B[39m__sklearn_is_fitted__()\n\u001B[1;32m   1398\u001B[0m fitted_attrs \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m-> 1399\u001B[0m     v \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mvars\u001B[39m(estimator) \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mendswith\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m_\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m v\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1400\u001B[0m ]\n\u001B[1;32m   1401\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(fitted_attrs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models.prectime import PrecTime\n",
    "from loss import WeightedCELoss\n",
    "\n",
    "cols = ['Inclination (deg)', 'Longitude (deg)', 'Eccentricity', 'Semimajor Axis (m)', 'RAAN (deg)', 'Argument of Periapsis (deg)', 'Vz (m/s)']\n",
    "classes = ['ES-ES', 'SS-CK', 'SS-EK', 'SS-HK', 'SS-NK', 'IK-CK', 'IK-EK', 'IK-HK', 'ID-NK', 'AD-NK']\n",
    "trn_data = SPLID(train_datalist, ground_truth, cols, classes=classes)\n",
    "tst_data = SPLID(test_datalist, ground_truth, cols, classes=classes)\n",
    "\n",
    "trn_loader = data.DataLoader(trn_data, shuffle=True, batch_size=10)\n",
    "tst_loader = data.DataLoader(tst_data, shuffle=True, batch_size=10)\n",
    "\n",
    "lr = 1e-5\n",
    "n_epochs = 1000\n",
    "best_tst_loss = 1_000_000.\n",
    "\n",
    "model = PrecTime(len(classes), n_win=92, l_win=24, c_in=len(cols), c_conv=128)\n",
    "model = model.cuda()\n",
    "criterion = WeightedCELoss()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "print('Start model training')\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/splid_trainer_{}'.format(timestamp))\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    print('EPOCH {}:'.format(epoch))\n",
    "    \n",
    "    running_loss = np.zeros(3)\n",
    "    last_loss = np.zeros(3)\n",
    "    model.train(True)\n",
    "    \n",
    "    for i, (x_batch, y_batch) in enumerate(trn_loader):\n",
    "        \n",
    "        x_batch = x_batch.cuda()\n",
    "        y_batch = y_batch.cuda()\n",
    "        # sched.step()\n",
    "        opt.zero_grad()\n",
    "        fine_out, coarse_out = model(x_batch)\n",
    "        tot_loss, fine_loss, coarse_loss = criterion(fine_out, coarse_out, y_batch)\n",
    "        tot_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += np.array([tot_loss.cpu().item(), fine_loss.cpu().item(), coarse_loss.cpu().item()])\n",
    "        if i % 50 == 49:\n",
    "            last_loss = running_loss / 50 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss[0]))\n",
    "            tb_x = epoch * len(trn_loader) + i + 1\n",
    "            writer.add_scalar('TotLoss/train', last_loss[0], tb_x)\n",
    "            writer.add_scalar('FineLoss/train', last_loss[1], tb_x)\n",
    "            writer.add_scalar('CoarseLoss/train', last_loss[2], tb_x)\n",
    "            running_loss = np.zeros(3)\n",
    "    \n",
    "    running_tst_loss = np.zeros(3)\n",
    "    model.eval()\n",
    "    \n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, (x_batch, y_batch) in enumerate(tst_loader):\n",
    "            x_batch = x_batch.cuda()\n",
    "            y_batch = y_batch.cuda()\n",
    "            fine_out, coarse_out = model(x_batch)\n",
    "            tot_tst_loss, fine_tst_loss, coarse_tst_loss = criterion(fine_out, coarse_out, y_batch)\n",
    "            running_tst_loss += np.array([tot_tst_loss.cpu(), fine_tst_loss.cpu(), coarse_tst_loss.cpu()])\n",
    "    \n",
    "    avg_tst_loss = running_tst_loss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(last_loss[0], avg_tst_loss[0]))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Total Loss',\n",
    "                    { 'Training' : last_loss[0], 'Validation' : avg_tst_loss[0] },\n",
    "                    epoch)\n",
    "    writer.add_scalars('Training vs. Validation Fine Loss',\n",
    "                    { 'Training' : last_loss[1], 'Validation' : avg_tst_loss[1] },\n",
    "                    epoch)\n",
    "    writer.add_scalars('Training vs. Validation Coarse Loss',\n",
    "                    { 'Training' : last_loss[2], 'Validation' : avg_tst_loss[2] },\n",
    "                    epoch)\n",
    "    writer.flush()\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_tst_loss[0] < best_tst_loss:\n",
    "        best_tst_loss = avg_tst_loss[0]\n",
    "        model_path = 'model_{}.pth'.format(timestamp)\n",
    "        torch.save(model.state_dict(), 'saved_models/' + model_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
