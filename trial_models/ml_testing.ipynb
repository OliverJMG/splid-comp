{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T09:10:33.150586755Z",
     "start_time": "2024-02-19T09:10:31.720849507Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fastcore.basics import Path, AttrDict\n",
    "from dataset import SPLID\n",
    "import torch\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T09:10:33.854985489Z",
     "start_time": "2024-02-19T09:10:33.847463562Z"
    }
   },
   "outputs": [],
   "source": [
    "config = AttrDict(\n",
    "    challenge_data_dir = Path('~/Projects/splid-comp/dataset').expanduser(),\n",
    "    valid_ratio = 0.1,\n",
    "    kernel_size = 5,\n",
    "    tolerance= 6, # Default evaluation tolerance\n",
    ")\n",
    "\n",
    "# Define the directory paths\n",
    "train_data_dir = config.challenge_data_dir / \"train_v2\"\n",
    "\n",
    "# Load the ground truth data\n",
    "ground_truth = config.challenge_data_dir / 'train_labels_v2.csv'\n",
    "\n",
    "datalist = []\n",
    "\n",
    "# Searching for training data within the dataset folder\n",
    "for file in os.listdir(train_data_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        datalist.append(os.path.join(train_data_dir, file))\n",
    "\n",
    "# Sort the training data and labels\n",
    "datalist = sorted(datalist, key=lambda i: int(os.path.splitext(os.path.basename(i))[0]))\n",
    "    \n",
    "\n",
    "train_datalist, test_datalist = train_test_split(datalist, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T09:50:11.351847134Z",
     "start_time": "2024-02-19T09:18:48.588912346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1520 files...\n",
      "Loaded file 0 of 1520\n",
      "Loaded file 50 of 1520\n",
      "Loaded file 100 of 1520\n",
      "Loaded file 150 of 1520\n",
      "Loaded file 200 of 1520\n",
      "Loaded file 250 of 1520\n",
      "Loaded file 300 of 1520\n",
      "Loaded file 350 of 1520\n",
      "Loaded file 400 of 1520\n",
      "Loaded file 450 of 1520\n",
      "Loaded file 500 of 1520\n",
      "Loaded file 550 of 1520\n",
      "Loaded file 600 of 1520\n",
      "Loaded file 650 of 1520\n",
      "Loaded file 700 of 1520\n",
      "Loaded file 750 of 1520\n",
      "Loaded file 800 of 1520\n",
      "Loaded file 850 of 1520\n",
      "Loaded file 900 of 1520\n",
      "Loaded file 950 of 1520\n",
      "Loaded file 1000 of 1520\n",
      "Loaded file 1050 of 1520\n",
      "Loaded file 1100 of 1520\n",
      "Loaded file 1150 of 1520\n",
      "Loaded file 1200 of 1520\n",
      "Loaded file 1250 of 1520\n",
      "Loaded file 1300 of 1520\n",
      "Loaded file 1350 of 1520\n",
      "Loaded file 1400 of 1520\n",
      "Loaded file 1450 of 1520\n",
      "Loaded file 1500 of 1520\n",
      "Joining dataframes...\n",
      "Done!\n",
      "Loading 380 files...\n",
      "Loaded file 0 of 380\n",
      "Loaded file 50 of 380\n",
      "Loaded file 100 of 380\n",
      "Loaded file 150 of 380\n",
      "Loaded file 200 of 380\n",
      "Loaded file 250 of 380\n",
      "Loaded file 300 of 380\n",
      "Loaded file 350 of 380\n",
      "Joining dataframes...\n",
      "Done!\n",
      "Start model training\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 6.889172258377076\n",
      "  batch 100 loss: 6.8088693523406985\n",
      "  batch 150 loss: 6.638567991256714\n",
      "LOSS train 6.638567991256714 valid 6.445601111964176\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 6.306343326568603\n",
      "  batch 100 loss: 5.971592445373535\n",
      "  batch 150 loss: 5.659675483703613\n",
      "LOSS train 5.659675483703613 valid 5.520196211965461\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 5.588722934722901\n",
      "  batch 100 loss: 5.418093814849853\n",
      "  batch 150 loss: 5.261169157028198\n",
      "LOSS train 5.261169157028198 valid 5.124692214162726\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 5.180093936920166\n",
      "  batch 100 loss: 5.219288387298584\n",
      "  batch 150 loss: 5.084953832626343\n",
      "LOSS train 5.084953832626343 valid 4.936238966490093\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 5.134963912963867\n",
      "  batch 100 loss: 4.9200904750823975\n",
      "  batch 150 loss: 4.945834765434265\n",
      "LOSS train 4.945834765434265 valid 4.789781827675669\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 4.84203104019165\n",
      "  batch 100 loss: 4.918409357070923\n",
      "  batch 150 loss: 4.748068628311157\n",
      "LOSS train 4.748068628311157 valid 4.624138079191509\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 4.801577110290527\n",
      "  batch 100 loss: 4.669614381790161\n",
      "  batch 150 loss: 4.682428293228149\n",
      "LOSS train 4.682428293228149 valid 4.455619962591874\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 4.531369700431823\n",
      "  batch 100 loss: 4.581651787757874\n",
      "  batch 150 loss: 4.403247423171997\n",
      "LOSS train 4.403247423171997 valid 4.262719148083737\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 4.374227652549743\n",
      "  batch 100 loss: 4.156827721595764\n",
      "  batch 150 loss: 4.20732536315918\n",
      "LOSS train 4.20732536315918 valid 3.9888208665345846\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 4.014175901412964\n",
      "  batch 100 loss: 3.953189969062805\n",
      "  batch 150 loss: 3.9119701051712035\n",
      "LOSS train 3.9119701051712035 valid 3.686097816417092\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 3.7111425399780273\n",
      "  batch 100 loss: 3.6566670989990233\n",
      "  batch 150 loss: 3.517747139930725\n",
      "LOSS train 3.517747139930725 valid 3.4399879731630025\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 3.418353810310364\n",
      "  batch 100 loss: 3.346151556968689\n",
      "  batch 150 loss: 3.4139810943603517\n",
      "LOSS train 3.4139810943603517 valid 3.2535654933829057\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 3.156244478225708\n",
      "  batch 100 loss: 3.138028163909912\n",
      "  batch 150 loss: 3.294134316444397\n",
      "LOSS train 3.294134316444397 valid 3.0829582904514514\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 3.0600051927566527\n",
      "  batch 100 loss: 3.138935523033142\n",
      "  batch 150 loss: 3.0408130788803103\n",
      "LOSS train 3.0408130788803103 valid 3.0025893951717175\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 3.015770921707153\n",
      "  batch 100 loss: 2.962424969673157\n",
      "  batch 150 loss: 2.9501203441619874\n",
      "LOSS train 2.9501203441619874 valid 2.9315170645713806\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 3.0844922971725466\n",
      "  batch 100 loss: 2.772639479637146\n",
      "  batch 150 loss: 2.8459724235534667\n",
      "LOSS train 2.8459724235534667 valid 2.8818525075912476\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 2.7962911319732666\n",
      "  batch 100 loss: 2.871977047920227\n",
      "  batch 150 loss: 2.7559424233436585\n",
      "LOSS train 2.7559424233436585 valid 2.787447690963745\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 2.7678264665603636\n",
      "  batch 100 loss: 2.8006813144683838\n",
      "  batch 150 loss: 2.767314462661743\n",
      "LOSS train 2.767314462661743 valid 2.7391978753240487\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 2.6944129252433777\n",
      "  batch 100 loss: 2.722955105304718\n",
      "  batch 150 loss: 2.6671691870689394\n",
      "LOSS train 2.6671691870689394 valid 2.711298086141285\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 2.67055801153183\n",
      "  batch 100 loss: 2.5792827105522154\n",
      "  batch 150 loss: 2.735110387802124\n",
      "LOSS train 2.735110387802124 valid 2.6731978027444137\n",
      "EPOCH 21:\n",
      "  batch 50 loss: 2.6780140876770018\n",
      "  batch 100 loss: 2.5622779989242552\n",
      "  batch 150 loss: 2.5265961480140686\n",
      "LOSS train 2.5265961480140686 valid 2.653041074150487\n",
      "EPOCH 22:\n",
      "  batch 50 loss: 2.4978452587127684\n",
      "  batch 100 loss: 2.543952610492706\n",
      "  batch 150 loss: 2.6874617767333984\n",
      "LOSS train 2.6874617767333984 valid 2.625245495846397\n",
      "EPOCH 23:\n",
      "  batch 50 loss: 2.432420175075531\n",
      "  batch 100 loss: 2.6995356369018553\n",
      "  batch 150 loss: 2.4738759994506836\n",
      "LOSS train 2.4738759994506836 valid 2.5894909720671806\n",
      "EPOCH 24:\n",
      "  batch 50 loss: 2.493744134902954\n",
      "  batch 100 loss: 2.439945094585419\n",
      "  batch 150 loss: 2.5388141822814942\n",
      "LOSS train 2.5388141822814942 valid 2.600755644472022\n",
      "EPOCH 25:\n",
      "  batch 50 loss: 2.470364010334015\n",
      "  batch 100 loss: 2.4197037529945375\n",
      "  batch 150 loss: 2.499683120250702\n",
      "LOSS train 2.499683120250702 valid 2.584335035399387\n",
      "EPOCH 26:\n",
      "  batch 50 loss: 2.466274359226227\n",
      "  batch 100 loss: 2.405106179714203\n",
      "  batch 150 loss: 2.4050008821487427\n",
      "LOSS train 2.4050008821487427 valid 2.571374739471235\n",
      "EPOCH 27:\n",
      "  batch 50 loss: 2.4314234161376955\n",
      "  batch 100 loss: 2.3255435061454772\n",
      "  batch 150 loss: 2.3837625694274904\n",
      "LOSS train 2.3837625694274904 valid 2.548160565526862\n",
      "EPOCH 28:\n",
      "  batch 50 loss: 2.4189523339271544\n",
      "  batch 100 loss: 2.2269746708869933\n",
      "  batch 150 loss: 2.3761752390861512\n",
      "LOSS train 2.3761752390861512 valid 2.49973895361549\n",
      "EPOCH 29:\n",
      "  batch 50 loss: 2.325205399990082\n",
      "  batch 100 loss: 2.292152667045593\n",
      "  batch 150 loss: 2.464389386177063\n",
      "LOSS train 2.464389386177063 valid 2.5338069953416524\n",
      "EPOCH 30:\n",
      "  batch 50 loss: 2.254562883377075\n",
      "  batch 100 loss: 2.3668964648246766\n",
      "  batch 150 loss: 2.278554027080536\n",
      "LOSS train 2.278554027080536 valid 2.4982912446323193\n",
      "EPOCH 31:\n",
      "  batch 50 loss: 2.296199953556061\n",
      "  batch 100 loss: 2.3699331760406492\n",
      "  batch 150 loss: 2.204877095222473\n",
      "LOSS train 2.204877095222473 valid 2.4533704048708866\n",
      "EPOCH 32:\n",
      "  batch 50 loss: 2.255743029117584\n",
      "  batch 100 loss: 2.270745358467102\n",
      "  batch 150 loss: 2.208028247356415\n",
      "LOSS train 2.208028247356415 valid 2.467938300810362\n",
      "EPOCH 33:\n",
      "  batch 50 loss: 2.2757778573036194\n",
      "  batch 100 loss: 2.1971811056137085\n",
      "  batch 150 loss: 2.1824439191818237\n",
      "LOSS train 2.1824439191818237 valid 2.4556050739790263\n",
      "EPOCH 34:\n",
      "  batch 50 loss: 2.188662602901459\n",
      "  batch 100 loss: 2.077528302669525\n",
      "  batch 150 loss: 2.3271070766448974\n",
      "LOSS train 2.3271070766448974 valid 2.457584848529414\n",
      "EPOCH 35:\n",
      "  batch 50 loss: 2.181838026046753\n",
      "  batch 100 loss: 2.1443674468994143\n",
      "  batch 150 loss: 2.1434134459495544\n",
      "LOSS train 2.1434134459495544 valid 2.3982849403431543\n",
      "EPOCH 36:\n",
      "  batch 50 loss: 2.0982038021087646\n",
      "  batch 100 loss: 2.1675732588768004\n",
      "  batch 150 loss: 2.2253540444374083\n",
      "LOSS train 2.2253540444374083 valid 2.4128487737555253\n",
      "EPOCH 37:\n",
      "  batch 50 loss: 2.2155389904975893\n",
      "  batch 100 loss: 2.111458804607391\n",
      "  batch 150 loss: 2.1780879735946654\n",
      "LOSS train 2.1780879735946654 valid 2.376698158289257\n",
      "EPOCH 38:\n",
      "  batch 50 loss: 2.006982915401459\n",
      "  batch 100 loss: 2.118895080089569\n",
      "  batch 150 loss: 2.1248896670341493\n",
      "LOSS train 2.1248896670341493 valid 2.375679866263741\n",
      "EPOCH 39:\n",
      "  batch 50 loss: 2.1904630780220034\n",
      "  batch 100 loss: 2.058992986679077\n",
      "  batch 150 loss: 2.0761409401893616\n",
      "LOSS train 2.0761409401893616 valid 2.3556264638900757\n",
      "EPOCH 40:\n",
      "  batch 50 loss: 2.1322671818733214\n",
      "  batch 100 loss: 2.0387001276016234\n",
      "  batch 150 loss: 2.117455337047577\n",
      "LOSS train 2.117455337047577 valid 2.3697647828804818\n",
      "EPOCH 41:\n",
      "  batch 50 loss: 2.224276089668274\n",
      "  batch 100 loss: 2.013393666744232\n",
      "  batch 150 loss: 2.108760902881622\n",
      "LOSS train 2.108760902881622 valid 2.364127639092897\n",
      "EPOCH 42:\n",
      "  batch 50 loss: 2.0792079615592955\n",
      "  batch 100 loss: 2.1749378752708437\n",
      "  batch 150 loss: 1.9464779353141786\n",
      "LOSS train 1.9464779353141786 valid 2.3130551827581307\n",
      "EPOCH 43:\n",
      "  batch 50 loss: 2.0693784093856813\n",
      "  batch 100 loss: 2.0177211952209473\n",
      "  batch 150 loss: 2.0019035851955413\n",
      "LOSS train 2.0019035851955413 valid 2.3136040631093477\n",
      "EPOCH 44:\n",
      "  batch 50 loss: 2.02042854309082\n",
      "  batch 100 loss: 1.9306759548187256\n",
      "  batch 150 loss: 2.0038955330848696\n",
      "LOSS train 2.0038955330848696 valid 2.335725768616325\n",
      "EPOCH 45:\n",
      "  batch 50 loss: 1.9441305708885193\n",
      "  batch 100 loss: 2.0891337108612063\n",
      "  batch 150 loss: 1.9673664891719818\n",
      "LOSS train 1.9673664891719818 valid 2.314155876636505\n",
      "EPOCH 46:\n",
      "  batch 50 loss: 1.9661344254016877\n",
      "  batch 100 loss: 1.9451170754432678\n",
      "  batch 150 loss: 2.029364447593689\n",
      "LOSS train 2.029364447593689 valid 2.339134476686779\n",
      "EPOCH 47:\n",
      "  batch 50 loss: 1.9101312422752381\n",
      "  batch 100 loss: 1.9590441703796386\n",
      "  batch 150 loss: 2.039099786281586\n",
      "LOSS train 2.039099786281586 valid 2.3004133042536283\n",
      "EPOCH 48:\n",
      "  batch 50 loss: 1.8694126391410828\n",
      "  batch 100 loss: 2.0248174691200256\n",
      "  batch 150 loss: 1.9375177788734437\n",
      "LOSS train 1.9375177788734437 valid 2.2905146573719226\n",
      "EPOCH 49:\n",
      "  batch 50 loss: 1.8869682931900025\n",
      "  batch 100 loss: 2.0145015287399293\n",
      "  batch 150 loss: 1.8983029913902283\n",
      "LOSS train 1.8983029913902283 valid 2.261932809101908\n",
      "EPOCH 50:\n",
      "  batch 50 loss: 1.8822204875946045\n",
      "  batch 100 loss: 1.8238409185409545\n",
      "  batch 150 loss: 1.992890019416809\n",
      "LOSS train 1.992890019416809 valid 2.32082190011677\n",
      "EPOCH 51:\n",
      "  batch 50 loss: 1.935280475616455\n",
      "  batch 100 loss: 1.8526178550720216\n",
      "  batch 150 loss: 1.999350049495697\n",
      "LOSS train 1.999350049495697 valid 2.315354874259547\n",
      "EPOCH 52:\n",
      "  batch 50 loss: 1.932359664440155\n",
      "  batch 100 loss: 1.883532340526581\n",
      "  batch 150 loss: 1.8617310404777527\n",
      "LOSS train 1.8617310404777527 valid 2.264560053223058\n",
      "EPOCH 53:\n",
      "  batch 50 loss: 1.9484556603431702\n",
      "  batch 100 loss: 1.8234588670730592\n",
      "  batch 150 loss: 1.8148789715766906\n",
      "LOSS train 1.8148789715766906 valid 2.286525845527649\n",
      "EPOCH 54:\n",
      "  batch 50 loss: 1.9543195497989654\n",
      "  batch 100 loss: 1.834369707107544\n",
      "  batch 150 loss: 1.825376410484314\n",
      "LOSS train 1.825376410484314 valid 2.290850990696957\n",
      "EPOCH 55:\n",
      "  batch 50 loss: 1.8482479643821716\n",
      "  batch 100 loss: 1.7954586815834046\n",
      "  batch 150 loss: 1.8904282557964325\n",
      "LOSS train 1.8904282557964325 valid 2.26416161813234\n",
      "EPOCH 56:\n",
      "  batch 50 loss: 1.8833474850654601\n",
      "  batch 100 loss: 1.8078097200393677\n",
      "  batch 150 loss: 1.8608087873458863\n",
      "LOSS train 1.8608087873458863 valid 2.2833481932941235\n",
      "EPOCH 57:\n",
      "  batch 50 loss: 1.9601395440101623\n",
      "  batch 100 loss: 1.669669940471649\n",
      "  batch 150 loss: 1.8667728757858277\n",
      "LOSS train 1.8667728757858277 valid 2.2449769942384017\n",
      "EPOCH 58:\n",
      "  batch 50 loss: 1.7801010727882385\n",
      "  batch 100 loss: 1.8145692253112793\n",
      "  batch 150 loss: 1.8287904644012452\n",
      "LOSS train 1.8287904644012452 valid 2.282501343049501\n",
      "EPOCH 59:\n",
      "  batch 50 loss: 1.7579389119148254\n",
      "  batch 100 loss: 1.793033103942871\n",
      "  batch 150 loss: 1.84306715965271\n",
      "LOSS train 1.84306715965271 valid 2.2906372515778792\n",
      "EPOCH 60:\n",
      "  batch 50 loss: 1.823148444890976\n",
      "  batch 100 loss: 1.801312154531479\n",
      "  batch 150 loss: 1.8289139795303344\n",
      "LOSS train 1.8289139795303344 valid 2.2607137529473555\n",
      "EPOCH 61:\n",
      "  batch 50 loss: 1.8048055350780488\n",
      "  batch 100 loss: 1.733118028640747\n",
      "  batch 150 loss: 1.7944578003883362\n",
      "LOSS train 1.7944578003883362 valid 2.2212356485818563\n",
      "EPOCH 62:\n",
      "  batch 50 loss: 1.7969012653827667\n",
      "  batch 100 loss: 1.706066734790802\n",
      "  batch 150 loss: 1.8117941641807556\n",
      "LOSS train 1.8117941641807556 valid 2.2521640940716394\n",
      "EPOCH 63:\n",
      "  batch 50 loss: 1.7608857917785645\n",
      "  batch 100 loss: 1.8271103596687317\n",
      "  batch 150 loss: 1.7270850825309754\n",
      "LOSS train 1.7270850825309754 valid 2.254251224429984\n",
      "EPOCH 64:\n",
      "  batch 50 loss: 1.7997497510910034\n",
      "  batch 100 loss: 1.753516001701355\n",
      "  batch 150 loss: 1.7014395809173584\n",
      "LOSS train 1.7014395809173584 valid 2.200236365983361\n",
      "EPOCH 65:\n",
      "  batch 50 loss: 1.720621976852417\n",
      "  batch 100 loss: 1.7473302149772645\n",
      "  batch 150 loss: 1.7089460790157318\n",
      "LOSS train 1.7089460790157318 valid 2.222217073566035\n",
      "EPOCH 66:\n",
      "  batch 50 loss: 1.787795262336731\n",
      "  batch 100 loss: 1.7840802335739137\n",
      "  batch 150 loss: 1.6090403640270232\n",
      "LOSS train 1.6090403640270232 valid 2.2107686745493034\n",
      "EPOCH 67:\n",
      "  batch 50 loss: 1.69537588596344\n",
      "  batch 100 loss: 1.753958809375763\n",
      "  batch 150 loss: 1.6621839857101441\n",
      "LOSS train 1.6621839857101441 valid 2.23076152487805\n",
      "EPOCH 68:\n",
      "  batch 50 loss: 1.757837448120117\n",
      "  batch 100 loss: 1.6982965278625488\n",
      "  batch 150 loss: 1.6422489309310913\n",
      "LOSS train 1.6422489309310913 valid 2.216990474023317\n",
      "EPOCH 69:\n",
      "  batch 50 loss: 1.6441134905815125\n",
      "  batch 100 loss: 1.645582344532013\n",
      "  batch 150 loss: 1.7480543386936187\n",
      "LOSS train 1.7480543386936187 valid 2.2103375384682105\n",
      "EPOCH 70:\n",
      "  batch 50 loss: 1.6110780143737793\n",
      "  batch 100 loss: 1.7702691602706908\n",
      "  batch 150 loss: 1.6206569504737853\n",
      "LOSS train 1.6206569504737853 valid 2.1653086072520207\n",
      "EPOCH 71:\n",
      "  batch 50 loss: 1.6457595825195312\n",
      "  batch 100 loss: 1.6236973643302917\n",
      "  batch 150 loss: 1.6686158430576326\n",
      "LOSS train 1.6686158430576326 valid 2.1360021296300387\n",
      "EPOCH 72:\n",
      "  batch 50 loss: 1.6468570923805237\n",
      "  batch 100 loss: 1.630911946296692\n",
      "  batch 150 loss: 1.679731047153473\n",
      "LOSS train 1.679731047153473 valid 2.15353473550395\n",
      "EPOCH 73:\n",
      "  batch 50 loss: 1.5963133370876312\n",
      "  batch 100 loss: 1.7044735693931579\n",
      "  batch 150 loss: 1.53384352684021\n",
      "LOSS train 1.53384352684021 valid 2.1529324995843986\n",
      "EPOCH 74:\n",
      "  batch 50 loss: 1.6535211884975434\n",
      "  batch 100 loss: 1.6662146377563476\n",
      "  batch 150 loss: 1.5890401875972748\n",
      "LOSS train 1.5890401875972748 valid 2.2259553106207597\n",
      "EPOCH 75:\n",
      "  batch 50 loss: 1.6672988796234132\n",
      "  batch 100 loss: 1.5773548352718354\n",
      "  batch 150 loss: 1.5732535672187806\n",
      "LOSS train 1.5732535672187806 valid 2.1486200721640336\n",
      "EPOCH 76:\n",
      "  batch 50 loss: 1.6627880239486694\n",
      "  batch 100 loss: 1.6371712613105773\n",
      "  batch 150 loss: 1.5650599193572998\n",
      "LOSS train 1.5650599193572998 valid 2.188097053452542\n",
      "EPOCH 77:\n",
      "  batch 50 loss: 1.5818508172035217\n",
      "  batch 100 loss: 1.5884631025791167\n",
      "  batch 150 loss: 1.6615442728996277\n",
      "LOSS train 1.6615442728996277 valid 2.1366516885004545\n",
      "EPOCH 78:\n",
      "  batch 50 loss: 1.4957785737514495\n",
      "  batch 100 loss: 1.5944323873519897\n",
      "  batch 150 loss: 1.6632760000228881\n",
      "LOSS train 1.6632760000228881 valid 2.16987039854652\n",
      "EPOCH 79:\n",
      "  batch 50 loss: 1.529287450313568\n",
      "  batch 100 loss: 1.4997099721431733\n",
      "  batch 150 loss: 1.6718422317504882\n",
      "LOSS train 1.6718422317504882 valid 2.1120104130945707\n",
      "EPOCH 80:\n",
      "  batch 50 loss: 1.6203918063640594\n",
      "  batch 100 loss: 1.6217902517318725\n",
      "  batch 150 loss: 1.5709298574924468\n",
      "LOSS train 1.5709298574924468 valid 2.1183501199672095\n",
      "EPOCH 81:\n",
      "  batch 50 loss: 1.5499262690544129\n",
      "  batch 100 loss: 1.6357380473613738\n",
      "  batch 150 loss: 1.5406673192977904\n",
      "LOSS train 1.5406673192977904 valid 2.107597285195401\n",
      "EPOCH 82:\n",
      "  batch 50 loss: 1.54835981965065\n",
      "  batch 100 loss: 1.4871431744098664\n",
      "  batch 150 loss: 1.6053557991981506\n",
      "LOSS train 1.6053557991981506 valid 2.1090843018732572\n",
      "EPOCH 83:\n",
      "  batch 50 loss: 1.579659469127655\n",
      "  batch 100 loss: 1.6405311834812164\n",
      "  batch 150 loss: 1.4887471663951874\n",
      "LOSS train 1.4887471663951874 valid 2.100119593896364\n",
      "EPOCH 84:\n",
      "  batch 50 loss: 1.4683811020851136\n",
      "  batch 100 loss: 1.5222509169578553\n",
      "  batch 150 loss: 1.5677105259895325\n",
      "LOSS train 1.5677105259895325 valid 2.108545952721646\n",
      "EPOCH 85:\n",
      "  batch 50 loss: 1.511143959760666\n",
      "  batch 100 loss: 1.5141306662559508\n",
      "  batch 150 loss: 1.5522966885566711\n",
      "LOSS train 1.5522966885566711 valid 2.1219667917803715\n",
      "EPOCH 86:\n",
      "  batch 50 loss: 1.4963046145439147\n",
      "  batch 100 loss: 1.4747336328029632\n",
      "  batch 150 loss: 1.5229151451587677\n",
      "LOSS train 1.5229151451587677 valid 2.075227828402268\n",
      "EPOCH 87:\n",
      "  batch 50 loss: 1.532068407535553\n",
      "  batch 100 loss: 1.563872195482254\n",
      "  batch 150 loss: 1.3831577897071838\n",
      "LOSS train 1.3831577897071838 valid 2.086739618527262\n",
      "EPOCH 88:\n",
      "  batch 50 loss: 1.4822188639640808\n",
      "  batch 100 loss: 1.4678842806816101\n",
      "  batch 150 loss: 1.5493034243583679\n",
      "LOSS train 1.5493034243583679 valid 2.0971374480347884\n",
      "EPOCH 89:\n",
      "  batch 50 loss: 1.4557879972457886\n",
      "  batch 100 loss: 1.4514735746383667\n",
      "  batch 150 loss: 1.4973494625091552\n",
      "LOSS train 1.4973494625091552 valid 2.1039467955890454\n",
      "EPOCH 90:\n",
      "  batch 50 loss: 1.609793187379837\n",
      "  batch 100 loss: 1.4472186601161956\n",
      "  batch 150 loss: 1.4216401171684265\n",
      "LOSS train 1.4216401171684265 valid 2.0886708937193217\n",
      "EPOCH 91:\n",
      "  batch 50 loss: 1.5169083964824677\n",
      "  batch 100 loss: 1.4612150585651398\n",
      "  batch 150 loss: 1.408955886363983\n",
      "LOSS train 1.408955886363983 valid 2.1049111893302515\n",
      "EPOCH 92:\n",
      "  batch 50 loss: 1.3786107432842254\n",
      "  batch 100 loss: 1.5207698166370391\n",
      "  batch 150 loss: 1.4506912207603455\n",
      "LOSS train 1.4506912207603455 valid 2.077740750814739\n",
      "EPOCH 93:\n",
      "  batch 50 loss: 1.4051630973815918\n",
      "  batch 100 loss: 1.465292010307312\n",
      "  batch 150 loss: 1.4774291455745696\n",
      "LOSS train 1.4774291455745696 valid 2.074033702674665\n",
      "EPOCH 94:\n",
      "  batch 50 loss: 1.4333414483070372\n",
      "  batch 100 loss: 1.5379589593410492\n",
      "  batch 150 loss: 1.4375813269615174\n",
      "LOSS train 1.4375813269615174 valid 2.069930235021993\n",
      "EPOCH 95:\n",
      "  batch 50 loss: 1.512770142555237\n",
      "  batch 100 loss: 1.4359844744205474\n",
      "  batch 150 loss: 1.361873996257782\n",
      "LOSS train 1.361873996257782 valid 2.102146343181008\n",
      "EPOCH 96:\n",
      "  batch 50 loss: 1.461403615474701\n",
      "  batch 100 loss: 1.4108223378658296\n",
      "  batch 150 loss: 1.3430057442188263\n",
      "LOSS train 1.3430057442188263 valid 2.008250603550359\n",
      "EPOCH 97:\n",
      "  batch 50 loss: 1.4010867643356324\n",
      "  batch 100 loss: 1.3592372179031371\n",
      "  batch 150 loss: 1.4528987145423888\n",
      "LOSS train 1.4528987145423888 valid 2.071862170570775\n",
      "EPOCH 98:\n",
      "  batch 50 loss: 1.4165225529670715\n",
      "  batch 100 loss: 1.528597445487976\n",
      "  batch 150 loss: 1.3489703810214997\n",
      "LOSS train 1.3489703810214997 valid 2.0472278406745508\n",
      "EPOCH 99:\n",
      "  batch 50 loss: 1.2962761580944062\n",
      "  batch 100 loss: 1.36543621301651\n",
      "  batch 150 loss: 1.4505384910106658\n",
      "LOSS train 1.4505384910106658 valid 2.0196919221627083\n",
      "EPOCH 100:\n",
      "  batch 50 loss: 1.3607376492023469\n",
      "  batch 100 loss: 1.3627318120002747\n",
      "  batch 150 loss: 1.5146881175041198\n",
      "LOSS train 1.5146881175041198 valid 2.0906700993839062\n",
      "EPOCH 101:\n",
      "  batch 50 loss: 1.435033221244812\n",
      "  batch 100 loss: 1.4073537278175354\n",
      "  batch 150 loss: 1.3317328262329102\n",
      "LOSS train 1.3317328262329102 valid 2.0492057580696907\n",
      "EPOCH 102:\n",
      "  batch 50 loss: 1.4237167513370514\n",
      "  batch 100 loss: 1.3847559237480163\n",
      "  batch 150 loss: 1.345084501504898\n",
      "LOSS train 1.345084501504898 valid 2.1555821017215124\n",
      "EPOCH 103:\n",
      "  batch 50 loss: 1.3762574625015258\n",
      "  batch 100 loss: 1.4143419897556304\n",
      "  batch 150 loss: 1.328014669418335\n",
      "LOSS train 1.328014669418335 valid 2.130218935640235\n",
      "EPOCH 104:\n",
      "  batch 50 loss: 1.3685433840751648\n",
      "  batch 100 loss: 1.323388102054596\n",
      "  batch 150 loss: 1.276409068107605\n",
      "LOSS train 1.276409068107605 valid 2.097652381972263\n",
      "EPOCH 105:\n",
      "  batch 50 loss: 1.3006244862079621\n",
      "  batch 100 loss: 1.3086004853248596\n",
      "  batch 150 loss: 1.4035001218318939\n",
      "LOSS train 1.4035001218318939 valid 2.117658489628842\n",
      "EPOCH 106:\n",
      "  batch 50 loss: 1.3813871216773987\n",
      "  batch 100 loss: 1.4640573692321777\n",
      "  batch 150 loss: 1.314382212162018\n",
      "LOSS train 1.314382212162018 valid 2.0424379675011886\n",
      "EPOCH 107:\n",
      "  batch 50 loss: 1.278831994533539\n",
      "  batch 100 loss: 1.4133510529994964\n",
      "  batch 150 loss: 1.346293261051178\n",
      "LOSS train 1.346293261051178 valid 2.0707298485856307\n",
      "EPOCH 108:\n",
      "  batch 50 loss: 1.3709396982192994\n",
      "  batch 100 loss: 1.3316259968280793\n",
      "  batch 150 loss: 1.2455937469005585\n",
      "LOSS train 1.2455937469005585 valid 2.0721240859282646\n",
      "EPOCH 109:\n",
      "  batch 50 loss: 1.3158736503124238\n",
      "  batch 100 loss: 1.2988735055923462\n",
      "  batch 150 loss: 1.3793290376663208\n",
      "LOSS train 1.3793290376663208 valid 2.0865768445165536\n",
      "EPOCH 110:\n",
      "  batch 50 loss: 1.2514816641807556\n",
      "  batch 100 loss: 1.3515256416797639\n",
      "  batch 150 loss: 1.4363037836551666\n",
      "LOSS train 1.4363037836551666 valid 2.084407495824914\n",
      "EPOCH 111:\n",
      "  batch 50 loss: 1.3280670702457429\n",
      "  batch 100 loss: 1.238550900220871\n",
      "  batch 150 loss: 1.3288545334339141\n",
      "LOSS train 1.3288545334339141 valid 2.1379548373975252\n",
      "EPOCH 112:\n",
      "  batch 50 loss: 1.2747441065311431\n",
      "  batch 100 loss: 1.3529025876522065\n",
      "  batch 150 loss: 1.3380261993408202\n",
      "LOSS train 1.3380261993408202 valid 2.092793847385206\n",
      "EPOCH 113:\n",
      "  batch 50 loss: 1.2138405454158783\n",
      "  batch 100 loss: 1.2667097735404969\n",
      "  batch 150 loss: 1.3835710453987122\n",
      "LOSS train 1.3835710453987122 valid 2.0979047794091072\n",
      "EPOCH 114:\n",
      "  batch 50 loss: 1.336719913482666\n",
      "  batch 100 loss: 1.3165723836421968\n",
      "  batch 150 loss: 1.2191981673240662\n",
      "LOSS train 1.2191981673240662 valid 2.1058152976788973\n",
      "EPOCH 115:\n",
      "  batch 50 loss: 1.3907275795936584\n",
      "  batch 100 loss: 1.2859561610221864\n",
      "  batch 150 loss: 1.1496479165554048\n",
      "LOSS train 1.1496479165554048 valid 2.0758789150338424\n",
      "EPOCH 116:\n",
      "  batch 50 loss: 1.2497094452381134\n",
      "  batch 100 loss: 1.3123963731527328\n",
      "  batch 150 loss: 1.3017958199977875\n",
      "LOSS train 1.3017958199977875 valid 2.1160905518029867\n",
      "EPOCH 117:\n",
      "  batch 50 loss: 1.334328565597534\n",
      "  batch 100 loss: 1.3162186920642853\n",
      "  batch 150 loss: 1.2513656985759736\n",
      "LOSS train 1.2513656985759736 valid 2.0148645466879795\n",
      "EPOCH 118:\n",
      "  batch 50 loss: 1.2675521647930146\n",
      "  batch 100 loss: 1.2913282406330109\n",
      "  batch 150 loss: 1.2686536419391632\n",
      "LOSS train 1.2686536419391632 valid 2.047533410160165\n",
      "EPOCH 119:\n",
      "  batch 50 loss: 1.3004178130626678\n",
      "  batch 100 loss: 1.2876394808292388\n",
      "  batch 150 loss: 1.282173764705658\n",
      "LOSS train 1.282173764705658 valid 2.1190787443989203\n",
      "EPOCH 120:\n",
      "  batch 50 loss: 1.3111818552017211\n",
      "  batch 100 loss: 1.3056158995628357\n",
      "  batch 150 loss: 1.224646165370941\n",
      "LOSS train 1.224646165370941 valid 2.089909547253659\n",
      "EPOCH 121:\n",
      "  batch 50 loss: 1.296839646100998\n",
      "  batch 100 loss: 1.2743271696567535\n",
      "  batch 150 loss: 1.2197330152988435\n",
      "LOSS train 1.2197330152988435 valid 2.0105478073421277\n",
      "EPOCH 122:\n",
      "  batch 50 loss: 1.295003024339676\n",
      "  batch 100 loss: 1.250992237329483\n",
      "  batch 150 loss: 1.2888013535737992\n",
      "LOSS train 1.2888013535737992 valid 1.965702938406091\n",
      "EPOCH 123:\n",
      "  batch 50 loss: 1.3348889195919036\n",
      "  batch 100 loss: 1.1985122132301331\n",
      "  batch 150 loss: 1.1542434883117676\n",
      "LOSS train 1.1542434883117676 valid 2.0039205080584477\n",
      "EPOCH 124:\n",
      "  batch 50 loss: 1.1826618099212647\n",
      "  batch 100 loss: 1.294021487236023\n",
      "  batch 150 loss: 1.225839902162552\n",
      "LOSS train 1.225839902162552 valid 2.033379761796249\n",
      "EPOCH 125:\n",
      "  batch 50 loss: 1.1824888181686402\n",
      "  batch 100 loss: 1.127064400911331\n",
      "  batch 150 loss: 1.3084998905658722\n",
      "LOSS train 1.3084998905658722 valid 1.9725719251130756\n",
      "EPOCH 126:\n",
      "  batch 50 loss: 1.2329035604000091\n",
      "  batch 100 loss: 1.1807522416114806\n",
      "  batch 150 loss: 1.2323875796794892\n",
      "LOSS train 1.2323875796794892 valid 2.033182608453851\n",
      "EPOCH 127:\n",
      "  batch 50 loss: 1.2875394868850707\n",
      "  batch 100 loss: 1.1296318703889847\n",
      "  batch 150 loss: 1.2041757667064668\n",
      "LOSS train 1.2041757667064668 valid 2.0161279032104895\n",
      "EPOCH 128:\n",
      "  batch 50 loss: 1.1615732562541963\n",
      "  batch 100 loss: 1.2287231659889222\n",
      "  batch 150 loss: 1.1656123399734497\n",
      "LOSS train 1.1656123399734497 valid 2.0415429739575637\n",
      "EPOCH 129:\n",
      "  batch 50 loss: 1.2360919654369353\n",
      "  batch 100 loss: 1.3126016223430634\n",
      "  batch 150 loss: 1.1651752161979676\n",
      "LOSS train 1.1651752161979676 valid 2.0767313116475155\n",
      "EPOCH 130:\n",
      "  batch 50 loss: 1.1929500246047973\n",
      "  batch 100 loss: 1.2105985414981841\n",
      "  batch 150 loss: 1.167651674747467\n",
      "LOSS train 1.167651674747467 valid 2.0394219662013806\n",
      "EPOCH 131:\n",
      "  batch 50 loss: 1.178547846674919\n",
      "  batch 100 loss: 1.1861572992801666\n",
      "  batch 150 loss: 1.1964081621170044\n",
      "LOSS train 1.1964081621170044 valid 2.0466674534898055\n",
      "EPOCH 132:\n",
      "  batch 50 loss: 1.150829069018364\n",
      "  batch 100 loss: 1.2299256801605225\n",
      "  batch 150 loss: 1.1559421551227569\n",
      "LOSS train 1.1559421551227569 valid 2.027553164645245\n",
      "EPOCH 133:\n",
      "  batch 50 loss: 1.119543468952179\n",
      "  batch 100 loss: 1.1646958458423615\n",
      "  batch 150 loss: 1.2628768229484557\n",
      "LOSS train 1.2628768229484557 valid 1.967953434115962\n",
      "EPOCH 134:\n",
      "  batch 50 loss: 1.197778193950653\n",
      "  batch 100 loss: 1.2218105697631836\n",
      "  batch 150 loss: 1.2332378911972046\n",
      "LOSS train 1.2332378911972046 valid 1.9895861776251542\n",
      "EPOCH 135:\n",
      "  batch 50 loss: 1.1521165508031845\n",
      "  batch 100 loss: 1.1530228781700134\n",
      "  batch 150 loss: 1.15728018283844\n",
      "LOSS train 1.15728018283844 valid 2.040711434263932\n",
      "EPOCH 136:\n",
      "  batch 50 loss: 1.1513066148757936\n",
      "  batch 100 loss: 1.193017510175705\n",
      "  batch 150 loss: 1.1400407505035401\n",
      "LOSS train 1.1400407505035401 valid 2.038024365901947\n",
      "EPOCH 137:\n",
      "  batch 50 loss: 1.1915036916732789\n",
      "  batch 100 loss: 1.1240674090385436\n",
      "  batch 150 loss: 1.2044071567058563\n",
      "LOSS train 1.2044071567058563 valid 2.0454070505342985\n",
      "EPOCH 138:\n",
      "  batch 50 loss: 1.0842648661136627\n",
      "  batch 100 loss: 1.1121338784694672\n",
      "  batch 150 loss: 1.2560626196861266\n",
      "LOSS train 1.2560626196861266 valid 2.0450298660679866\n",
      "EPOCH 139:\n",
      "  batch 50 loss: 1.1727483522892\n",
      "  batch 100 loss: 1.12822265625\n",
      "  batch 150 loss: 1.0397610318660737\n",
      "LOSS train 1.0397610318660737 valid 2.036717869733509\n",
      "EPOCH 140:\n",
      "  batch 50 loss: 1.1464523166418075\n",
      "  batch 100 loss: 1.1006981670856475\n",
      "  batch 150 loss: 1.165911204814911\n",
      "LOSS train 1.165911204814911 valid 1.98185753979181\n",
      "EPOCH 141:\n",
      "  batch 50 loss: 1.1622766411304475\n",
      "  batch 100 loss: 1.0888331604003907\n",
      "  batch 150 loss: 1.1344518214464188\n",
      "LOSS train 1.1344518214464188 valid 2.0335754014943777\n",
      "EPOCH 142:\n",
      "  batch 50 loss: 1.1662536895275115\n",
      "  batch 100 loss: 1.1404710698127747\n",
      "  batch 150 loss: 1.13997704744339\n",
      "LOSS train 1.13997704744339 valid 2.0552108225069547\n",
      "EPOCH 143:\n",
      "  batch 50 loss: 1.1162115395069123\n",
      "  batch 100 loss: 1.0806465184688567\n",
      "  batch 150 loss: 1.161300709247589\n",
      "LOSS train 1.161300709247589 valid 2.092835125170256\n",
      "EPOCH 144:\n",
      "  batch 50 loss: 1.1874901735782624\n",
      "  batch 100 loss: 1.0455909591913224\n",
      "  batch 150 loss: 1.1534564638137816\n",
      "LOSS train 1.1534564638137816 valid 2.0652848074310706\n",
      "EPOCH 145:\n",
      "  batch 50 loss: 1.0801363801956176\n",
      "  batch 100 loss: 1.138722540140152\n",
      "  batch 150 loss: 1.0670050752162934\n",
      "LOSS train 1.0670050752162934 valid 2.013752802422172\n",
      "EPOCH 146:\n",
      "  batch 50 loss: 1.1862910640239717\n",
      "  batch 100 loss: 1.0898989820480347\n",
      "  batch 150 loss: 1.0823032939434052\n",
      "LOSS train 1.0823032939434052 valid 2.0153858693022477\n",
      "EPOCH 147:\n",
      "  batch 50 loss: 1.1064804422855377\n",
      "  batch 100 loss: 1.1448564743995666\n",
      "  batch 150 loss: 1.1026369655132293\n",
      "LOSS train 1.1026369655132293 valid 2.0779998835764433\n",
      "EPOCH 148:\n",
      "  batch 50 loss: 1.0621918988227845\n",
      "  batch 100 loss: 1.0856273293495178\n",
      "  batch 150 loss: 1.1637700724601745\n",
      "LOSS train 1.1637700724601745 valid 2.013809859752655\n",
      "EPOCH 149:\n",
      "  batch 50 loss: 1.1087101888656616\n",
      "  batch 100 loss: 1.1117310392856599\n",
      "  batch 150 loss: 1.075890337228775\n",
      "LOSS train 1.075890337228775 valid 2.027202207791178\n",
      "EPOCH 150:\n",
      "  batch 50 loss: 1.1523590195178985\n",
      "  batch 100 loss: 1.0664737391471864\n",
      "  batch 150 loss: 1.0347053748369217\n",
      "LOSS train 1.0347053748369217 valid 2.016792289520565\n",
      "EPOCH 151:\n",
      "  batch 50 loss: 1.0986960291862489\n",
      "  batch 100 loss: 1.1332154506444931\n",
      "  batch 150 loss: 1.0893882417678833\n",
      "LOSS train 1.0893882417678833 valid 2.008873390524011\n",
      "EPOCH 152:\n",
      "  batch 50 loss: 1.137054535150528\n",
      "  batch 100 loss: 1.0302826654911041\n",
      "  batch 150 loss: 1.0897073090076446\n",
      "LOSS train 1.0897073090076446 valid 2.0084771906074725\n",
      "EPOCH 153:\n",
      "  batch 50 loss: 1.1438605356216431\n",
      "  batch 100 loss: 1.0739175647497177\n",
      "  batch 150 loss: 1.064186007976532\n",
      "LOSS train 1.064186007976532 valid 2.0666515262503373\n",
      "EPOCH 154:\n",
      "  batch 50 loss: 1.0373705983161927\n",
      "  batch 100 loss: 1.0889909821748733\n",
      "  batch 150 loss: 1.0547872722148894\n",
      "LOSS train 1.0547872722148894 valid 2.0242139885300086\n",
      "EPOCH 155:\n",
      "  batch 50 loss: 1.04057550907135\n",
      "  batch 100 loss: 1.0041213834285736\n",
      "  batch 150 loss: 1.1461335325241089\n",
      "LOSS train 1.1461335325241089 valid 2.0768854806297705\n",
      "EPOCH 156:\n",
      "  batch 50 loss: 1.1182186901569366\n",
      "  batch 100 loss: 1.1421996939182282\n",
      "  batch 150 loss: 1.004413330554962\n",
      "LOSS train 1.004413330554962 valid 2.0996171675230326\n",
      "EPOCH 157:\n",
      "  batch 50 loss: 1.0007867121696472\n",
      "  batch 100 loss: 1.0967493975162506\n",
      "  batch 150 loss: 1.0595277285575866\n",
      "LOSS train 1.0595277285575866 valid 2.054913919222982\n",
      "EPOCH 158:\n",
      "  batch 50 loss: 1.0971855199337006\n",
      "  batch 100 loss: 0.9838963860273361\n",
      "  batch 150 loss: 1.0846399784088134\n",
      "LOSS train 1.0846399784088134 valid 2.106776918235578\n",
      "EPOCH 159:\n",
      "  batch 50 loss: 1.0348698335886002\n",
      "  batch 100 loss: 1.1121888309717178\n",
      "  batch 150 loss: 0.9838765668869018\n",
      "LOSS train 0.9838765668869018 valid 2.063232801462475\n",
      "EPOCH 160:\n",
      "  batch 50 loss: 1.0521693855524064\n",
      "  batch 100 loss: 1.1369338965415954\n",
      "  batch 150 loss: 0.9859665310382844\n",
      "LOSS train 0.9859665310382844 valid 2.122057362606651\n",
      "EPOCH 161:\n",
      "  batch 50 loss: 1.0447787857055664\n",
      "  batch 100 loss: 1.0723138391971587\n",
      "  batch 150 loss: 1.0268030285835266\n",
      "LOSS train 1.0268030285835266 valid 2.1145208164265283\n",
      "EPOCH 162:\n",
      "  batch 50 loss: 1.0515265047550202\n",
      "  batch 100 loss: 0.9824920701980591\n",
      "  batch 150 loss: 1.0612207674980163\n",
      "LOSS train 1.0612207674980163 valid 2.081573908266268\n",
      "EPOCH 163:\n",
      "  batch 50 loss: 0.9586042553186417\n",
      "  batch 100 loss: 1.0821856236457825\n",
      "  batch 150 loss: 1.0613776898384095\n",
      "LOSS train 1.0613776898384095 valid 2.0733930029367147\n",
      "EPOCH 164:\n",
      "  batch 50 loss: 1.0154236203432083\n",
      "  batch 100 loss: 1.0222502380609513\n",
      "  batch 150 loss: 1.0627879416942596\n",
      "LOSS train 1.0627879416942596 valid 2.0397166139201115\n",
      "EPOCH 165:\n",
      "  batch 50 loss: 0.9927845543622971\n",
      "  batch 100 loss: 0.9529511040449142\n",
      "  batch 150 loss: 1.059681566953659\n",
      "LOSS train 1.059681566953659 valid 2.0087269842624664\n",
      "EPOCH 166:\n",
      "  batch 50 loss: 1.090854271054268\n",
      "  batch 100 loss: 0.9872816681861878\n",
      "  batch 150 loss: 0.9916638767719269\n",
      "LOSS train 0.9916638767719269 valid 2.098327439082296\n",
      "EPOCH 167:\n",
      "  batch 50 loss: 0.9633088874816894\n",
      "  batch 100 loss: 0.9484183239936829\n",
      "  batch 150 loss: 1.0538137364387512\n",
      "LOSS train 1.0538137364387512 valid 1.963238222034354\n",
      "EPOCH 168:\n",
      "  batch 50 loss: 0.998013334274292\n",
      "  batch 100 loss: 1.0235047006607056\n",
      "  batch 150 loss: 0.9317073881626129\n",
      "LOSS train 0.9317073881626129 valid 2.0392640295781588\n",
      "EPOCH 169:\n",
      "  batch 50 loss: 0.961119070649147\n",
      "  batch 100 loss: 0.9583845365047455\n",
      "  batch 150 loss: 1.0806469678878785\n",
      "LOSS train 1.0806469678878785 valid 2.1378376970165656\n",
      "EPOCH 170:\n",
      "  batch 50 loss: 0.9694508183002472\n",
      "  batch 100 loss: 1.0078433537483216\n",
      "  batch 150 loss: 1.0029187059402467\n",
      "LOSS train 1.0029187059402467 valid 1.9541556097959216\n",
      "EPOCH 171:\n",
      "  batch 50 loss: 1.0466645109653472\n",
      "  batch 100 loss: 1.0539660918712617\n",
      "  batch 150 loss: 0.9810740077495574\n",
      "LOSS train 0.9810740077495574 valid 1.999313523894862\n",
      "EPOCH 172:\n",
      "  batch 50 loss: 1.04239164352417\n",
      "  batch 100 loss: 0.9754705303907394\n",
      "  batch 150 loss: 1.0010269170999526\n",
      "LOSS train 1.0010269170999526 valid 2.0177818003453707\n",
      "EPOCH 173:\n",
      "  batch 50 loss: 0.9559867942333221\n",
      "  batch 100 loss: 1.019512681365013\n",
      "  batch 150 loss: 1.004498654603958\n",
      "LOSS train 1.004498654603958 valid 1.9972843072916333\n",
      "EPOCH 174:\n",
      "  batch 50 loss: 0.9027874612808228\n",
      "  batch 100 loss: 0.9963243770599365\n",
      "  batch 150 loss: 1.0015408778190613\n",
      "LOSS train 1.0015408778190613 valid 2.079791934866654\n",
      "EPOCH 175:\n",
      "  batch 50 loss: 0.9767146021127701\n",
      "  batch 100 loss: 1.027450326681137\n",
      "  batch 150 loss: 0.988284957408905\n",
      "LOSS train 0.988284957408905 valid 2.0046223402023315\n",
      "EPOCH 176:\n",
      "  batch 50 loss: 0.97368497133255\n",
      "  batch 100 loss: 0.9862385654449463\n",
      "  batch 150 loss: 0.954437837600708\n",
      "LOSS train 0.954437837600708 valid 1.9935315423890163\n",
      "EPOCH 177:\n",
      "  batch 50 loss: 0.9637721800804138\n",
      "  batch 100 loss: 1.0134643989801406\n",
      "  batch 150 loss: 0.9520910102128982\n",
      "LOSS train 0.9520910102128982 valid 2.066035656552566\n",
      "EPOCH 178:\n",
      "  batch 50 loss: 0.9452189671993255\n",
      "  batch 100 loss: 0.9465567421913147\n",
      "  batch 150 loss: 0.9790026235580445\n",
      "LOSS train 0.9790026235580445 valid 2.0257795060935773\n",
      "EPOCH 179:\n",
      "  batch 50 loss: 1.0280311179161072\n",
      "  batch 100 loss: 0.9501027035713195\n",
      "  batch 150 loss: 1.0123759722709655\n",
      "LOSS train 1.0123759722709655 valid 2.0854533251963163\n",
      "EPOCH 180:\n",
      "  batch 50 loss: 0.9576305496692658\n",
      "  batch 100 loss: 0.9175327783823013\n",
      "  batch 150 loss: 1.044205175638199\n",
      "LOSS train 1.044205175638199 valid 2.040551946351403\n",
      "EPOCH 181:\n",
      "  batch 50 loss: 1.021141009926796\n",
      "  batch 100 loss: 0.9174217545986175\n",
      "  batch 150 loss: 0.9527195167541503\n",
      "LOSS train 0.9527195167541503 valid 2.0192007642043266\n",
      "EPOCH 182:\n",
      "  batch 50 loss: 1.056916856765747\n",
      "  batch 100 loss: 0.8792023181915283\n",
      "  batch 150 loss: 0.9058998328447342\n",
      "LOSS train 0.9058998328447342 valid 2.0489272381130017\n",
      "EPOCH 183:\n",
      "  batch 50 loss: 0.9590649825334548\n",
      "  batch 100 loss: 0.9838190996646881\n",
      "  batch 150 loss: 1.0423164343833924\n",
      "LOSS train 1.0423164343833924 valid 2.015741106710936\n",
      "EPOCH 184:\n",
      "  batch 50 loss: 0.8841322410106659\n",
      "  batch 100 loss: 0.9747977727651596\n",
      "  batch 150 loss: 0.9560306763648987\n",
      "LOSS train 0.9560306763648987 valid 2.1181434667424153\n",
      "EPOCH 185:\n",
      "  batch 50 loss: 0.9127595710754395\n",
      "  batch 100 loss: 0.9267216223478317\n",
      "  batch 150 loss: 0.9424441516399383\n",
      "LOSS train 0.9424441516399383 valid 2.0709471828059147\n",
      "EPOCH 186:\n",
      "  batch 50 loss: 0.9846842783689499\n",
      "  batch 100 loss: 0.9202431845664978\n",
      "  batch 150 loss: 0.9097601473331451\n",
      "LOSS train 0.9097601473331451 valid 2.0421767987703023\n",
      "EPOCH 187:\n",
      "  batch 50 loss: 1.014735653400421\n",
      "  batch 100 loss: 0.9837709128856659\n",
      "  batch 150 loss: 0.9239982700347901\n",
      "LOSS train 0.9239982700347901 valid 2.1844333692600855\n",
      "EPOCH 188:\n",
      "  batch 50 loss: 1.021191964149475\n",
      "  batch 100 loss: 0.8903271031379699\n",
      "  batch 150 loss: 0.9349737787246704\n",
      "LOSS train 0.9349737787246704 valid 2.1362685137673427\n",
      "EPOCH 189:\n",
      "  batch 50 loss: 0.964213811159134\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 49\u001B[0m\n\u001B[1;32m     46\u001B[0m tot_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     47\u001B[0m opt\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m---> 49\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[43mtot_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mitem(), fine_loss\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mitem(), coarse_loss\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mitem()])\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m50\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m49\u001B[39m:\n\u001B[1;32m     51\u001B[0m     last_loss \u001B[38;5;241m=\u001B[39m running_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m50\u001B[39m \u001B[38;5;66;03m# loss per batch\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models.prectime import PrecTime\n",
    "from loss import WeightedCELoss\n",
    "\n",
    "cols = ['Inclination (deg)', 'Longitude (deg)', 'Eccentricity', 'Semimajor Axis (m)']\n",
    "classes = ['ES-ES', 'SS-CK', 'SS-EK', 'SS-HK', 'SS-NK', 'IK-CK', 'IK-EK', 'IK-HK', 'ID-NK', 'AD-NK']\n",
    "trn_data = SPLID(train_datalist, ground_truth, cols, classes=classes)\n",
    "tst_data = SPLID(test_datalist, ground_truth, cols, classes=classes)\n",
    "\n",
    "trn_loader = data.DataLoader(trn_data, shuffle=True, batch_size=10)\n",
    "tst_loader = data.DataLoader(tst_data, shuffle=True, batch_size=10)\n",
    "\n",
    "lr = 5e-6\n",
    "n_epochs = 1000\n",
    "best_tst_loss = 1_000_000.\n",
    "\n",
    "model = PrecTime(len(classes), n_win=92, l_win=24, c_in=len(cols), c_conv=128)\n",
    "model = model.cuda()\n",
    "criterion = WeightedCELoss()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "print('Start model training')\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/splid_trainer_{}'.format(timestamp))\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    print('EPOCH {}:'.format(epoch))\n",
    "    \n",
    "    running_loss = np.zeros(3)\n",
    "    last_loss = np.zeros(3)\n",
    "    model.train(True)\n",
    "    \n",
    "    for i, (x_batch, y_batch) in enumerate(trn_loader):\n",
    "        \n",
    "        x_batch = x_batch.cuda()\n",
    "        y_batch = y_batch.cuda()\n",
    "        # sched.step()\n",
    "        opt.zero_grad()\n",
    "        fine_out, coarse_out = model(x_batch)\n",
    "        tot_loss, fine_loss, coarse_loss = criterion(fine_out, coarse_out, y_batch)\n",
    "        tot_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += np.array([tot_loss.cpu().item(), fine_loss.cpu().item(), coarse_loss.cpu().item()])\n",
    "        if i % 50 == 49:\n",
    "            last_loss = running_loss / 50 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss[0]))\n",
    "            tb_x = epoch * len(trn_loader) + i + 1\n",
    "            writer.add_scalar('TotLoss/train', last_loss[0], tb_x)\n",
    "            writer.add_scalar('FineLoss/train', last_loss[1], tb_x)\n",
    "            writer.add_scalar('CoarseLoss/train', last_loss[2], tb_x)\n",
    "            running_loss = np.zeros(3)\n",
    "    \n",
    "    running_tst_loss = np.zeros(3)\n",
    "    model.eval()\n",
    "    \n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, (x_batch, y_batch) in enumerate(tst_loader):\n",
    "            x_batch = x_batch.cuda()\n",
    "            y_batch = y_batch.cuda()\n",
    "            fine_out, coarse_out = model(x_batch)\n",
    "            tot_tst_loss, fine_tst_loss, coarse_tst_loss = criterion(fine_out, coarse_out, y_batch)\n",
    "            running_tst_loss += np.array([tot_tst_loss.cpu(), fine_tst_loss.cpu(), coarse_tst_loss.cpu()])\n",
    "    \n",
    "    avg_tst_loss = running_tst_loss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(last_loss[0], avg_tst_loss[0]))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Total Loss',\n",
    "                    { 'Training' : last_loss[0], 'Validation' : avg_tst_loss[0] },\n",
    "                    epoch)\n",
    "    writer.add_scalars('Training vs. Validation Fine Loss',\n",
    "                    { 'Training' : last_loss[1], 'Validation' : avg_tst_loss[1] },\n",
    "                    epoch)\n",
    "    writer.add_scalars('Training vs. Validation Coarse Loss',\n",
    "                    { 'Training' : last_loss[2], 'Validation' : avg_tst_loss[2] },\n",
    "                    epoch)\n",
    "    writer.flush()\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_tst_loss[0] < best_tst_loss:\n",
    "        best_tst_loss = avg_tst_loss[0]\n",
    "        model_path = 'model_{}.pth'.format(timestamp)\n",
    "        torch.save(model.state_dict(), 'saved_models/' + model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test = torch.arange(200)\n",
    "test = torch.reshape(test, (4, 2, 25))\n",
    "test2 = torch.arange(0, -200, -1)\n",
    "test2 = torch.reshape(test2, (4, 2, 25))\n",
    "m = torch.cat((test, test2), dim=1)\n",
    "m\n",
    "# m = torch.permute(m, (0, 2, 1))\n",
    "# torch.reshape(m, (4, 5, 20))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
