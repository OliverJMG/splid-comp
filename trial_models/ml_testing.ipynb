{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T02:58:25.310104892Z",
     "start_time": "2024-02-17T02:58:23.501630463Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from fastcore.basics import Path, AttrDict\n",
    "from dataset import SPLID\n",
    "import torch\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T02:58:33.089464991Z",
     "start_time": "2024-02-17T02:58:33.080294176Z"
    }
   },
   "outputs": [],
   "source": [
    "config = AttrDict(\n",
    "    challenge_data_dir = Path('~/Projects/splid-comp/dataset').expanduser(),\n",
    "    valid_ratio = 0.1,\n",
    "    kernel_size = 5,\n",
    "    tolerance= 6, # Default evaluation tolerance\n",
    ")\n",
    "\n",
    "# Define the directory paths\n",
    "train_data_dir = config.challenge_data_dir / \"train_v2\"\n",
    "\n",
    "# Load the ground truth data\n",
    "ground_truth = config.challenge_data_dir / 'train_labels_v2.csv'\n",
    "\n",
    "datalist = []\n",
    "\n",
    "# Searching for training data within the dataset folder\n",
    "for file in os.listdir(train_data_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        datalist.append(os.path.join(train_data_dir, file))\n",
    "\n",
    "# Sort the training data and labels\n",
    "datalist = sorted(datalist, key=lambda i: int(os.path.splitext(os.path.basename(i))[0]))\n",
    "\n",
    "\n",
    "train_datalist, test_datalist = train_test_split(datalist, test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T09:17:24.335360358Z",
     "start_time": "2024-02-17T02:58:34.471064223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1425 files...\n",
      "Loaded file 0 of 1425\n",
      "Loaded file 100 of 1425\n",
      "Loaded file 150 of 1425\n",
      "Loaded file 200 of 1425\n",
      "Loaded file 250 of 1425\n",
      "Loaded file 300 of 1425\n",
      "Loaded file 400 of 1425\n",
      "Loaded file 450 of 1425\n",
      "Loaded file 500 of 1425\n",
      "Loaded file 550 of 1425\n",
      "Loaded file 600 of 1425\n",
      "Loaded file 650 of 1425\n",
      "Loaded file 700 of 1425\n",
      "Loaded file 750 of 1425\n",
      "Loaded file 800 of 1425\n",
      "Loaded file 850 of 1425\n",
      "Loaded file 900 of 1425\n",
      "Loaded file 950 of 1425\n",
      "Loaded file 1000 of 1425\n",
      "Loaded file 1100 of 1425\n",
      "Loaded file 1150 of 1425\n",
      "Loaded file 1200 of 1425\n",
      "Loaded file 1250 of 1425\n",
      "Loaded file 1300 of 1425\n",
      "Loaded file 1350 of 1425\n",
      "Loaded file 1400 of 1425\n",
      "Joining dataframes...\n",
      "Done!\n",
      "Loading 475 files...\n",
      "Loaded file 0 of 475\n",
      "Loaded file 100 of 475\n",
      "Loaded file 150 of 475\n",
      "Loaded file 200 of 475\n",
      "Loaded file 250 of 475\n",
      "Loaded file 300 of 475\n",
      "Loaded file 350 of 475\n",
      "Loaded file 400 of 475\n",
      "Loaded file 450 of 475\n",
      "Joining dataframes...\n",
      "Done!\n",
      "Start model training\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 2.33897198677063\n",
      "  batch 100 loss: 2.3369680738449095\n",
      "LOSS train 2.3369680738449095 valid 2.33443546295166\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 2.3260074043273926\n",
      "  batch 100 loss: 2.325981478691101\n",
      "LOSS train 2.325981478691101 valid 2.3653371334075928\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 2.31608736038208\n",
      "  batch 100 loss: 2.3259675121307373\n",
      "LOSS train 2.3259675121307373 valid 2.367635726928711\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 2.312997989654541\n",
      "  batch 100 loss: 2.3126342821121217\n",
      "LOSS train 2.3126342821121217 valid 2.358752489089966\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 2.3084530782699586\n",
      "  batch 100 loss: 2.3058362722396852\n",
      "LOSS train 2.3058362722396852 valid 2.353315830230713\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 2.2962727117538453\n",
      "  batch 100 loss: 2.3043206214904783\n",
      "LOSS train 2.3043206214904783 valid 2.3524043560028076\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 2.296991157531738\n",
      "  batch 100 loss: 2.290769810676575\n",
      "LOSS train 2.290769810676575 valid 2.3459157943725586\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 2.2914668893814087\n",
      "  batch 100 loss: 2.2845163536071778\n",
      "LOSS train 2.2845163536071778 valid 2.3503034114837646\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 2.283303589820862\n",
      "  batch 100 loss: 2.2820281171798706\n",
      "LOSS train 2.2820281171798706 valid 2.3247148990631104\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 2.275290665626526\n",
      "  batch 100 loss: 2.284611339569092\n",
      "LOSS train 2.284611339569092 valid 2.3436169624328613\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 2.2766366243362426\n",
      "  batch 100 loss: 2.277202877998352\n",
      "LOSS train 2.277202877998352 valid 2.3224852085113525\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 2.273042039871216\n",
      "  batch 100 loss: 2.2705706548690796\n",
      "LOSS train 2.2705706548690796 valid 2.3216512203216553\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 2.271907777786255\n",
      "  batch 100 loss: 2.2629648351669314\n",
      "LOSS train 2.2629648351669314 valid 2.308098316192627\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 2.2645636129379274\n",
      "  batch 100 loss: 2.2629599571228027\n",
      "LOSS train 2.2629599571228027 valid 2.3106777667999268\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 2.2641094732284546\n",
      "  batch 100 loss: 2.261652569770813\n",
      "LOSS train 2.261652569770813 valid 2.318570852279663\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 2.252519669532776\n",
      "  batch 100 loss: 2.2568997383117675\n",
      "LOSS train 2.2568997383117675 valid 2.3006670475006104\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 2.2483142757415773\n",
      "  batch 100 loss: 2.2570114660263063\n",
      "LOSS train 2.2570114660263063 valid 2.306994676589966\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 2.251329255104065\n",
      "  batch 100 loss: 2.2505975484848024\n",
      "LOSS train 2.2505975484848024 valid 2.309649705886841\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 2.24917489528656\n",
      "  batch 100 loss: 2.2455446672439576\n",
      "LOSS train 2.2455446672439576 valid 2.2851226329803467\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 2.2452655744552614\n",
      "  batch 100 loss: 2.240080270767212\n",
      "LOSS train 2.240080270767212 valid 2.291555881500244\n",
      "EPOCH 21:\n",
      "  batch 50 loss: 2.240590753555298\n",
      "  batch 100 loss: 2.236426191329956\n",
      "LOSS train 2.236426191329956 valid 2.287806749343872\n",
      "EPOCH 22:\n",
      "  batch 50 loss: 2.2381394624710085\n",
      "  batch 100 loss: 2.232537922859192\n",
      "LOSS train 2.232537922859192 valid 2.2704555988311768\n",
      "EPOCH 23:\n",
      "  batch 50 loss: 2.233022952079773\n",
      "  batch 100 loss: 2.229973483085632\n",
      "LOSS train 2.229973483085632 valid 2.2761752605438232\n",
      "EPOCH 24:\n",
      "  batch 50 loss: 2.2301377296447753\n",
      "  batch 100 loss: 2.2276215744018555\n",
      "LOSS train 2.2276215744018555 valid 2.277639150619507\n",
      "EPOCH 25:\n",
      "  batch 50 loss: 2.2236205625534056\n",
      "  batch 100 loss: 2.2214591455459596\n",
      "LOSS train 2.2214591455459596 valid 2.2667815685272217\n",
      "EPOCH 26:\n",
      "  batch 50 loss: 2.220701804161072\n",
      "  batch 100 loss: 2.2203724575042725\n",
      "LOSS train 2.2203724575042725 valid 2.2629196643829346\n",
      "EPOCH 27:\n",
      "  batch 50 loss: 2.2223561000823975\n",
      "  batch 100 loss: 2.214802761077881\n",
      "LOSS train 2.214802761077881 valid 2.2616472244262695\n",
      "EPOCH 28:\n",
      "  batch 50 loss: 2.212293543815613\n",
      "  batch 100 loss: 2.2145299530029297\n",
      "LOSS train 2.2145299530029297 valid 2.2551028728485107\n",
      "EPOCH 29:\n",
      "  batch 50 loss: 2.2136167526245116\n",
      "  batch 100 loss: 2.2068667650222777\n",
      "LOSS train 2.2068667650222777 valid 2.2608907222747803\n",
      "EPOCH 30:\n",
      "  batch 50 loss: 2.21130410194397\n",
      "  batch 100 loss: 2.210890522003174\n",
      "LOSS train 2.210890522003174 valid 2.2539210319519043\n",
      "EPOCH 31:\n",
      "  batch 50 loss: 2.2047693967819213\n",
      "  batch 100 loss: 2.2053808212280273\n",
      "LOSS train 2.2053808212280273 valid 2.251629590988159\n",
      "EPOCH 32:\n",
      "  batch 50 loss: 2.2039443016052247\n",
      "  batch 100 loss: 2.1982327032089235\n",
      "LOSS train 2.1982327032089235 valid 2.2453420162200928\n",
      "EPOCH 33:\n",
      "  batch 50 loss: 2.200121431350708\n",
      "  batch 100 loss: 2.19788311958313\n",
      "LOSS train 2.19788311958313 valid 2.237607717514038\n",
      "EPOCH 34:\n",
      "  batch 50 loss: 2.1951970052719116\n",
      "  batch 100 loss: 2.1998350954055788\n",
      "LOSS train 2.1998350954055788 valid 2.239032745361328\n",
      "EPOCH 35:\n",
      "  batch 50 loss: 2.1889049434661865\n",
      "  batch 100 loss: 2.194332003593445\n",
      "LOSS train 2.194332003593445 valid 2.23746919631958\n",
      "EPOCH 36:\n",
      "  batch 50 loss: 2.1879500102996827\n",
      "  batch 100 loss: 2.19134973526001\n",
      "LOSS train 2.19134973526001 valid 2.2351267337799072\n",
      "EPOCH 37:\n",
      "  batch 50 loss: 2.1894124364852905\n",
      "  batch 100 loss: 2.182571392059326\n",
      "LOSS train 2.182571392059326 valid 2.2326202392578125\n",
      "EPOCH 38:\n",
      "  batch 50 loss: 2.183570957183838\n",
      "  batch 100 loss: 2.1852143716812136\n",
      "LOSS train 2.1852143716812136 valid 2.226534128189087\n",
      "EPOCH 39:\n",
      "  batch 50 loss: 2.1788235473632813\n",
      "  batch 100 loss: 2.182388973236084\n",
      "LOSS train 2.182388973236084 valid 2.229979991912842\n",
      "EPOCH 40:\n",
      "  batch 50 loss: 2.1771347427368166\n",
      "  batch 100 loss: 2.1778343963623046\n",
      "LOSS train 2.1778343963623046 valid 2.2249093055725098\n",
      "EPOCH 41:\n",
      "  batch 50 loss: 2.186730351448059\n",
      "  batch 100 loss: 2.167484903335571\n",
      "LOSS train 2.167484903335571 valid 2.220973253250122\n",
      "EPOCH 42:\n",
      "  batch 50 loss: 2.172105407714844\n",
      "  batch 100 loss: 2.1708791780471803\n",
      "LOSS train 2.1708791780471803 valid 2.2184183597564697\n",
      "EPOCH 43:\n",
      "  batch 50 loss: 2.166057562828064\n",
      "  batch 100 loss: 2.169469027519226\n",
      "LOSS train 2.169469027519226 valid 2.210862159729004\n",
      "EPOCH 44:\n",
      "  batch 50 loss: 2.1712939023971556\n",
      "  batch 100 loss: 2.1614131116867066\n",
      "LOSS train 2.1614131116867066 valid 2.2135226726531982\n",
      "EPOCH 45:\n",
      "  batch 50 loss: 2.16396050453186\n",
      "  batch 100 loss: 2.1622012567520144\n",
      "LOSS train 2.1622012567520144 valid 2.2137224674224854\n",
      "EPOCH 46:\n",
      "  batch 50 loss: 2.159817028045654\n",
      "  batch 100 loss: 2.161160821914673\n",
      "LOSS train 2.161160821914673 valid 2.208618402481079\n",
      "EPOCH 47:\n",
      "  batch 50 loss: 2.1603559970855715\n",
      "  batch 100 loss: 2.1602808618545533\n",
      "LOSS train 2.1602808618545533 valid 2.208986520767212\n",
      "EPOCH 48:\n",
      "  batch 50 loss: 2.152620916366577\n",
      "  batch 100 loss: 2.1566632890701296\n",
      "LOSS train 2.1566632890701296 valid 2.207642078399658\n",
      "EPOCH 49:\n",
      "  batch 50 loss: 2.155740694999695\n",
      "  batch 100 loss: 2.147558960914612\n",
      "LOSS train 2.147558960914612 valid 2.215695381164551\n",
      "EPOCH 50:\n",
      "  batch 50 loss: 2.147695279121399\n",
      "  batch 100 loss: 2.1488187313079834\n",
      "LOSS train 2.1488187313079834 valid 2.2025842666625977\n",
      "EPOCH 51:\n",
      "  batch 50 loss: 2.1518139839172363\n",
      "  batch 100 loss: 2.1414432907104493\n",
      "LOSS train 2.1414432907104493 valid 2.197627067565918\n",
      "EPOCH 52:\n",
      "  batch 50 loss: 2.145104422569275\n",
      "  batch 100 loss: 2.140155472755432\n",
      "LOSS train 2.140155472755432 valid 2.1945605278015137\n",
      "EPOCH 53:\n",
      "  batch 50 loss: 2.143332953453064\n",
      "  batch 100 loss: 2.1441547107696532\n",
      "LOSS train 2.1441547107696532 valid 2.192542552947998\n",
      "EPOCH 54:\n",
      "  batch 50 loss: 2.1341940212249755\n",
      "  batch 100 loss: 2.1411695671081543\n",
      "LOSS train 2.1411695671081543 valid 2.2054827213287354\n",
      "EPOCH 55:\n",
      "  batch 50 loss: 2.1355052900314333\n",
      "  batch 100 loss: 2.13557204246521\n",
      "LOSS train 2.13557204246521 valid 2.183168649673462\n",
      "EPOCH 56:\n",
      "  batch 50 loss: 2.13423330783844\n",
      "  batch 100 loss: 2.1305519008636473\n",
      "LOSS train 2.1305519008636473 valid 2.182750940322876\n",
      "EPOCH 57:\n",
      "  batch 50 loss: 2.1327799129486085\n",
      "  batch 100 loss: 2.1309535455703736\n",
      "LOSS train 2.1309535455703736 valid 2.1874890327453613\n",
      "EPOCH 58:\n",
      "  batch 50 loss: 2.1300663328170777\n",
      "  batch 100 loss: 2.125275411605835\n",
      "LOSS train 2.125275411605835 valid 2.18622088432312\n",
      "EPOCH 59:\n",
      "  batch 50 loss: 2.11467405796051\n",
      "  batch 100 loss: 2.1283426666259766\n",
      "LOSS train 2.1283426666259766 valid 2.1823699474334717\n",
      "EPOCH 60:\n",
      "  batch 50 loss: 2.125129747390747\n",
      "  batch 100 loss: 2.1189308547973633\n",
      "LOSS train 2.1189308547973633 valid 2.190445899963379\n",
      "EPOCH 61:\n",
      "  batch 50 loss: 2.1252458572387694\n",
      "  batch 100 loss: 2.114215693473816\n",
      "LOSS train 2.114215693473816 valid 2.185995101928711\n",
      "EPOCH 62:\n",
      "  batch 50 loss: 2.1157586336135865\n",
      "  batch 100 loss: 2.1110807991027833\n",
      "LOSS train 2.1110807991027833 valid 2.170236825942993\n",
      "EPOCH 63:\n",
      "  batch 50 loss: 2.123410534858704\n",
      "  batch 100 loss: 2.1174800252914427\n",
      "LOSS train 2.1174800252914427 valid 2.1628715991973877\n",
      "EPOCH 64:\n",
      "  batch 50 loss: 2.1109017276763917\n",
      "  batch 100 loss: 2.109284906387329\n",
      "LOSS train 2.109284906387329 valid 2.171649694442749\n",
      "EPOCH 65:\n",
      "  batch 50 loss: 2.1080489587783813\n",
      "  batch 100 loss: 2.1108912134170534\n",
      "LOSS train 2.1108912134170534 valid 2.1698830127716064\n",
      "EPOCH 66:\n",
      "  batch 50 loss: 2.1148708772659304\n",
      "  batch 100 loss: 2.1007695722579958\n",
      "LOSS train 2.1007695722579958 valid 2.1691784858703613\n",
      "EPOCH 67:\n",
      "  batch 50 loss: 2.1046045446395873\n",
      "  batch 100 loss: 2.1084160232543945\n",
      "LOSS train 2.1084160232543945 valid 2.1640734672546387\n",
      "EPOCH 68:\n",
      "  batch 50 loss: 2.1028696966171263\n",
      "  batch 100 loss: 2.097963962554932\n",
      "LOSS train 2.097963962554932 valid 2.1619396209716797\n",
      "EPOCH 69:\n",
      "  batch 50 loss: 2.0908647441864012\n",
      "  batch 100 loss: 2.1128612041473387\n",
      "LOSS train 2.1128612041473387 valid 2.1575675010681152\n",
      "EPOCH 70:\n",
      "  batch 50 loss: 2.098844065666199\n",
      "  batch 100 loss: 2.0925016450881957\n",
      "LOSS train 2.0925016450881957 valid 2.1569159030914307\n",
      "EPOCH 71:\n",
      "  batch 50 loss: 2.0940895652770997\n",
      "  batch 100 loss: 2.0958507585525514\n",
      "LOSS train 2.0958507585525514 valid 2.149765968322754\n",
      "EPOCH 72:\n",
      "  batch 50 loss: 2.0928281259536745\n",
      "  batch 100 loss: 2.089388074874878\n",
      "LOSS train 2.089388074874878 valid 2.1501636505126953\n",
      "EPOCH 73:\n",
      "  batch 50 loss: 2.0944683289527894\n",
      "  batch 100 loss: 2.088757848739624\n",
      "LOSS train 2.088757848739624 valid 2.1491005420684814\n",
      "EPOCH 74:\n",
      "  batch 50 loss: 2.087403428554535\n",
      "  batch 100 loss: 2.0877786612510683\n",
      "LOSS train 2.0877786612510683 valid 2.1426663398742676\n",
      "EPOCH 75:\n",
      "  batch 50 loss: 2.0861282968521118\n",
      "  batch 100 loss: 2.0875434064865113\n",
      "LOSS train 2.0875434064865113 valid 2.146909236907959\n",
      "EPOCH 76:\n",
      "  batch 50 loss: 2.079829421043396\n",
      "  batch 100 loss: 2.0801663208007812\n",
      "LOSS train 2.0801663208007812 valid 2.1417839527130127\n",
      "EPOCH 77:\n",
      "  batch 50 loss: 2.0807121634483337\n",
      "  batch 100 loss: 2.0870235681533815\n",
      "LOSS train 2.0870235681533815 valid 2.1404192447662354\n",
      "EPOCH 78:\n",
      "  batch 50 loss: 2.0784285259246826\n",
      "  batch 100 loss: 2.0756528782844543\n",
      "LOSS train 2.0756528782844543 valid 2.1404006481170654\n",
      "EPOCH 79:\n",
      "  batch 50 loss: 2.0703101897239686\n",
      "  batch 100 loss: 2.076580617427826\n",
      "LOSS train 2.076580617427826 valid 2.1301770210266113\n",
      "EPOCH 80:\n",
      "  batch 50 loss: 2.0708569860458375\n",
      "  batch 100 loss: 2.079783101081848\n",
      "LOSS train 2.079783101081848 valid 2.1357555389404297\n",
      "EPOCH 81:\n",
      "  batch 50 loss: 2.0642325687408447\n",
      "  batch 100 loss: 2.0749393987655638\n",
      "LOSS train 2.0749393987655638 valid 2.146958589553833\n",
      "EPOCH 82:\n",
      "  batch 50 loss: 2.0682701230049134\n",
      "  batch 100 loss: 2.070628638267517\n",
      "LOSS train 2.070628638267517 valid 2.1349830627441406\n",
      "EPOCH 83:\n",
      "  batch 50 loss: 2.065696737766266\n",
      "  batch 100 loss: 2.0663509654998777\n",
      "LOSS train 2.0663509654998777 valid 2.1290626525878906\n",
      "EPOCH 84:\n",
      "  batch 50 loss: 2.064901044368744\n",
      "  batch 100 loss: 2.056965651512146\n",
      "LOSS train 2.056965651512146 valid 2.1305770874023438\n",
      "EPOCH 85:\n",
      "  batch 50 loss: 2.069230182170868\n",
      "  batch 100 loss: 2.0502179932594298\n",
      "LOSS train 2.0502179932594298 valid 2.118875026702881\n",
      "EPOCH 86:\n",
      "  batch 50 loss: 2.0712278270721436\n",
      "  batch 100 loss: 2.047098774909973\n",
      "LOSS train 2.047098774909973 valid 2.1191513538360596\n",
      "EPOCH 87:\n",
      "  batch 50 loss: 2.0557756543159487\n",
      "  batch 100 loss: 2.0552059173583985\n",
      "LOSS train 2.0552059173583985 valid 2.147495985031128\n",
      "EPOCH 88:\n",
      "  batch 50 loss: 2.052384762763977\n",
      "  batch 100 loss: 2.0531123328208922\n",
      "LOSS train 2.0531123328208922 valid 2.117666244506836\n",
      "EPOCH 89:\n",
      "  batch 50 loss: 2.0497553372383117\n",
      "  batch 100 loss: 2.0545023250579835\n",
      "LOSS train 2.0545023250579835 valid 2.113553524017334\n",
      "EPOCH 90:\n",
      "  batch 50 loss: 2.0422300267219544\n",
      "  batch 100 loss: 2.0580790996551515\n",
      "LOSS train 2.0580790996551515 valid 2.1136722564697266\n",
      "EPOCH 91:\n",
      "  batch 50 loss: 2.051466991901398\n",
      "  batch 100 loss: 2.0474313950538634\n",
      "LOSS train 2.0474313950538634 valid 2.1090087890625\n",
      "EPOCH 92:\n",
      "  batch 50 loss: 2.0449469470977784\n",
      "  batch 100 loss: 2.044220972061157\n",
      "LOSS train 2.044220972061157 valid 2.106276512145996\n",
      "EPOCH 93:\n",
      "  batch 50 loss: 2.0426006007194517\n",
      "  batch 100 loss: 2.033043203353882\n",
      "LOSS train 2.033043203353882 valid 2.1048402786254883\n",
      "EPOCH 94:\n",
      "  batch 50 loss: 2.0363512206077576\n",
      "  batch 100 loss: 2.046100175380707\n",
      "LOSS train 2.046100175380707 valid 2.107191562652588\n",
      "EPOCH 95:\n",
      "  batch 50 loss: 2.0457354617118835\n",
      "  batch 100 loss: 2.037947533130646\n",
      "LOSS train 2.037947533130646 valid 2.1064512729644775\n",
      "EPOCH 96:\n",
      "  batch 50 loss: 2.0329378414154053\n",
      "  batch 100 loss: 2.0440346693992613\n",
      "LOSS train 2.0440346693992613 valid 2.097052812576294\n",
      "EPOCH 97:\n",
      "  batch 50 loss: 2.0330289244651794\n",
      "  batch 100 loss: 2.0416540813446047\n",
      "LOSS train 2.0416540813446047 valid 2.0984699726104736\n",
      "EPOCH 98:\n",
      "  batch 50 loss: 2.0280939030647276\n",
      "  batch 100 loss: 2.041551461219788\n",
      "LOSS train 2.041551461219788 valid 2.099200487136841\n",
      "EPOCH 99:\n",
      "  batch 50 loss: 2.0291382908821105\n",
      "  batch 100 loss: 2.0331068015098572\n",
      "LOSS train 2.0331068015098572 valid 2.0908100605010986\n",
      "EPOCH 100:\n",
      "  batch 50 loss: 2.025151188373566\n",
      "  batch 100 loss: 2.034524075984955\n",
      "LOSS train 2.034524075984955 valid 2.0882599353790283\n",
      "EPOCH 101:\n",
      "  batch 50 loss: 2.028121705055237\n",
      "  batch 100 loss: 2.0291120195388794\n",
      "LOSS train 2.0291120195388794 valid 2.0956313610076904\n",
      "EPOCH 102:\n",
      "  batch 50 loss: 2.0284344124794007\n",
      "  batch 100 loss: 2.031795868873596\n",
      "LOSS train 2.031795868873596 valid 2.090553045272827\n",
      "EPOCH 103:\n",
      "  batch 50 loss: 2.02865581035614\n",
      "  batch 100 loss: 2.0202654218673706\n",
      "LOSS train 2.0202654218673706 valid 2.109187126159668\n",
      "EPOCH 104:\n",
      "  batch 50 loss: 2.0114788889884947\n",
      "  batch 100 loss: 2.0230311131477356\n",
      "LOSS train 2.0230311131477356 valid 2.0867486000061035\n",
      "EPOCH 105:\n",
      "  batch 50 loss: 2.0145196866989137\n",
      "  batch 100 loss: 2.0198233246803285\n",
      "LOSS train 2.0198233246803285 valid 2.0817291736602783\n",
      "EPOCH 106:\n",
      "  batch 50 loss: 2.013375713825226\n",
      "  batch 100 loss: 2.016693685054779\n",
      "LOSS train 2.016693685054779 valid 2.106149673461914\n",
      "EPOCH 107:\n",
      "  batch 50 loss: 2.018086814880371\n",
      "  batch 100 loss: 2.0288330483436585\n",
      "LOSS train 2.0288330483436585 valid 2.081071376800537\n",
      "EPOCH 108:\n",
      "  batch 50 loss: 1.9988617777824402\n",
      "  batch 100 loss: 2.0195028042793273\n",
      "LOSS train 2.0195028042793273 valid 2.0812642574310303\n",
      "EPOCH 109:\n",
      "  batch 50 loss: 2.013888208866119\n",
      "  batch 100 loss: 2.009457898139954\n",
      "LOSS train 2.009457898139954 valid 2.0817525386810303\n",
      "EPOCH 110:\n",
      "  batch 50 loss: 2.0197128367424013\n",
      "  batch 100 loss: 2.0040439891815187\n",
      "LOSS train 2.0040439891815187 valid 2.0781006813049316\n",
      "EPOCH 111:\n",
      "  batch 50 loss: 2.014245345592499\n",
      "  batch 100 loss: 1.9970027661323548\n",
      "LOSS train 1.9970027661323548 valid 2.0825998783111572\n",
      "EPOCH 112:\n",
      "  batch 50 loss: 2.012782862186432\n",
      "  batch 100 loss: 2.0032069945335387\n",
      "LOSS train 2.0032069945335387 valid 2.0699660778045654\n",
      "EPOCH 113:\n",
      "  batch 50 loss: 2.0078635740280153\n",
      "  batch 100 loss: 2.0125268292427063\n",
      "LOSS train 2.0125268292427063 valid 2.070366621017456\n",
      "EPOCH 114:\n",
      "  batch 50 loss: 2.0047655725479125\n",
      "  batch 100 loss: 1.997357816696167\n",
      "LOSS train 1.997357816696167 valid 2.06549334526062\n",
      "EPOCH 115:\n",
      "  batch 50 loss: 2.006078007221222\n",
      "  batch 100 loss: 2.00287736415863\n",
      "LOSS train 2.00287736415863 valid 2.0703139305114746\n",
      "EPOCH 116:\n",
      "  batch 50 loss: 1.9950609397888184\n",
      "  batch 100 loss: 2.0029039430618285\n",
      "LOSS train 2.0029039430618285 valid 2.083219289779663\n",
      "EPOCH 117:\n",
      "  batch 50 loss: 1.9984092617034912\n",
      "  batch 100 loss: 2.0061292004585267\n",
      "LOSS train 2.0061292004585267 valid 2.0607926845550537\n",
      "EPOCH 118:\n",
      "  batch 50 loss: 1.9917531561851503\n",
      "  batch 100 loss: 1.9996329236030579\n",
      "LOSS train 1.9996329236030579 valid 2.0621376037597656\n",
      "EPOCH 119:\n",
      "  batch 50 loss: 2.0099054527282716\n",
      "  batch 100 loss: 1.9850678539276123\n",
      "LOSS train 1.9850678539276123 valid 2.0640556812286377\n",
      "EPOCH 120:\n",
      "  batch 50 loss: 1.988489773273468\n",
      "  batch 100 loss: 1.9912566661834716\n",
      "LOSS train 1.9912566661834716 valid 2.0970067977905273\n",
      "EPOCH 121:\n",
      "  batch 50 loss: 1.987016360759735\n",
      "  batch 100 loss: 1.9832851219177245\n",
      "LOSS train 1.9832851219177245 valid 2.0445141792297363\n",
      "EPOCH 122:\n",
      "  batch 50 loss: 1.989466631412506\n",
      "  batch 100 loss: 1.981377410888672\n",
      "LOSS train 1.981377410888672 valid 2.053750991821289\n",
      "EPOCH 123:\n",
      "  batch 50 loss: 1.9954533743858338\n",
      "  batch 100 loss: 1.9968938899040223\n",
      "LOSS train 1.9968938899040223 valid 2.054628610610962\n",
      "EPOCH 124:\n",
      "  batch 50 loss: 1.9804015398025512\n",
      "  batch 100 loss: 1.9785463881492615\n",
      "LOSS train 1.9785463881492615 valid 2.046468734741211\n",
      "EPOCH 125:\n",
      "  batch 50 loss: 1.9821820497512816\n",
      "  batch 100 loss: 1.9842544794082642\n",
      "LOSS train 1.9842544794082642 valid 2.056384801864624\n",
      "EPOCH 126:\n",
      "  batch 50 loss: 1.9883541512489318\n",
      "  batch 100 loss: 1.9738913774490356\n",
      "LOSS train 1.9738913774490356 valid 2.05253267288208\n",
      "EPOCH 127:\n",
      "  batch 50 loss: 1.9751822876930236\n",
      "  batch 100 loss: 1.9756834530830383\n",
      "LOSS train 1.9756834530830383 valid 2.0438430309295654\n",
      "EPOCH 128:\n",
      "  batch 50 loss: 1.9805764961242676\n",
      "  batch 100 loss: 1.980442771911621\n",
      "LOSS train 1.980442771911621 valid 2.0586845874786377\n",
      "EPOCH 129:\n",
      "  batch 50 loss: 1.97846054315567\n",
      "  batch 100 loss: 1.9900783944129943\n",
      "LOSS train 1.9900783944129943 valid 2.0445332527160645\n",
      "EPOCH 130:\n",
      "  batch 50 loss: 1.98419606924057\n",
      "  batch 100 loss: 1.9724147701263428\n",
      "LOSS train 1.9724147701263428 valid 2.051903247833252\n",
      "EPOCH 131:\n",
      "  batch 50 loss: 1.9715738940238952\n",
      "  batch 100 loss: 1.9739248657226562\n",
      "LOSS train 1.9739248657226562 valid 2.038900136947632\n",
      "EPOCH 132:\n",
      "  batch 50 loss: 1.9758985877037047\n",
      "  batch 100 loss: 1.9740373706817627\n",
      "LOSS train 1.9740373706817627 valid 2.0434372425079346\n",
      "EPOCH 133:\n",
      "  batch 50 loss: 1.9751291918754577\n",
      "  batch 100 loss: 1.9561818981170653\n",
      "LOSS train 1.9561818981170653 valid 2.04060697555542\n",
      "EPOCH 134:\n",
      "  batch 50 loss: 1.960746567249298\n",
      "  batch 100 loss: 1.9746380496025084\n",
      "LOSS train 1.9746380496025084 valid 2.0355076789855957\n",
      "EPOCH 135:\n",
      "  batch 50 loss: 1.9715504932403565\n",
      "  batch 100 loss: 1.9631528329849244\n",
      "LOSS train 1.9631528329849244 valid 2.051154136657715\n",
      "EPOCH 136:\n",
      "  batch 50 loss: 1.9746863460540771\n",
      "  batch 100 loss: 1.9626596808433532\n",
      "LOSS train 1.9626596808433532 valid 2.0374786853790283\n",
      "EPOCH 137:\n",
      "  batch 50 loss: 1.96451678276062\n",
      "  batch 100 loss: 1.9568773126602172\n",
      "LOSS train 1.9568773126602172 valid 2.0392799377441406\n",
      "EPOCH 138:\n",
      "  batch 50 loss: 1.9583110308647156\n",
      "  batch 100 loss: 1.959662368297577\n",
      "LOSS train 1.959662368297577 valid 2.043941020965576\n",
      "EPOCH 139:\n",
      "  batch 50 loss: 1.9554503655433655\n",
      "  batch 100 loss: 1.9807033395767213\n",
      "LOSS train 1.9807033395767213 valid 2.0345380306243896\n",
      "EPOCH 140:\n",
      "  batch 50 loss: 1.9629151225090027\n",
      "  batch 100 loss: 1.943375895023346\n",
      "LOSS train 1.943375895023346 valid 2.0257914066314697\n",
      "EPOCH 141:\n",
      "  batch 50 loss: 1.9792085480690003\n",
      "  batch 100 loss: 1.9532740306854248\n",
      "LOSS train 1.9532740306854248 valid 2.0263936519622803\n",
      "EPOCH 142:\n",
      "  batch 50 loss: 1.9613964867591858\n",
      "  batch 100 loss: 1.954400291442871\n",
      "LOSS train 1.954400291442871 valid 2.0725531578063965\n",
      "EPOCH 143:\n",
      "  batch 50 loss: 1.9530127120018006\n",
      "  batch 100 loss: 1.9646030974388122\n",
      "LOSS train 1.9646030974388122 valid 2.0284271240234375\n",
      "EPOCH 144:\n",
      "  batch 50 loss: 1.957620701789856\n",
      "  batch 100 loss: 1.950412449836731\n",
      "LOSS train 1.950412449836731 valid 2.0373005867004395\n",
      "EPOCH 145:\n",
      "  batch 50 loss: 1.9533254837989806\n",
      "  batch 100 loss: 1.9560240244865417\n",
      "LOSS train 1.9560240244865417 valid 2.022230625152588\n",
      "EPOCH 146:\n",
      "  batch 50 loss: 1.9545635056495667\n",
      "  batch 100 loss: 1.9468063354492187\n",
      "LOSS train 1.9468063354492187 valid 2.0305466651916504\n",
      "EPOCH 147:\n",
      "  batch 50 loss: 1.9613449478149414\n",
      "  batch 100 loss: 1.949293885231018\n",
      "LOSS train 1.949293885231018 valid 2.047686815261841\n",
      "EPOCH 148:\n",
      "  batch 50 loss: 1.9514079689979553\n",
      "  batch 100 loss: 1.9386248803138733\n",
      "LOSS train 1.9386248803138733 valid 2.022005081176758\n",
      "EPOCH 149:\n",
      "  batch 50 loss: 1.9466563963890076\n",
      "  batch 100 loss: 1.9458071160316468\n",
      "LOSS train 1.9458071160316468 valid 2.0290565490722656\n",
      "EPOCH 150:\n",
      "  batch 50 loss: 1.9445826959609986\n",
      "  batch 100 loss: 1.928717885017395\n",
      "LOSS train 1.928717885017395 valid 2.0226597785949707\n",
      "EPOCH 151:\n",
      "  batch 50 loss: 1.9519253158569336\n",
      "  batch 100 loss: 1.9351886129379272\n",
      "LOSS train 1.9351886129379272 valid 2.017418146133423\n",
      "EPOCH 152:\n",
      "  batch 50 loss: 1.9456821751594544\n",
      "  batch 100 loss: 1.9377904033660889\n",
      "LOSS train 1.9377904033660889 valid 2.0208892822265625\n",
      "EPOCH 153:\n",
      "  batch 50 loss: 1.9409693241119386\n",
      "  batch 100 loss: 1.9347866320610045\n",
      "LOSS train 1.9347866320610045 valid 2.0029489994049072\n",
      "EPOCH 154:\n",
      "  batch 50 loss: 1.9425845623016358\n",
      "  batch 100 loss: 1.9301594042778014\n",
      "LOSS train 1.9301594042778014 valid 2.0359513759613037\n",
      "EPOCH 155:\n",
      "  batch 50 loss: 1.956936583518982\n",
      "  batch 100 loss: 1.9316690850257874\n",
      "LOSS train 1.9316690850257874 valid 2.01202392578125\n",
      "EPOCH 156:\n",
      "  batch 50 loss: 1.9445779967308043\n",
      "  batch 100 loss: 1.92575364112854\n",
      "LOSS train 1.92575364112854 valid 2.006887435913086\n",
      "EPOCH 157:\n",
      "  batch 50 loss: 1.9291035318374634\n",
      "  batch 100 loss: 1.9508437204360962\n",
      "LOSS train 1.9508437204360962 valid 2.0125644207000732\n",
      "EPOCH 158:\n",
      "  batch 50 loss: 1.938585548400879\n",
      "  batch 100 loss: 1.9358768892288207\n",
      "LOSS train 1.9358768892288207 valid 2.0087153911590576\n",
      "EPOCH 159:\n",
      "  batch 50 loss: 1.9385505533218383\n",
      "  batch 100 loss: 1.9176118493080139\n",
      "LOSS train 1.9176118493080139 valid 2.0268640518188477\n",
      "EPOCH 160:\n",
      "  batch 50 loss: 1.9365111327171325\n",
      "  batch 100 loss: 1.9340377855300903\n",
      "LOSS train 1.9340377855300903 valid 2.0078682899475098\n",
      "EPOCH 161:\n",
      "  batch 50 loss: 1.934520547389984\n",
      "  batch 100 loss: 1.9401155304908753\n",
      "LOSS train 1.9401155304908753 valid 2.0097625255584717\n",
      "EPOCH 162:\n",
      "  batch 50 loss: 1.9269573879241944\n",
      "  batch 100 loss: 1.921051640510559\n",
      "LOSS train 1.921051640510559 valid 2.0075607299804688\n",
      "EPOCH 163:\n",
      "  batch 50 loss: 1.9336148262023927\n",
      "  batch 100 loss: 1.922064368724823\n",
      "LOSS train 1.922064368724823 valid 2.0065386295318604\n",
      "EPOCH 164:\n",
      "  batch 50 loss: 1.9194061613082887\n",
      "  batch 100 loss: 1.9257143139839172\n",
      "LOSS train 1.9257143139839172 valid 2.0085930824279785\n",
      "EPOCH 165:\n",
      "  batch 50 loss: 1.932559757232666\n",
      "  batch 100 loss: 1.9250768756866454\n",
      "LOSS train 1.9250768756866454 valid 1.9960969686508179\n",
      "EPOCH 166:\n",
      "  batch 50 loss: 1.933399567604065\n",
      "  batch 100 loss: 1.9219963765144348\n",
      "LOSS train 1.9219963765144348 valid 1.9944703578948975\n",
      "EPOCH 167:\n",
      "  batch 50 loss: 1.931161470413208\n",
      "  batch 100 loss: 1.932836856842041\n",
      "LOSS train 1.932836856842041 valid 2.019130229949951\n",
      "EPOCH 168:\n",
      "  batch 50 loss: 1.9262234926223756\n",
      "  batch 100 loss: 1.923406572341919\n",
      "LOSS train 1.923406572341919 valid 2.002016544342041\n",
      "EPOCH 169:\n",
      "  batch 50 loss: 1.9278363180160523\n",
      "  batch 100 loss: 1.9277720856666565\n",
      "LOSS train 1.9277720856666565 valid 1.9987620115280151\n",
      "EPOCH 170:\n",
      "  batch 50 loss: 1.9189793395996093\n",
      "  batch 100 loss: 1.911410195827484\n",
      "LOSS train 1.911410195827484 valid 2.0017166137695312\n",
      "EPOCH 171:\n",
      "  batch 50 loss: 1.922078354358673\n",
      "  batch 100 loss: 1.9195692682266234\n",
      "LOSS train 1.9195692682266234 valid 2.037738800048828\n",
      "EPOCH 172:\n",
      "  batch 50 loss: 1.914785282611847\n",
      "  batch 100 loss: 1.9255281710624694\n",
      "LOSS train 1.9255281710624694 valid 1.9991681575775146\n",
      "EPOCH 173:\n",
      "  batch 50 loss: 1.9047200393676758\n",
      "  batch 100 loss: 1.9235908603668213\n",
      "LOSS train 1.9235908603668213 valid 1.9916075468063354\n",
      "EPOCH 174:\n",
      "  batch 50 loss: 1.9255988121032714\n",
      "  batch 100 loss: 1.8997520589828492\n",
      "LOSS train 1.8997520589828492 valid 1.9923793077468872\n",
      "EPOCH 175:\n",
      "  batch 50 loss: 1.9149941420555114\n",
      "  batch 100 loss: 1.9072284269332886\n",
      "LOSS train 1.9072284269332886 valid 1.9978618621826172\n",
      "EPOCH 176:\n",
      "  batch 50 loss: 1.908304750919342\n",
      "  batch 100 loss: 1.921883327960968\n",
      "LOSS train 1.921883327960968 valid 1.9918720722198486\n",
      "EPOCH 177:\n",
      "  batch 50 loss: 1.9231763458251954\n",
      "  batch 100 loss: 1.9145871949195863\n",
      "LOSS train 1.9145871949195863 valid 1.9910776615142822\n",
      "EPOCH 178:\n",
      "  batch 50 loss: 1.903565628528595\n",
      "  batch 100 loss: 1.9223736572265624\n",
      "LOSS train 1.9223736572265624 valid 1.986606478691101\n",
      "EPOCH 179:\n",
      "  batch 50 loss: 1.913650164604187\n",
      "  batch 100 loss: 1.9017698907852172\n",
      "LOSS train 1.9017698907852172 valid 1.99070143699646\n",
      "EPOCH 180:\n",
      "  batch 50 loss: 1.9152593183517457\n",
      "  batch 100 loss: 1.9143829941749573\n",
      "LOSS train 1.9143829941749573 valid 1.9813923835754395\n",
      "EPOCH 181:\n",
      "  batch 50 loss: 1.89877117395401\n",
      "  batch 100 loss: 1.9250101399421693\n",
      "LOSS train 1.9250101399421693 valid 1.9800554513931274\n",
      "EPOCH 182:\n",
      "  batch 50 loss: 1.9105106854438783\n",
      "  batch 100 loss: 1.908233823776245\n",
      "LOSS train 1.908233823776245 valid 1.9818776845932007\n",
      "EPOCH 183:\n",
      "  batch 50 loss: 1.8961471676826478\n",
      "  batch 100 loss: 1.9130967879295349\n",
      "LOSS train 1.9130967879295349 valid 1.9987215995788574\n",
      "EPOCH 184:\n",
      "  batch 50 loss: 1.9209314799308777\n",
      "  batch 100 loss: 1.8873793387413025\n",
      "LOSS train 1.8873793387413025 valid 1.978031039237976\n",
      "EPOCH 185:\n",
      "  batch 50 loss: 1.8885094833374023\n",
      "  batch 100 loss: 1.9092851543426514\n",
      "LOSS train 1.9092851543426514 valid 1.976365566253662\n",
      "EPOCH 186:\n",
      "  batch 50 loss: 1.896811411380768\n",
      "  batch 100 loss: 1.891302354335785\n",
      "LOSS train 1.891302354335785 valid 1.9798564910888672\n",
      "EPOCH 187:\n",
      "  batch 50 loss: 1.9103494095802307\n",
      "  batch 100 loss: 1.8957862854003906\n",
      "LOSS train 1.8957862854003906 valid 1.9827169179916382\n",
      "EPOCH 188:\n",
      "  batch 50 loss: 1.9005117893218995\n",
      "  batch 100 loss: 1.9060186743736267\n",
      "LOSS train 1.9060186743736267 valid 1.9957793951034546\n",
      "EPOCH 189:\n",
      "  batch 50 loss: 1.900212938785553\n",
      "  batch 100 loss: 1.9094708013534545\n",
      "LOSS train 1.9094708013534545 valid 1.9807952642440796\n",
      "EPOCH 190:\n",
      "  batch 50 loss: 1.8803790974617005\n",
      "  batch 100 loss: 1.9062618470191957\n",
      "LOSS train 1.9062618470191957 valid 1.9748752117156982\n",
      "EPOCH 191:\n",
      "  batch 50 loss: 1.8973538613319396\n",
      "  batch 100 loss: 1.8991565251350402\n",
      "LOSS train 1.8991565251350402 valid 1.985660195350647\n",
      "EPOCH 192:\n",
      "  batch 50 loss: 1.8884606957435608\n",
      "  batch 100 loss: 1.8917366576194763\n",
      "LOSS train 1.8917366576194763 valid 2.006502389907837\n",
      "EPOCH 193:\n",
      "  batch 50 loss: 1.8971281147003174\n",
      "  batch 100 loss: 1.8822336292266846\n",
      "LOSS train 1.8822336292266846 valid 1.9685436487197876\n",
      "EPOCH 194:\n",
      "  batch 50 loss: 1.8763625454902648\n",
      "  batch 100 loss: 1.8838993883132935\n",
      "LOSS train 1.8838993883132935 valid 1.9758381843566895\n",
      "EPOCH 195:\n",
      "  batch 50 loss: 1.8776079082489014\n",
      "  batch 100 loss: 1.9071667313575744\n",
      "LOSS train 1.9071667313575744 valid 1.970591425895691\n",
      "EPOCH 196:\n",
      "  batch 50 loss: 1.8878645253181459\n",
      "  batch 100 loss: 1.898248143196106\n",
      "LOSS train 1.898248143196106 valid 1.9686927795410156\n",
      "EPOCH 197:\n",
      "  batch 50 loss: 1.8999613738059997\n",
      "  batch 100 loss: 1.896586377620697\n",
      "LOSS train 1.896586377620697 valid 1.9675549268722534\n",
      "EPOCH 198:\n",
      "  batch 50 loss: 1.9030341339111327\n",
      "  batch 100 loss: 1.8762971591949462\n",
      "LOSS train 1.8762971591949462 valid 1.9707361459732056\n",
      "EPOCH 199:\n",
      "  batch 50 loss: 1.897329170703888\n",
      "  batch 100 loss: 1.875058114528656\n",
      "LOSS train 1.875058114528656 valid 1.9635026454925537\n",
      "EPOCH 200:\n",
      "  batch 50 loss: 1.90399893283844\n",
      "  batch 100 loss: 1.8747086381912232\n",
      "LOSS train 1.8747086381912232 valid 1.9689099788665771\n",
      "EPOCH 201:\n",
      "  batch 50 loss: 1.8952537703514098\n",
      "  batch 100 loss: 1.8994697070121764\n",
      "LOSS train 1.8994697070121764 valid 2.014240264892578\n",
      "EPOCH 202:\n",
      "  batch 50 loss: 1.8869892406463622\n",
      "  batch 100 loss: 1.8917301058769227\n",
      "LOSS train 1.8917301058769227 valid 1.9653284549713135\n",
      "EPOCH 203:\n",
      "  batch 50 loss: 1.8770538330078126\n",
      "  batch 100 loss: 1.8852583408355712\n",
      "LOSS train 1.8852583408355712 valid 1.967930793762207\n",
      "EPOCH 204:\n",
      "  batch 50 loss: 1.8796378326416017\n",
      "  batch 100 loss: 1.8879203820228576\n",
      "LOSS train 1.8879203820228576 valid 1.9691708087921143\n",
      "EPOCH 205:\n",
      "  batch 50 loss: 1.9039638876914977\n",
      "  batch 100 loss: 1.8682711815834045\n",
      "LOSS train 1.8682711815834045 valid 1.9732640981674194\n",
      "EPOCH 206:\n",
      "  batch 50 loss: 1.880000033378601\n",
      "  batch 100 loss: 1.8837464761734009\n",
      "LOSS train 1.8837464761734009 valid 1.9716318845748901\n",
      "EPOCH 207:\n",
      "  batch 50 loss: 1.8711319136619569\n",
      "  batch 100 loss: 1.8736370348930358\n",
      "LOSS train 1.8736370348930358 valid 1.9649357795715332\n",
      "EPOCH 208:\n",
      "  batch 50 loss: 1.8766832971572875\n",
      "  batch 100 loss: 1.9047805571556091\n",
      "LOSS train 1.9047805571556091 valid 1.964828372001648\n",
      "EPOCH 209:\n",
      "  batch 50 loss: 1.864588098526001\n",
      "  batch 100 loss: 1.8733344554901123\n",
      "LOSS train 1.8733344554901123 valid 1.9771397113800049\n",
      "EPOCH 210:\n",
      "  batch 50 loss: 1.868813979625702\n",
      "  batch 100 loss: 1.8756700515747071\n",
      "LOSS train 1.8756700515747071 valid 1.9605952501296997\n",
      "EPOCH 211:\n",
      "  batch 50 loss: 1.8696683216094971\n",
      "  batch 100 loss: 1.8795346856117248\n",
      "LOSS train 1.8795346856117248 valid 1.9643193483352661\n",
      "EPOCH 212:\n",
      "  batch 50 loss: 1.879130048751831\n",
      "  batch 100 loss: 1.884685754776001\n",
      "LOSS train 1.884685754776001 valid 1.9537006616592407\n",
      "EPOCH 213:\n",
      "  batch 50 loss: 1.8863796710968017\n",
      "  batch 100 loss: 1.8743945026397706\n",
      "LOSS train 1.8743945026397706 valid 1.9685688018798828\n",
      "EPOCH 214:\n",
      "  batch 50 loss: 1.8646155643463134\n",
      "  batch 100 loss: 1.8875760674476623\n",
      "LOSS train 1.8875760674476623 valid 1.9631708860397339\n",
      "EPOCH 215:\n",
      "  batch 50 loss: 1.8795914220809937\n",
      "  batch 100 loss: 1.863490343093872\n",
      "LOSS train 1.863490343093872 valid 1.964148759841919\n",
      "EPOCH 216:\n",
      "  batch 50 loss: 1.8563405466079712\n",
      "  batch 100 loss: 1.894372615814209\n",
      "LOSS train 1.894372615814209 valid 1.963929295539856\n",
      "EPOCH 217:\n",
      "  batch 50 loss: 1.8895031237602233\n",
      "  batch 100 loss: 1.8569024729728698\n",
      "LOSS train 1.8569024729728698 valid 1.9672629833221436\n",
      "EPOCH 218:\n",
      "  batch 50 loss: 1.8772424101829528\n",
      "  batch 100 loss: 1.8712851977348328\n",
      "LOSS train 1.8712851977348328 valid 1.9645761251449585\n",
      "EPOCH 219:\n",
      "  batch 50 loss: 1.864405415058136\n",
      "  batch 100 loss: 1.8668578052520752\n",
      "LOSS train 1.8668578052520752 valid 1.9525173902511597\n",
      "EPOCH 220:\n",
      "  batch 50 loss: 1.8600058341026307\n",
      "  batch 100 loss: 1.8805706882476807\n",
      "LOSS train 1.8805706882476807 valid 1.9537986516952515\n",
      "EPOCH 221:\n",
      "  batch 50 loss: 1.862810869216919\n",
      "  batch 100 loss: 1.8662847948074341\n",
      "LOSS train 1.8662847948074341 valid 1.9609466791152954\n",
      "EPOCH 222:\n",
      "  batch 50 loss: 1.8755005311965942\n",
      "  batch 100 loss: 1.8669574093818664\n",
      "LOSS train 1.8669574093818664 valid 1.9565863609313965\n",
      "EPOCH 223:\n",
      "  batch 50 loss: 1.8579374384880065\n",
      "  batch 100 loss: 1.8705585145950316\n",
      "LOSS train 1.8705585145950316 valid 1.9488248825073242\n",
      "EPOCH 224:\n",
      "  batch 50 loss: 1.8571129250526428\n",
      "  batch 100 loss: 1.8776165199279786\n",
      "LOSS train 1.8776165199279786 valid 1.9517416954040527\n",
      "EPOCH 225:\n",
      "  batch 50 loss: 1.882267611026764\n",
      "  batch 100 loss: 1.8496978378295899\n",
      "LOSS train 1.8496978378295899 valid 1.9529253244400024\n",
      "EPOCH 226:\n",
      "  batch 50 loss: 1.8769897723197937\n",
      "  batch 100 loss: 1.8546292805671691\n",
      "LOSS train 1.8546292805671691 valid 1.9630136489868164\n",
      "EPOCH 227:\n",
      "  batch 50 loss: 1.8701622295379638\n",
      "  batch 100 loss: 1.8693717670440675\n",
      "LOSS train 1.8693717670440675 valid 1.9589042663574219\n",
      "EPOCH 228:\n",
      "  batch 50 loss: 1.8572885918617248\n",
      "  batch 100 loss: 1.8645109057426452\n",
      "LOSS train 1.8645109057426452 valid 1.9621988534927368\n",
      "EPOCH 229:\n",
      "  batch 50 loss: 1.8653072381019593\n",
      "  batch 100 loss: 1.8575705194473267\n",
      "LOSS train 1.8575705194473267 valid 1.9584602117538452\n",
      "EPOCH 230:\n",
      "  batch 50 loss: 1.855605444908142\n",
      "  batch 100 loss: 1.8643305778503418\n",
      "LOSS train 1.8643305778503418 valid 1.955113172531128\n",
      "EPOCH 231:\n",
      "  batch 50 loss: 1.8526765966415406\n",
      "  batch 100 loss: 1.8675554299354553\n",
      "LOSS train 1.8675554299354553 valid 1.9470319747924805\n",
      "EPOCH 232:\n",
      "  batch 50 loss: 1.876835765838623\n",
      "  batch 100 loss: 1.8599858212471008\n",
      "LOSS train 1.8599858212471008 valid 1.9693700075149536\n",
      "EPOCH 233:\n",
      "  batch 50 loss: 1.8682230353355407\n",
      "  batch 100 loss: 1.862546317577362\n",
      "LOSS train 1.862546317577362 valid 1.9517549276351929\n",
      "EPOCH 234:\n",
      "  batch 50 loss: 1.8560735988616943\n",
      "  batch 100 loss: 1.8671427321434022\n",
      "LOSS train 1.8671427321434022 valid 1.951346755027771\n",
      "EPOCH 235:\n",
      "  batch 50 loss: 1.8687725114822387\n",
      "  batch 100 loss: 1.8626906657218933\n",
      "LOSS train 1.8626906657218933 valid 1.955986738204956\n",
      "EPOCH 236:\n",
      "  batch 50 loss: 1.8566886401176452\n",
      "  batch 100 loss: 1.8622659492492675\n",
      "LOSS train 1.8622659492492675 valid 1.969154953956604\n",
      "EPOCH 237:\n",
      "  batch 50 loss: 1.8600015068054199\n",
      "  batch 100 loss: 1.856845986843109\n",
      "LOSS train 1.856845986843109 valid 1.9629747867584229\n",
      "EPOCH 238:\n",
      "  batch 50 loss: 1.8465681457519532\n",
      "  batch 100 loss: 1.8532689809799194\n",
      "LOSS train 1.8532689809799194 valid 1.953890323638916\n",
      "EPOCH 239:\n",
      "  batch 50 loss: 1.8490576434135437\n",
      "  batch 100 loss: 1.8758989000320434\n",
      "LOSS train 1.8758989000320434 valid 1.9491835832595825\n",
      "EPOCH 240:\n",
      "  batch 50 loss: 1.8607571268081664\n",
      "  batch 100 loss: 1.8509795546531678\n",
      "LOSS train 1.8509795546531678 valid 1.9530514478683472\n",
      "EPOCH 241:\n",
      "  batch 50 loss: 1.8622203755378723\n",
      "  batch 100 loss: 1.8592028713226318\n",
      "LOSS train 1.8592028713226318 valid 1.9503898620605469\n",
      "EPOCH 242:\n",
      "  batch 50 loss: 1.8630909824371338\n",
      "  batch 100 loss: 1.84932865858078\n",
      "LOSS train 1.84932865858078 valid 1.9471118450164795\n",
      "EPOCH 243:\n",
      "  batch 50 loss: 1.8735091638565065\n",
      "  batch 100 loss: 1.846043391227722\n",
      "LOSS train 1.846043391227722 valid 1.9418803453445435\n",
      "EPOCH 244:\n",
      "  batch 50 loss: 1.8499945831298827\n",
      "  batch 100 loss: 1.8422816443443297\n",
      "LOSS train 1.8422816443443297 valid 1.9387880563735962\n",
      "EPOCH 245:\n",
      "  batch 50 loss: 1.8557471013069153\n",
      "  batch 100 loss: 1.8635312056541442\n",
      "LOSS train 1.8635312056541442 valid 1.9365661144256592\n",
      "EPOCH 246:\n",
      "  batch 50 loss: 1.8413586592674256\n",
      "  batch 100 loss: 1.8583581471443176\n",
      "LOSS train 1.8583581471443176 valid 1.9471012353897095\n",
      "EPOCH 247:\n",
      "  batch 50 loss: 1.854053680896759\n",
      "  batch 100 loss: 1.8575321578979491\n",
      "LOSS train 1.8575321578979491 valid 1.947257399559021\n",
      "EPOCH 248:\n",
      "  batch 50 loss: 1.8585924124717712\n",
      "  batch 100 loss: 1.850012767314911\n",
      "LOSS train 1.850012767314911 valid 1.981268286705017\n",
      "EPOCH 249:\n",
      "  batch 50 loss: 1.8729822301864625\n",
      "  batch 100 loss: 1.8397070717811586\n",
      "LOSS train 1.8397070717811586 valid 1.943718671798706\n",
      "EPOCH 250:\n",
      "  batch 50 loss: 1.8457879853248595\n",
      "  batch 100 loss: 1.8448785066604614\n",
      "LOSS train 1.8448785066604614 valid 1.9407777786254883\n",
      "EPOCH 251:\n",
      "  batch 50 loss: 1.8571243405342102\n",
      "  batch 100 loss: 1.841282684803009\n",
      "LOSS train 1.841282684803009 valid 1.962609052658081\n",
      "EPOCH 252:\n",
      "  batch 50 loss: 1.8405124616622925\n",
      "  batch 100 loss: 1.8470428061485291\n",
      "LOSS train 1.8470428061485291 valid 1.935235619544983\n",
      "EPOCH 253:\n",
      "  batch 50 loss: 1.8654186463356017\n",
      "  batch 100 loss: 1.8345611691474915\n",
      "LOSS train 1.8345611691474915 valid 1.9326703548431396\n",
      "EPOCH 254:\n",
      "  batch 50 loss: 1.8544964265823365\n",
      "  batch 100 loss: 1.8425860500335693\n",
      "LOSS train 1.8425860500335693 valid 1.9370486736297607\n",
      "EPOCH 255:\n",
      "  batch 50 loss: 1.8298819875717163\n",
      "  batch 100 loss: 1.8437973618507386\n",
      "LOSS train 1.8437973618507386 valid 1.9375680685043335\n",
      "EPOCH 256:\n",
      "  batch 50 loss: 1.83279931306839\n",
      "  batch 100 loss: 1.8479503464698792\n",
      "LOSS train 1.8479503464698792 valid 1.9365788698196411\n",
      "EPOCH 257:\n",
      "  batch 50 loss: 1.8286975979804994\n",
      "  batch 100 loss: 1.833937885761261\n",
      "LOSS train 1.833937885761261 valid 1.9385709762573242\n",
      "EPOCH 258:\n",
      "  batch 50 loss: 1.865369040966034\n",
      "  batch 100 loss: 1.8446147179603576\n",
      "LOSS train 1.8446147179603576 valid 1.9331369400024414\n",
      "EPOCH 259:\n",
      "  batch 50 loss: 1.83122243642807\n",
      "  batch 100 loss: 1.8519126772880554\n",
      "LOSS train 1.8519126772880554 valid 1.9360452890396118\n",
      "EPOCH 260:\n",
      "  batch 50 loss: 1.8309354400634765\n",
      "  batch 100 loss: 1.8411198854446411\n",
      "LOSS train 1.8411198854446411 valid 1.940984845161438\n",
      "EPOCH 261:\n",
      "  batch 50 loss: 1.8645903778076172\n",
      "  batch 100 loss: 1.8321365165710448\n",
      "LOSS train 1.8321365165710448 valid 1.9434821605682373\n",
      "EPOCH 262:\n",
      "  batch 50 loss: 1.8399829936027527\n",
      "  batch 100 loss: 1.84006831407547\n",
      "LOSS train 1.84006831407547 valid 1.9419127702713013\n",
      "EPOCH 263:\n",
      "  batch 50 loss: 1.8450300264358521\n",
      "  batch 100 loss: 1.820446698665619\n",
      "LOSS train 1.820446698665619 valid 1.9310084581375122\n",
      "EPOCH 264:\n",
      "  batch 50 loss: 1.8376238346099854\n",
      "  batch 100 loss: 1.8267664742469787\n",
      "LOSS train 1.8267664742469787 valid 1.937487006187439\n",
      "EPOCH 265:\n",
      "  batch 50 loss: 1.8386428189277648\n",
      "  batch 100 loss: 1.84942777633667\n",
      "LOSS train 1.84942777633667 valid 1.9383758306503296\n",
      "EPOCH 266:\n",
      "  batch 50 loss: 1.8432681560516357\n",
      "  batch 100 loss: 1.8234713554382325\n",
      "LOSS train 1.8234713554382325 valid 1.9276902675628662\n",
      "EPOCH 267:\n",
      "  batch 50 loss: 1.8570842242240906\n",
      "  batch 100 loss: 1.8179073095321656\n",
      "LOSS train 1.8179073095321656 valid 1.957311749458313\n",
      "EPOCH 268:\n",
      "  batch 50 loss: 1.8372696399688722\n",
      "  batch 100 loss: 1.838579409122467\n",
      "LOSS train 1.838579409122467 valid 1.9353243112564087\n",
      "EPOCH 269:\n",
      "  batch 50 loss: 1.8348693394660949\n",
      "  batch 100 loss: 1.8441277527809143\n",
      "LOSS train 1.8441277527809143 valid 1.9306169748306274\n",
      "EPOCH 270:\n",
      "  batch 50 loss: 1.8426966571807861\n",
      "  batch 100 loss: 1.8171025276184083\n",
      "LOSS train 1.8171025276184083 valid 1.9383641481399536\n",
      "EPOCH 271:\n",
      "  batch 50 loss: 1.8324725866317748\n",
      "  batch 100 loss: 1.8323068356513976\n",
      "LOSS train 1.8323068356513976 valid 1.9308768510818481\n",
      "EPOCH 272:\n",
      "  batch 50 loss: 1.8253534317016602\n",
      "  batch 100 loss: 1.8316846990585327\n",
      "LOSS train 1.8316846990585327 valid 1.925413966178894\n",
      "EPOCH 273:\n",
      "  batch 50 loss: 1.828534359931946\n",
      "  batch 100 loss: 1.8484188413619995\n",
      "LOSS train 1.8484188413619995 valid 1.9395631551742554\n",
      "EPOCH 274:\n",
      "  batch 50 loss: 1.8283618021011352\n",
      "  batch 100 loss: 1.8334088873863221\n",
      "LOSS train 1.8334088873863221 valid 1.9328970909118652\n",
      "EPOCH 275:\n",
      "  batch 50 loss: 1.8287793803215027\n",
      "  batch 100 loss: 1.8439765787124633\n",
      "LOSS train 1.8439765787124633 valid 1.929625391960144\n",
      "EPOCH 276:\n",
      "  batch 50 loss: 1.8213752222061157\n",
      "  batch 100 loss: 1.8433062148094177\n",
      "LOSS train 1.8433062148094177 valid 1.9311777353286743\n",
      "EPOCH 277:\n",
      "  batch 50 loss: 1.8312194156646728\n",
      "  batch 100 loss: 1.8221765780448913\n",
      "LOSS train 1.8221765780448913 valid 1.9360406398773193\n",
      "EPOCH 278:\n",
      "  batch 50 loss: 1.8504459547996521\n",
      "  batch 100 loss: 1.8279040026664735\n",
      "LOSS train 1.8279040026664735 valid 1.9261988401412964\n",
      "EPOCH 279:\n",
      "  batch 50 loss: 1.832294316291809\n",
      "  batch 100 loss: 1.8493424296379088\n",
      "LOSS train 1.8493424296379088 valid 1.9340540170669556\n",
      "EPOCH 280:\n",
      "  batch 50 loss: 1.8387904930114747\n",
      "  batch 100 loss: 1.824219264984131\n",
      "LOSS train 1.824219264984131 valid 1.934149146080017\n",
      "EPOCH 281:\n",
      "  batch 50 loss: 1.8460437488555907\n",
      "  batch 100 loss: 1.8255886101722718\n",
      "LOSS train 1.8255886101722718 valid 1.9323564767837524\n",
      "EPOCH 282:\n",
      "  batch 50 loss: 1.8522832322120666\n",
      "  batch 100 loss: 1.8287112307548523\n",
      "LOSS train 1.8287112307548523 valid 1.9330472946166992\n",
      "EPOCH 283:\n",
      "  batch 50 loss: 1.8530012273788452\n",
      "  batch 100 loss: 1.8245359563827515\n",
      "LOSS train 1.8245359563827515 valid 1.96298086643219\n",
      "EPOCH 284:\n",
      "  batch 50 loss: 1.8429147028923034\n",
      "  batch 100 loss: 1.829940845966339\n",
      "LOSS train 1.829940845966339 valid 1.9396642446517944\n",
      "EPOCH 285:\n",
      "  batch 50 loss: 1.8414252924919128\n",
      "  batch 100 loss: 1.816965527534485\n",
      "LOSS train 1.816965527534485 valid 1.9309591054916382\n",
      "EPOCH 286:\n",
      "  batch 50 loss: 1.800999982357025\n",
      "  batch 100 loss: 1.831432192325592\n",
      "LOSS train 1.831432192325592 valid 1.933936357498169\n",
      "EPOCH 287:\n",
      "  batch 50 loss: 1.8175057578086853\n",
      "  batch 100 loss: 1.8293935108184813\n",
      "LOSS train 1.8293935108184813 valid 1.928844928741455\n",
      "EPOCH 288:\n",
      "  batch 50 loss: 1.8184998750686645\n",
      "  batch 100 loss: 1.8335172200202943\n",
      "LOSS train 1.8335172200202943 valid 1.924189567565918\n",
      "EPOCH 289:\n",
      "  batch 50 loss: 1.8409724187850953\n",
      "  batch 100 loss: 1.8215813422203064\n",
      "LOSS train 1.8215813422203064 valid 1.9259079694747925\n",
      "EPOCH 290:\n",
      "  batch 50 loss: 1.8109147024154664\n",
      "  batch 100 loss: 1.83038081407547\n",
      "LOSS train 1.83038081407547 valid 1.9243731498718262\n",
      "EPOCH 291:\n",
      "  batch 50 loss: 1.8436869549751282\n",
      "  batch 100 loss: 1.8206277561187745\n",
      "LOSS train 1.8206277561187745 valid 1.9300169944763184\n",
      "EPOCH 292:\n",
      "  batch 50 loss: 1.8371371006965638\n",
      "  batch 100 loss: 1.8220364880561828\n",
      "LOSS train 1.8220364880561828 valid 1.9209306240081787\n",
      "EPOCH 293:\n",
      "  batch 50 loss: 1.8281778740882872\n",
      "  batch 100 loss: 1.8182703161239624\n",
      "LOSS train 1.8182703161239624 valid 1.916580319404602\n",
      "EPOCH 294:\n",
      "  batch 50 loss: 1.8325104212760925\n",
      "  batch 100 loss: 1.8195139598846435\n",
      "LOSS train 1.8195139598846435 valid 1.9157012701034546\n",
      "EPOCH 295:\n",
      "  batch 50 loss: 1.8259353351593017\n",
      "  batch 100 loss: 1.817757637500763\n",
      "LOSS train 1.817757637500763 valid 1.9342244863510132\n",
      "EPOCH 296:\n",
      "  batch 50 loss: 1.8141105580329895\n",
      "  batch 100 loss: 1.8127324819564818\n",
      "LOSS train 1.8127324819564818 valid 1.9209085702896118\n",
      "EPOCH 297:\n",
      "  batch 50 loss: 1.8113744163513184\n",
      "  batch 100 loss: 1.8322716689109801\n",
      "LOSS train 1.8322716689109801 valid 1.9216865301132202\n",
      "EPOCH 298:\n",
      "  batch 50 loss: 1.8166949319839478\n",
      "  batch 100 loss: 1.8295090866088868\n",
      "LOSS train 1.8295090866088868 valid 1.9249510765075684\n",
      "EPOCH 299:\n",
      "  batch 50 loss: 1.8067539978027343\n",
      "  batch 100 loss: 1.8234435534477234\n",
      "LOSS train 1.8234435534477234 valid 1.9207954406738281\n",
      "EPOCH 300:\n",
      "  batch 50 loss: 1.834182391166687\n",
      "  batch 100 loss: 1.8046125507354736\n",
      "LOSS train 1.8046125507354736 valid 1.942075490951538\n",
      "EPOCH 301:\n",
      "  batch 50 loss: 1.8362126326560975\n",
      "  batch 100 loss: 1.8136427998542786\n",
      "LOSS train 1.8136427998542786 valid 1.9221705198287964\n",
      "EPOCH 302:\n",
      "  batch 50 loss: 1.826818482875824\n",
      "  batch 100 loss: 1.7985707926750183\n",
      "LOSS train 1.7985707926750183 valid 1.9260954856872559\n",
      "EPOCH 303:\n",
      "  batch 50 loss: 1.8263375067710876\n",
      "  batch 100 loss: 1.8177591133117676\n",
      "LOSS train 1.8177591133117676 valid 1.9200446605682373\n",
      "EPOCH 304:\n",
      "  batch 50 loss: 1.8083749318122864\n",
      "  batch 100 loss: 1.8100673747062683\n",
      "LOSS train 1.8100673747062683 valid 1.9437912702560425\n",
      "EPOCH 305:\n",
      "  batch 50 loss: 1.8094361066818236\n",
      "  batch 100 loss: 1.8238119912147521\n",
      "LOSS train 1.8238119912147521 valid 1.9265049695968628\n",
      "EPOCH 306:\n",
      "  batch 50 loss: 1.81537184715271\n",
      "  batch 100 loss: 1.8265851354598999\n",
      "LOSS train 1.8265851354598999 valid 1.9151239395141602\n",
      "EPOCH 307:\n",
      "  batch 50 loss: 1.7994563913345336\n",
      "  batch 100 loss: 1.832146008014679\n",
      "LOSS train 1.832146008014679 valid 1.9340910911560059\n",
      "EPOCH 308:\n",
      "  batch 50 loss: 1.8181154656410217\n",
      "  batch 100 loss: 1.8043165850639342\n",
      "LOSS train 1.8043165850639342 valid 1.9225798845291138\n",
      "EPOCH 309:\n",
      "  batch 50 loss: 1.8194179391860963\n",
      "  batch 100 loss: 1.8135008764266969\n",
      "LOSS train 1.8135008764266969 valid 1.9264752864837646\n",
      "EPOCH 310:\n",
      "  batch 50 loss: 1.837397470474243\n",
      "  batch 100 loss: 1.7988005900382995\n",
      "LOSS train 1.7988005900382995 valid 1.9175317287445068\n",
      "EPOCH 311:\n",
      "  batch 50 loss: 1.8110458040237427\n",
      "  batch 100 loss: 1.811504054069519\n",
      "LOSS train 1.811504054069519 valid 1.922284483909607\n",
      "EPOCH 312:\n",
      "  batch 50 loss: 1.797290403842926\n",
      "  batch 100 loss: 1.8275318717956544\n",
      "LOSS train 1.8275318717956544 valid 1.9412062168121338\n",
      "EPOCH 313:\n",
      "  batch 50 loss: 1.8145202612876892\n",
      "  batch 100 loss: 1.8135124802589417\n",
      "LOSS train 1.8135124802589417 valid 1.9340420961380005\n",
      "EPOCH 314:\n",
      "  batch 50 loss: 1.8228219771385192\n",
      "  batch 100 loss: 1.8068772506713868\n",
      "LOSS train 1.8068772506713868 valid 1.9268804788589478\n",
      "EPOCH 315:\n",
      "  batch 50 loss: 1.7936099314689635\n",
      "  batch 100 loss: 1.8254404973983764\n",
      "LOSS train 1.8254404973983764 valid 1.934719443321228\n",
      "EPOCH 316:\n",
      "  batch 50 loss: 1.805789442062378\n",
      "  batch 100 loss: 1.8155553841590881\n",
      "LOSS train 1.8155553841590881 valid 1.9412295818328857\n",
      "EPOCH 317:\n",
      "  batch 50 loss: 1.8103263258934021\n",
      "  batch 100 loss: 1.8186633253097535\n",
      "LOSS train 1.8186633253097535 valid 1.9094123840332031\n",
      "EPOCH 318:\n",
      "  batch 50 loss: 1.8058556962013244\n",
      "  batch 100 loss: 1.8180765342712402\n",
      "LOSS train 1.8180765342712402 valid 1.91348135471344\n",
      "EPOCH 319:\n",
      "  batch 50 loss: 1.8108595895767212\n",
      "  batch 100 loss: 1.8063683915138244\n",
      "LOSS train 1.8063683915138244 valid 1.961124300956726\n",
      "EPOCH 320:\n",
      "  batch 50 loss: 1.8025636863708496\n",
      "  batch 100 loss: 1.8296805024147034\n",
      "LOSS train 1.8296805024147034 valid 1.9220762252807617\n",
      "EPOCH 321:\n",
      "  batch 50 loss: 1.8116778326034546\n",
      "  batch 100 loss: 1.8037793159484863\n",
      "LOSS train 1.8037793159484863 valid 1.9396355152130127\n",
      "EPOCH 322:\n",
      "  batch 50 loss: 1.8008786964416503\n",
      "  batch 100 loss: 1.802068407535553\n",
      "LOSS train 1.802068407535553 valid 1.9266842603683472\n",
      "EPOCH 323:\n",
      "  batch 50 loss: 1.796301748752594\n",
      "  batch 100 loss: 1.804882483482361\n",
      "LOSS train 1.804882483482361 valid 1.9226083755493164\n",
      "EPOCH 324:\n",
      "  batch 50 loss: 1.812665512561798\n",
      "  batch 100 loss: 1.799431722164154\n",
      "LOSS train 1.799431722164154 valid 1.961856484413147\n",
      "EPOCH 325:\n",
      "  batch 50 loss: 1.786318280696869\n",
      "  batch 100 loss: 1.8082527446746826\n",
      "LOSS train 1.8082527446746826 valid 1.9386191368103027\n",
      "EPOCH 326:\n",
      "  batch 50 loss: 1.8086439299583434\n",
      "  batch 100 loss: 1.8059160375595094\n",
      "LOSS train 1.8059160375595094 valid 1.9487518072128296\n",
      "EPOCH 327:\n",
      "  batch 50 loss: 1.80894962310791\n",
      "  batch 100 loss: 1.7883951306343078\n",
      "LOSS train 1.7883951306343078 valid 1.9089791774749756\n",
      "EPOCH 328:\n",
      "  batch 50 loss: 1.8097335815429687\n",
      "  batch 100 loss: 1.8150394701957702\n",
      "LOSS train 1.8150394701957702 valid 1.9167020320892334\n",
      "EPOCH 329:\n",
      "  batch 50 loss: 1.8103718972206115\n",
      "  batch 100 loss: 1.7995991587638855\n",
      "LOSS train 1.7995991587638855 valid 1.906297206878662\n",
      "EPOCH 330:\n",
      "  batch 50 loss: 1.7974459385871888\n",
      "  batch 100 loss: 1.8065384721755982\n",
      "LOSS train 1.8065384721755982 valid 1.9130650758743286\n",
      "EPOCH 331:\n",
      "  batch 50 loss: 1.824683768749237\n",
      "  batch 100 loss: 1.8440587043762207\n",
      "LOSS train 1.8440587043762207 valid 1.9177266359329224\n",
      "EPOCH 332:\n",
      "  batch 50 loss: 1.7995582008361817\n",
      "  batch 100 loss: 1.8181121706962586\n",
      "LOSS train 1.8181121706962586 valid 1.944968342781067\n",
      "EPOCH 333:\n",
      "  batch 50 loss: 1.8201605820655822\n",
      "  batch 100 loss: 1.784031834602356\n",
      "LOSS train 1.784031834602356 valid 1.921938419342041\n",
      "EPOCH 334:\n",
      "  batch 50 loss: 1.7724324440956116\n",
      "  batch 100 loss: 1.8112320399284363\n",
      "LOSS train 1.8112320399284363 valid 1.9110002517700195\n",
      "EPOCH 335:\n",
      "  batch 50 loss: 1.8212831974029542\n",
      "  batch 100 loss: 1.8061986207962035\n",
      "LOSS train 1.8061986207962035 valid 1.9134842157363892\n",
      "EPOCH 336:\n",
      "  batch 50 loss: 1.8070741868019105\n",
      "  batch 100 loss: 1.8046766304969788\n",
      "LOSS train 1.8046766304969788 valid 1.9172171354293823\n",
      "EPOCH 337:\n",
      "  batch 50 loss: 1.811016299724579\n",
      "  batch 100 loss: 1.7988493871688842\n",
      "LOSS train 1.7988493871688842 valid 1.911010503768921\n",
      "EPOCH 338:\n",
      "  batch 50 loss: 1.795420277118683\n",
      "  batch 100 loss: 1.8146172499656676\n",
      "LOSS train 1.8146172499656676 valid 1.90717613697052\n",
      "EPOCH 339:\n",
      "  batch 50 loss: 1.7929987955093383\n",
      "  batch 100 loss: 1.815483396053314\n",
      "LOSS train 1.815483396053314 valid 1.908494234085083\n",
      "EPOCH 340:\n",
      "  batch 50 loss: 1.7956943535804748\n",
      "  batch 100 loss: 1.8203967499732971\n",
      "LOSS train 1.8203967499732971 valid 1.9262944459915161\n",
      "EPOCH 341:\n",
      "  batch 50 loss: 1.7821176147460938\n",
      "  batch 100 loss: 1.8132166051864624\n",
      "LOSS train 1.8132166051864624 valid 1.9185564517974854\n",
      "EPOCH 342:\n",
      "  batch 50 loss: 1.8007999920845033\n",
      "  batch 100 loss: 1.8067133116722107\n",
      "LOSS train 1.8067133116722107 valid 1.9164689779281616\n",
      "EPOCH 343:\n",
      "  batch 50 loss: 1.8053733205795288\n",
      "  batch 100 loss: 1.7921241116523743\n",
      "LOSS train 1.7921241116523743 valid 1.9193058013916016\n",
      "EPOCH 344:\n",
      "  batch 50 loss: 1.7840860891342163\n",
      "  batch 100 loss: 1.8186791586875914\n",
      "LOSS train 1.8186791586875914 valid 1.9232362508773804\n",
      "EPOCH 345:\n",
      "  batch 50 loss: 1.7924343967437744\n",
      "  batch 100 loss: 1.8132109117507935\n",
      "LOSS train 1.8132109117507935 valid 1.9081639051437378\n",
      "EPOCH 346:\n",
      "  batch 50 loss: 1.8156403255462648\n",
      "  batch 100 loss: 1.7712773847579957\n",
      "LOSS train 1.7712773847579957 valid 1.908750057220459\n",
      "EPOCH 347:\n",
      "  batch 50 loss: 1.8061872410774231\n",
      "  batch 100 loss: 1.788680305480957\n",
      "LOSS train 1.788680305480957 valid 1.9321812391281128\n",
      "EPOCH 348:\n",
      "  batch 50 loss: 1.773489661216736\n",
      "  batch 100 loss: 1.83277893781662\n",
      "LOSS train 1.83277893781662 valid 1.9009734392166138\n",
      "EPOCH 349:\n",
      "  batch 50 loss: 1.8071487760543823\n",
      "  batch 100 loss: 1.7904231309890748\n",
      "LOSS train 1.7904231309890748 valid 1.9051779508590698\n",
      "EPOCH 350:\n",
      "  batch 50 loss: 1.8046059584617615\n",
      "  batch 100 loss: 1.8019196677207947\n",
      "LOSS train 1.8019196677207947 valid 1.9147260189056396\n",
      "EPOCH 351:\n",
      "  batch 50 loss: 1.8054287219047547\n",
      "  batch 100 loss: 1.788050639629364\n",
      "LOSS train 1.788050639629364 valid 1.9032678604125977\n",
      "EPOCH 352:\n",
      "  batch 50 loss: 1.804746811389923\n",
      "  batch 100 loss: 1.789855453968048\n",
      "LOSS train 1.789855453968048 valid 1.905898928642273\n",
      "EPOCH 353:\n",
      "  batch 50 loss: 1.8057430291175842\n",
      "  batch 100 loss: 1.7879012870788573\n",
      "LOSS train 1.7879012870788573 valid 1.908298134803772\n",
      "EPOCH 354:\n",
      "  batch 50 loss: 1.7940824842453003\n",
      "  batch 100 loss: 1.8256693029403686\n",
      "LOSS train 1.8256693029403686 valid 1.9140141010284424\n",
      "EPOCH 355:\n",
      "  batch 50 loss: 1.8173505878448486\n",
      "  batch 100 loss: 1.7916214871406555\n",
      "LOSS train 1.7916214871406555 valid 1.9002457857131958\n",
      "EPOCH 356:\n",
      "  batch 50 loss: 1.8103567957878113\n",
      "  batch 100 loss: 1.7797909188270569\n",
      "LOSS train 1.7797909188270569 valid 1.914670467376709\n",
      "EPOCH 357:\n",
      "  batch 50 loss: 1.806368999481201\n",
      "  batch 100 loss: 1.7956162524223327\n",
      "LOSS train 1.7956162524223327 valid 1.9136492013931274\n",
      "EPOCH 358:\n",
      "  batch 50 loss: 1.7903208827972412\n",
      "  batch 100 loss: 1.7906898498535155\n",
      "LOSS train 1.7906898498535155 valid 1.9031320810317993\n",
      "EPOCH 359:\n",
      "  batch 50 loss: 1.7814527535438538\n",
      "  batch 100 loss: 1.8101700830459595\n",
      "LOSS train 1.8101700830459595 valid 1.901281476020813\n",
      "EPOCH 360:\n",
      "  batch 50 loss: 1.794608600139618\n",
      "  batch 100 loss: 1.7826570916175841\n",
      "LOSS train 1.7826570916175841 valid 1.9156193733215332\n",
      "EPOCH 361:\n",
      "  batch 50 loss: 1.793884472846985\n",
      "  batch 100 loss: 1.7946303462982178\n",
      "LOSS train 1.7946303462982178 valid 1.9044002294540405\n",
      "EPOCH 362:\n",
      "  batch 50 loss: 1.7928940725326539\n",
      "  batch 100 loss: 1.7911768460273743\n",
      "LOSS train 1.7911768460273743 valid 1.900729775428772\n",
      "EPOCH 363:\n",
      "  batch 50 loss: 1.8191546297073364\n",
      "  batch 100 loss: 1.7817056727409364\n",
      "LOSS train 1.7817056727409364 valid 1.9053767919540405\n",
      "EPOCH 364:\n",
      "  batch 50 loss: 1.785973129272461\n",
      "  batch 100 loss: 1.800986077785492\n",
      "LOSS train 1.800986077785492 valid 1.9517459869384766\n",
      "EPOCH 365:\n",
      "  batch 50 loss: 1.7795781874656678\n",
      "  batch 100 loss: 1.8051296949386597\n",
      "LOSS train 1.8051296949386597 valid 1.9107853174209595\n",
      "EPOCH 366:\n",
      "  batch 50 loss: 1.7852289748191834\n",
      "  batch 100 loss: 1.800514132976532\n",
      "LOSS train 1.800514132976532 valid 1.9187366962432861\n",
      "EPOCH 367:\n",
      "  batch 50 loss: 1.7858453392982483\n",
      "  batch 100 loss: 1.8132310175895692\n",
      "LOSS train 1.8132310175895692 valid 1.9573959112167358\n",
      "EPOCH 368:\n",
      "  batch 50 loss: 1.7912095594406128\n",
      "  batch 100 loss: 1.7921818041801452\n",
      "LOSS train 1.7921818041801452 valid 1.8985486030578613\n",
      "EPOCH 369:\n",
      "  batch 50 loss: 1.791950922012329\n",
      "  batch 100 loss: 1.8045312190055847\n",
      "LOSS train 1.8045312190055847 valid 1.9044159650802612\n",
      "EPOCH 370:\n",
      "  batch 50 loss: 1.7880787181854247\n",
      "  batch 100 loss: 1.7815133595466615\n",
      "LOSS train 1.7815133595466615 valid 1.9012202024459839\n",
      "EPOCH 371:\n",
      "  batch 50 loss: 1.8118996787071229\n",
      "  batch 100 loss: 1.7785973787307738\n",
      "LOSS train 1.7785973787307738 valid 1.9048123359680176\n",
      "EPOCH 372:\n",
      "  batch 50 loss: 1.8053144025802612\n",
      "  batch 100 loss: 1.7874412512779236\n",
      "LOSS train 1.7874412512779236 valid 1.9052741527557373\n",
      "EPOCH 373:\n",
      "  batch 50 loss: 1.7903881096839904\n",
      "  batch 100 loss: 1.76899188041687\n",
      "LOSS train 1.76899188041687 valid 1.918381929397583\n",
      "EPOCH 374:\n",
      "  batch 50 loss: 1.8054488611221313\n",
      "  batch 100 loss: 1.7723416233062743\n",
      "LOSS train 1.7723416233062743 valid 1.9182064533233643\n",
      "EPOCH 375:\n",
      "  batch 50 loss: 1.7719168710708617\n",
      "  batch 100 loss: 1.8358907628059387\n",
      "LOSS train 1.8358907628059387 valid 1.902100682258606\n",
      "EPOCH 376:\n",
      "  batch 50 loss: 1.7915741777420044\n",
      "  batch 100 loss: 1.7813894534111023\n",
      "LOSS train 1.7813894534111023 valid 1.9162635803222656\n",
      "EPOCH 377:\n",
      "  batch 50 loss: 1.7862577152252197\n",
      "  batch 100 loss: 1.77057293176651\n",
      "LOSS train 1.77057293176651 valid 1.9020951986312866\n",
      "EPOCH 378:\n",
      "  batch 50 loss: 1.789749584197998\n",
      "  batch 100 loss: 1.7704142689704896\n",
      "LOSS train 1.7704142689704896 valid 1.927964210510254\n",
      "EPOCH 379:\n",
      "  batch 50 loss: 1.7901025295257569\n",
      "  batch 100 loss: 1.7951999926567077\n",
      "LOSS train 1.7951999926567077 valid 1.8986457586288452\n",
      "EPOCH 380:\n",
      "  batch 50 loss: 1.798069772720337\n",
      "  batch 100 loss: 1.7934529638290406\n",
      "LOSS train 1.7934529638290406 valid 1.8906559944152832\n",
      "EPOCH 381:\n",
      "  batch 50 loss: 1.7825074458122254\n",
      "  batch 100 loss: 1.802811360359192\n",
      "LOSS train 1.802811360359192 valid 1.910178780555725\n",
      "EPOCH 382:\n",
      "  batch 50 loss: 1.7986201930046082\n",
      "  batch 100 loss: 1.77449800491333\n",
      "LOSS train 1.77449800491333 valid 1.902058482170105\n",
      "EPOCH 383:\n",
      "  batch 50 loss: 1.7877879881858825\n",
      "  batch 100 loss: 1.8024870300292968\n",
      "LOSS train 1.8024870300292968 valid 1.895837426185608\n",
      "EPOCH 384:\n",
      "  batch 50 loss: 1.787960979938507\n",
      "  batch 100 loss: 1.803868567943573\n",
      "LOSS train 1.803868567943573 valid 1.9083670377731323\n",
      "EPOCH 385:\n",
      "  batch 50 loss: 1.7969095134735107\n",
      "  batch 100 loss: 1.7834000134468078\n",
      "LOSS train 1.7834000134468078 valid 1.8903963565826416\n",
      "EPOCH 386:\n",
      "  batch 50 loss: 1.8018759679794312\n",
      "  batch 100 loss: 1.7815647625923157\n",
      "LOSS train 1.7815647625923157 valid 1.9074708223342896\n",
      "EPOCH 387:\n",
      "  batch 50 loss: 1.8001446032524109\n",
      "  batch 100 loss: 1.7863384580612183\n",
      "LOSS train 1.7863384580612183 valid 1.908892035484314\n",
      "EPOCH 388:\n",
      "  batch 50 loss: 1.7872338724136352\n",
      "  batch 100 loss: 1.7671470999717713\n",
      "LOSS train 1.7671470999717713 valid 1.9036219120025635\n",
      "EPOCH 389:\n",
      "  batch 50 loss: 1.8028246688842773\n",
      "  batch 100 loss: 1.7967988991737365\n",
      "LOSS train 1.7967988991737365 valid 1.8949143886566162\n",
      "EPOCH 390:\n",
      "  batch 50 loss: 1.802465434074402\n",
      "  batch 100 loss: 1.788224880695343\n",
      "LOSS train 1.788224880695343 valid 1.8977946043014526\n",
      "EPOCH 391:\n",
      "  batch 50 loss: 1.7712222862243652\n",
      "  batch 100 loss: 1.7919117975234986\n",
      "LOSS train 1.7919117975234986 valid 1.9053027629852295\n",
      "EPOCH 392:\n",
      "  batch 50 loss: 1.8022243618965148\n",
      "  batch 100 loss: 1.7628646111488342\n",
      "LOSS train 1.7628646111488342 valid 1.8966550827026367\n",
      "EPOCH 393:\n",
      "  batch 50 loss: 1.747334554195404\n",
      "  batch 100 loss: 1.8357313895225524\n",
      "LOSS train 1.8357313895225524 valid 1.9282801151275635\n",
      "EPOCH 394:\n",
      "  batch 50 loss: 1.7785556364059447\n",
      "  batch 100 loss: 1.8162182974815368\n",
      "LOSS train 1.8162182974815368 valid 1.9349110126495361\n",
      "EPOCH 395:\n",
      "  batch 50 loss: 1.7720512437820435\n",
      "  batch 100 loss: 1.778402497768402\n",
      "LOSS train 1.778402497768402 valid 1.9066150188446045\n",
      "EPOCH 396:\n",
      "  batch 50 loss: 1.7713569211959839\n",
      "  batch 100 loss: 1.8090589833259583\n",
      "LOSS train 1.8090589833259583 valid 1.9191598892211914\n",
      "EPOCH 397:\n",
      "  batch 50 loss: 1.7811998224258423\n",
      "  batch 100 loss: 1.7679015040397643\n",
      "LOSS train 1.7679015040397643 valid 1.9090076684951782\n",
      "EPOCH 398:\n",
      "  batch 50 loss: 1.7841185927391052\n",
      "  batch 100 loss: 1.7844231629371643\n",
      "LOSS train 1.7844231629371643 valid 1.9424960613250732\n",
      "EPOCH 399:\n",
      "  batch 50 loss: 1.7997515249252318\n",
      "  batch 100 loss: 1.7930125451087953\n",
      "LOSS train 1.7930125451087953 valid 1.8958431482315063\n",
      "EPOCH 400:\n",
      "  batch 50 loss: 1.7828955483436584\n",
      "  batch 100 loss: 1.8094848537445067\n",
      "LOSS train 1.8094848537445067 valid 1.9048978090286255\n",
      "EPOCH 401:\n",
      "  batch 50 loss: 1.7974351048469543\n",
      "  batch 100 loss: 1.7727633905410767\n",
      "LOSS train 1.7727633905410767 valid 1.8952313661575317\n",
      "EPOCH 402:\n",
      "  batch 50 loss: 1.7760120987892152\n",
      "  batch 100 loss: 1.7908451056480408\n",
      "LOSS train 1.7908451056480408 valid 1.9168784618377686\n",
      "EPOCH 403:\n",
      "  batch 50 loss: 1.7735183691978456\n",
      "  batch 100 loss: 1.7761089134216308\n",
      "LOSS train 1.7761089134216308 valid 1.8996284008026123\n",
      "EPOCH 404:\n",
      "  batch 50 loss: 1.7769783449172973\n",
      "  batch 100 loss: 1.7819196939468385\n",
      "LOSS train 1.7819196939468385 valid 1.9258487224578857\n",
      "EPOCH 405:\n",
      "  batch 50 loss: 1.7836317920684814\n",
      "  batch 100 loss: 1.77579909324646\n",
      "LOSS train 1.77579909324646 valid 1.8943257331848145\n",
      "EPOCH 406:\n",
      "  batch 50 loss: 1.7695651578903198\n",
      "  batch 100 loss: 1.7945125246047973\n",
      "LOSS train 1.7945125246047973 valid 1.9433273077011108\n",
      "EPOCH 407:\n",
      "  batch 50 loss: 1.7925961112976074\n",
      "  batch 100 loss: 1.7442260432243346\n",
      "LOSS train 1.7442260432243346 valid 1.8989927768707275\n",
      "EPOCH 408:\n",
      "  batch 50 loss: 1.782207009792328\n",
      "  batch 100 loss: 1.7702551698684692\n",
      "LOSS train 1.7702551698684692 valid 1.9005769491195679\n",
      "EPOCH 409:\n",
      "  batch 50 loss: 1.7533247947692872\n",
      "  batch 100 loss: 1.7981302165985107\n",
      "LOSS train 1.7981302165985107 valid 1.9008768796920776\n",
      "EPOCH 410:\n",
      "  batch 50 loss: 1.7705853152275086\n",
      "  batch 100 loss: 1.7911810207366943\n",
      "LOSS train 1.7911810207366943 valid 1.9242515563964844\n",
      "EPOCH 411:\n",
      "  batch 50 loss: 1.8183477926254272\n",
      "  batch 100 loss: 1.7429104447364807\n",
      "LOSS train 1.7429104447364807 valid 1.927977442741394\n",
      "EPOCH 412:\n",
      "  batch 50 loss: 1.7864991998672486\n",
      "  batch 100 loss: 1.7678271794319154\n",
      "LOSS train 1.7678271794319154 valid 1.890856385231018\n",
      "EPOCH 413:\n",
      "  batch 50 loss: 1.7894495105743409\n",
      "  batch 100 loss: 1.7675499176979066\n",
      "LOSS train 1.7675499176979066 valid 1.920353889465332\n",
      "EPOCH 414:\n",
      "  batch 50 loss: 1.7778371381759643\n",
      "  batch 100 loss: 1.7701342105865479\n",
      "LOSS train 1.7701342105865479 valid 1.8902374505996704\n",
      "EPOCH 415:\n",
      "  batch 50 loss: 1.7817365002632142\n",
      "  batch 100 loss: 1.7809638047218324\n",
      "LOSS train 1.7809638047218324 valid 1.9180487394332886\n",
      "EPOCH 416:\n",
      "  batch 50 loss: 1.7916311025619507\n",
      "  batch 100 loss: 1.7687726354598998\n",
      "LOSS train 1.7687726354598998 valid 1.901493787765503\n",
      "EPOCH 417:\n",
      "  batch 50 loss: 1.8111589670181274\n",
      "  batch 100 loss: 1.7660750293731688\n",
      "LOSS train 1.7660750293731688 valid 1.8939502239227295\n",
      "EPOCH 418:\n",
      "  batch 50 loss: 1.7908151173591613\n",
      "  batch 100 loss: 1.766808705329895\n",
      "LOSS train 1.766808705329895 valid 1.9002944231033325\n",
      "EPOCH 419:\n",
      "  batch 50 loss: 1.764635112285614\n",
      "  batch 100 loss: 1.7821547198295593\n",
      "LOSS train 1.7821547198295593 valid 1.9041013717651367\n",
      "EPOCH 420:\n",
      "  batch 50 loss: 1.7869031500816346\n",
      "  batch 100 loss: 1.7730525040626526\n",
      "LOSS train 1.7730525040626526 valid 1.8960156440734863\n",
      "EPOCH 421:\n",
      "  batch 50 loss: 1.7881403851509095\n",
      "  batch 100 loss: 1.7784656143188478\n",
      "LOSS train 1.7784656143188478 valid 1.902287244796753\n",
      "EPOCH 422:\n",
      "  batch 50 loss: 1.7946299338340759\n",
      "  batch 100 loss: 1.7738487911224365\n",
      "LOSS train 1.7738487911224365 valid 1.89327871799469\n",
      "EPOCH 423:\n",
      "  batch 50 loss: 1.773277759552002\n",
      "  batch 100 loss: 1.7857053565979004\n",
      "LOSS train 1.7857053565979004 valid 1.8856912851333618\n",
      "EPOCH 424:\n",
      "  batch 50 loss: 1.7765323734283447\n",
      "  batch 100 loss: 1.8101965975761414\n",
      "LOSS train 1.8101965975761414 valid 1.8776674270629883\n",
      "EPOCH 425:\n",
      "  batch 50 loss: 1.7986266136169433\n",
      "  batch 100 loss: 1.7575903415679932\n",
      "LOSS train 1.7575903415679932 valid 1.8894121646881104\n",
      "EPOCH 426:\n",
      "  batch 50 loss: 1.7609380531311034\n",
      "  batch 100 loss: 1.7901762509346009\n",
      "LOSS train 1.7901762509346009 valid 1.8937586545944214\n",
      "EPOCH 427:\n",
      "  batch 50 loss: 1.7506535649299622\n",
      "  batch 100 loss: 1.760504837036133\n",
      "LOSS train 1.760504837036133 valid 1.8962897062301636\n",
      "EPOCH 428:\n",
      "  batch 50 loss: 1.7727515125274658\n",
      "  batch 100 loss: 1.79547705411911\n",
      "LOSS train 1.79547705411911 valid 1.9037498235702515\n",
      "EPOCH 429:\n",
      "  batch 50 loss: 1.7711817288398743\n",
      "  batch 100 loss: 1.7895831370353699\n",
      "LOSS train 1.7895831370353699 valid 1.9256117343902588\n",
      "EPOCH 430:\n",
      "  batch 50 loss: 1.7686168503761293\n",
      "  batch 100 loss: 1.7680036067962646\n",
      "LOSS train 1.7680036067962646 valid 1.9054734706878662\n",
      "EPOCH 431:\n",
      "  batch 50 loss: 1.7552096509933472\n",
      "  batch 100 loss: 1.7845509719848633\n",
      "LOSS train 1.7845509719848633 valid 1.900231122970581\n",
      "EPOCH 432:\n",
      "  batch 50 loss: 1.803995842933655\n",
      "  batch 100 loss: 1.7580047869682311\n",
      "LOSS train 1.7580047869682311 valid 1.8892011642456055\n",
      "EPOCH 433:\n",
      "  batch 50 loss: 1.784193320274353\n",
      "  batch 100 loss: 1.7900826907157898\n",
      "LOSS train 1.7900826907157898 valid 1.8943867683410645\n",
      "EPOCH 434:\n",
      "  batch 50 loss: 1.7610815358161926\n",
      "  batch 100 loss: 1.780309329032898\n",
      "LOSS train 1.780309329032898 valid 1.900451421737671\n",
      "EPOCH 435:\n",
      "  batch 50 loss: 1.7416355752944945\n",
      "  batch 100 loss: 1.799698669910431\n",
      "LOSS train 1.799698669910431 valid 1.8952467441558838\n",
      "EPOCH 436:\n",
      "  batch 50 loss: 1.7926747822761535\n",
      "  batch 100 loss: 1.7660862874984742\n",
      "LOSS train 1.7660862874984742 valid 1.8859326839447021\n",
      "EPOCH 437:\n",
      "  batch 50 loss: 1.780999779701233\n",
      "  batch 100 loss: 1.7747188544273376\n",
      "LOSS train 1.7747188544273376 valid 1.8919341564178467\n",
      "EPOCH 438:\n",
      "  batch 50 loss: 1.7634563899040223\n",
      "  batch 100 loss: 1.7701372051239013\n",
      "LOSS train 1.7701372051239013 valid 1.8842384815216064\n",
      "EPOCH 439:\n",
      "  batch 50 loss: 1.761187391281128\n",
      "  batch 100 loss: 1.767485909461975\n",
      "LOSS train 1.767485909461975 valid 1.9082655906677246\n",
      "EPOCH 440:\n",
      "  batch 50 loss: 1.791887731552124\n",
      "  batch 100 loss: 1.7444747352600098\n",
      "LOSS train 1.7444747352600098 valid 1.911149024963379\n",
      "EPOCH 441:\n",
      "  batch 50 loss: 1.7799897241592406\n",
      "  batch 100 loss: 1.7647063779830932\n",
      "LOSS train 1.7647063779830932 valid 1.8896318674087524\n",
      "EPOCH 442:\n",
      "  batch 50 loss: 1.760055387020111\n",
      "  batch 100 loss: 1.7606858134269714\n",
      "LOSS train 1.7606858134269714 valid 1.884579062461853\n",
      "EPOCH 443:\n",
      "  batch 50 loss: 1.7684913063049317\n",
      "  batch 100 loss: 1.7794567918777466\n",
      "LOSS train 1.7794567918777466 valid 1.9009270668029785\n",
      "EPOCH 444:\n",
      "  batch 50 loss: 1.7690115761756897\n",
      "  batch 100 loss: 1.7803941440582276\n",
      "LOSS train 1.7803941440582276 valid 1.8846104145050049\n",
      "EPOCH 445:\n",
      "  batch 50 loss: 1.7444563007354736\n",
      "  batch 100 loss: 1.7799131345748902\n",
      "LOSS train 1.7799131345748902 valid 1.892092227935791\n",
      "EPOCH 446:\n",
      "  batch 50 loss: 1.7598852658271789\n",
      "  batch 100 loss: 1.7987277698516846\n",
      "LOSS train 1.7987277698516846 valid 1.9168769121170044\n",
      "EPOCH 447:\n",
      "  batch 50 loss: 1.753692409992218\n",
      "  batch 100 loss: 1.7779125094413757\n",
      "LOSS train 1.7779125094413757 valid 1.8888107538223267\n",
      "EPOCH 448:\n",
      "  batch 50 loss: 1.7470933985710144\n",
      "  batch 100 loss: 1.7788854074478149\n",
      "LOSS train 1.7788854074478149 valid 1.8850458860397339\n",
      "EPOCH 449:\n",
      "  batch 50 loss: 1.7471368622779846\n",
      "  batch 100 loss: 1.784621901512146\n",
      "LOSS train 1.784621901512146 valid 1.909213900566101\n",
      "EPOCH 450:\n",
      "  batch 50 loss: 1.7767586350440978\n",
      "  batch 100 loss: 1.7519789290428163\n",
      "LOSS train 1.7519789290428163 valid 1.888860821723938\n",
      "EPOCH 451:\n",
      "  batch 50 loss: 1.7891834378242493\n",
      "  batch 100 loss: 1.7673188161849975\n",
      "LOSS train 1.7673188161849975 valid 1.9054607152938843\n",
      "EPOCH 452:\n",
      "  batch 50 loss: 1.7693608379364014\n",
      "  batch 100 loss: 1.7675302982330323\n",
      "LOSS train 1.7675302982330323 valid 1.8899080753326416\n",
      "EPOCH 453:\n",
      "  batch 50 loss: 1.772217218875885\n",
      "  batch 100 loss: 1.7487265634536744\n",
      "LOSS train 1.7487265634536744 valid 1.8906066417694092\n",
      "EPOCH 454:\n",
      "  batch 50 loss: 1.7625507020950317\n",
      "  batch 100 loss: 1.7617668914794922\n",
      "LOSS train 1.7617668914794922 valid 1.8898712396621704\n",
      "EPOCH 455:\n",
      "  batch 50 loss: 1.774642517566681\n",
      "  batch 100 loss: 1.7976287555694581\n",
      "LOSS train 1.7976287555694581 valid 1.8981940746307373\n",
      "EPOCH 456:\n",
      "  batch 50 loss: 1.780976710319519\n",
      "  batch 100 loss: 1.7778915143013\n",
      "LOSS train 1.7778915143013 valid 1.8717137575149536\n",
      "EPOCH 457:\n",
      "  batch 50 loss: 1.7644087314605712\n",
      "  batch 100 loss: 1.7763036704063415\n",
      "LOSS train 1.7763036704063415 valid 1.8856472969055176\n",
      "EPOCH 458:\n",
      "  batch 50 loss: 1.7513834381103515\n",
      "  batch 100 loss: 1.7584041547775269\n",
      "LOSS train 1.7584041547775269 valid 1.8858283758163452\n",
      "EPOCH 459:\n",
      "  batch 50 loss: 1.787478702068329\n",
      "  batch 100 loss: 1.7534523344039916\n",
      "LOSS train 1.7534523344039916 valid 1.8930068016052246\n",
      "EPOCH 460:\n",
      "  batch 50 loss: 1.7723106169700622\n",
      "  batch 100 loss: 1.7768567967414857\n",
      "LOSS train 1.7768567967414857 valid 1.8892358541488647\n",
      "EPOCH 461:\n",
      "  batch 50 loss: 1.7789262413978577\n",
      "  batch 100 loss: 1.7747899770736695\n",
      "LOSS train 1.7747899770736695 valid 1.8724806308746338\n",
      "EPOCH 462:\n",
      "  batch 50 loss: 1.7785682559013367\n",
      "  batch 100 loss: 1.7684684467315674\n",
      "LOSS train 1.7684684467315674 valid 1.8806633949279785\n",
      "EPOCH 463:\n",
      "  batch 50 loss: 1.7919754076004029\n",
      "  batch 100 loss: 1.7614551448822022\n",
      "LOSS train 1.7614551448822022 valid 1.8958019018173218\n",
      "EPOCH 464:\n",
      "  batch 50 loss: 1.7620393657684326\n",
      "  batch 100 loss: 1.7756197595596312\n",
      "LOSS train 1.7756197595596312 valid 1.888253927230835\n",
      "EPOCH 465:\n",
      "  batch 50 loss: 1.777863984107971\n",
      "  batch 100 loss: 1.7512547755241394\n",
      "LOSS train 1.7512547755241394 valid 1.881897211074829\n",
      "EPOCH 466:\n",
      "  batch 50 loss: 1.7755888533592223\n",
      "  batch 100 loss: 1.7542406058311462\n",
      "LOSS train 1.7542406058311462 valid 1.8850537538528442\n",
      "EPOCH 467:\n",
      "  batch 50 loss: 1.7567345643043517\n",
      "  batch 100 loss: 1.7755818462371826\n",
      "LOSS train 1.7755818462371826 valid 1.8705679178237915\n",
      "EPOCH 468:\n",
      "  batch 50 loss: 1.772987813949585\n",
      "  batch 100 loss: 1.757468616962433\n",
      "LOSS train 1.757468616962433 valid 1.8886977434158325\n",
      "EPOCH 469:\n",
      "  batch 50 loss: 1.7643158078193664\n",
      "  batch 100 loss: 1.7603238105773926\n",
      "LOSS train 1.7603238105773926 valid 1.8729432821273804\n",
      "EPOCH 470:\n",
      "  batch 50 loss: 1.7466113829612733\n",
      "  batch 100 loss: 1.7788881301879882\n",
      "LOSS train 1.7788881301879882 valid 1.9063689708709717\n",
      "EPOCH 471:\n",
      "  batch 50 loss: 1.7764044380187989\n",
      "  batch 100 loss: 1.7506586194038392\n",
      "LOSS train 1.7506586194038392 valid 1.9136565923690796\n",
      "EPOCH 472:\n",
      "  batch 50 loss: 1.7820688581466675\n",
      "  batch 100 loss: 1.7675077271461488\n",
      "LOSS train 1.7675077271461488 valid 1.9192631244659424\n",
      "EPOCH 473:\n",
      "  batch 50 loss: 1.7794115686416625\n",
      "  batch 100 loss: 1.773100838661194\n",
      "LOSS train 1.773100838661194 valid 1.9295350313186646\n",
      "EPOCH 474:\n",
      "  batch 50 loss: 1.756056022644043\n",
      "  batch 100 loss: 1.7686268234252929\n",
      "LOSS train 1.7686268234252929 valid 1.8942056894302368\n",
      "EPOCH 475:\n",
      "  batch 50 loss: 1.7690605974197389\n",
      "  batch 100 loss: 1.770550057888031\n",
      "LOSS train 1.770550057888031 valid 1.9267704486846924\n",
      "EPOCH 476:\n",
      "  batch 50 loss: 1.780358076095581\n",
      "  batch 100 loss: 1.7523915219306945\n",
      "LOSS train 1.7523915219306945 valid 1.8887851238250732\n",
      "EPOCH 477:\n",
      "  batch 50 loss: 1.755198450088501\n",
      "  batch 100 loss: 1.7610784649848938\n",
      "LOSS train 1.7610784649848938 valid 1.8776013851165771\n",
      "EPOCH 478:\n",
      "  batch 50 loss: 1.7783453822135926\n",
      "  batch 100 loss: 1.7670368003845214\n",
      "LOSS train 1.7670368003845214 valid 1.9281340837478638\n",
      "EPOCH 479:\n",
      "  batch 50 loss: 1.785823323726654\n",
      "  batch 100 loss: 1.7494919657707215\n",
      "LOSS train 1.7494919657707215 valid 1.8872803449630737\n",
      "EPOCH 480:\n",
      "  batch 50 loss: 1.7588769793510437\n",
      "  batch 100 loss: 1.764107618331909\n",
      "LOSS train 1.764107618331909 valid 1.8853968381881714\n",
      "EPOCH 481:\n",
      "  batch 50 loss: 1.763162853717804\n",
      "  batch 100 loss: 1.7748656487464904\n",
      "LOSS train 1.7748656487464904 valid 1.9057828187942505\n",
      "EPOCH 482:\n",
      "  batch 50 loss: 1.7702389025688172\n",
      "  batch 100 loss: 1.7490820431709289\n",
      "LOSS train 1.7490820431709289 valid 1.8815699815750122\n",
      "EPOCH 483:\n",
      "  batch 50 loss: 1.7369603061676024\n",
      "  batch 100 loss: 1.7718544840812682\n",
      "LOSS train 1.7718544840812682 valid 1.9047000408172607\n",
      "EPOCH 484:\n",
      "  batch 50 loss: 1.7689888048171998\n",
      "  batch 100 loss: 1.7456706762313843\n",
      "LOSS train 1.7456706762313843 valid 1.8835405111312866\n",
      "EPOCH 485:\n",
      "  batch 50 loss: 1.7396133160591125\n",
      "  batch 100 loss: 1.7916110014915467\n",
      "LOSS train 1.7916110014915467 valid 1.8800547122955322\n",
      "EPOCH 486:\n",
      "  batch 50 loss: 1.762029995918274\n",
      "  batch 100 loss: 1.7748027014732362\n",
      "LOSS train 1.7748027014732362 valid 1.8813047409057617\n",
      "EPOCH 487:\n",
      "  batch 50 loss: 1.752971568107605\n",
      "  batch 100 loss: 1.7584805035591125\n",
      "LOSS train 1.7584805035591125 valid 1.9119330644607544\n",
      "EPOCH 488:\n",
      "  batch 50 loss: 1.7550847268104552\n",
      "  batch 100 loss: 1.7456353902816772\n",
      "LOSS train 1.7456353902816772 valid 1.9100860357284546\n",
      "EPOCH 489:\n",
      "  batch 50 loss: 1.7502787494659424\n",
      "  batch 100 loss: 1.7797971558570862\n",
      "LOSS train 1.7797971558570862 valid 1.8892914056777954\n",
      "EPOCH 490:\n",
      "  batch 50 loss: 1.748955376148224\n",
      "  batch 100 loss: 1.7684602642059326\n",
      "LOSS train 1.7684602642059326 valid 1.8860435485839844\n",
      "EPOCH 491:\n",
      "  batch 50 loss: 1.766137924194336\n",
      "  batch 100 loss: 1.7763729763031006\n",
      "LOSS train 1.7763729763031006 valid 1.8918417692184448\n",
      "EPOCH 492:\n",
      "  batch 50 loss: 1.7535672569274903\n",
      "  batch 100 loss: 1.762381796836853\n",
      "LOSS train 1.762381796836853 valid 1.8804129362106323\n",
      "EPOCH 493:\n",
      "  batch 50 loss: 1.7901543164253235\n",
      "  batch 100 loss: 1.750694694519043\n",
      "LOSS train 1.750694694519043 valid 1.8793741464614868\n",
      "EPOCH 494:\n",
      "  batch 50 loss: 1.713574948310852\n",
      "  batch 100 loss: 1.8055075407028198\n",
      "LOSS train 1.8055075407028198 valid 1.8730847835540771\n",
      "EPOCH 495:\n",
      "  batch 50 loss: 1.7647835540771484\n",
      "  batch 100 loss: 1.766999776363373\n",
      "LOSS train 1.766999776363373 valid 1.8780627250671387\n",
      "EPOCH 496:\n",
      "  batch 50 loss: 1.7649942636489868\n",
      "  batch 100 loss: 1.7696018505096436\n",
      "LOSS train 1.7696018505096436 valid 1.8743641376495361\n",
      "EPOCH 497:\n",
      "  batch 50 loss: 1.7498578691482545\n",
      "  batch 100 loss: 1.7485352492332458\n",
      "LOSS train 1.7485352492332458 valid 1.8860960006713867\n",
      "EPOCH 498:\n",
      "  batch 50 loss: 1.7330738139152526\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 49\u001B[0m\n\u001B[1;32m     46\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     47\u001B[0m opt\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m---> 49\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m50\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m49\u001B[39m:\n\u001B[1;32m     51\u001B[0m     last_loss \u001B[38;5;241m=\u001B[39m running_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m50\u001B[39m \u001B[38;5;66;03m# loss per batch\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models import UTime\n",
    "\n",
    "cols = ['Inclination (deg)', 'Longitude (deg)']\n",
    "trn_data = SPLID(train_datalist, ground_truth, cols)\n",
    "tst_data = SPLID(test_datalist, ground_truth, cols, classes=trn_data.le_type.classes_)\n",
    "\n",
    "trn_loader = data.DataLoader(trn_data, shuffle=True, batch_size=10)\n",
    "tst_loader = data.DataLoader(tst_data, shuffle=True, batch_size=10)\n",
    "\n",
    "lr = 5e-6\n",
    "n_epochs = 1000\n",
    "best_tst_loss = 1_000_000.\n",
    "\n",
    "model = UTime(len(trn_data.le_type.classes_))\n",
    "model = model.cuda()\n",
    "criterion = nn.NLLLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "print('Start model training')\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/splid_trainer_{}'.format(timestamp))\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    print('EPOCH {}:'.format(epoch))\n",
    "    \n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    model.train(True)\n",
    "    \n",
    "    for i, (x_batch, y_batch) in enumerate(trn_loader):\n",
    "        \n",
    "        x_batch = x_batch.cuda()\n",
    "        y_batch = y_batch.cuda()\n",
    "        # sched.step()\n",
    "        opt.zero_grad()\n",
    "        out = model(x_batch)\n",
    "        loss = criterion(out, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:\n",
    "            last_loss = running_loss / 50 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch * len(trn_loader) + i + 1\n",
    "            writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "    \n",
    "    running_tst_loss = 0.0\n",
    "    model.eval()\n",
    "    \n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, (x_batch, y_batch) in enumerate(tst_loader):\n",
    "            x_batch = x_batch.cuda()\n",
    "            y_batch = y_batch.cuda()\n",
    "            outputs = model(x_batch)\n",
    "            tst_loss = criterion(outputs, y_batch)\n",
    "            running_tst_loss += tst_loss\n",
    "    \n",
    "    avg_tst_loss = running_tst_loss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(last_loss, avg_tst_loss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : last_loss, 'Validation' : avg_tst_loss },\n",
    "                    epoch)\n",
    "    writer.flush()\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_tst_loss < best_tst_loss:\n",
    "        best_tst_loss = avg_tst_loss\n",
    "        model_path = 'model_{}.pth'.format(timestamp)\n",
    "        torch.save(model.state_dict(), 'saved_models/' + model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `NodeDetectionEvaluator` class in the evaluation module allows not only to\n",
    "compute the general score for a given dataset, but get evaluations per object, and\n",
    "even plots that show how the predictions look like in a timeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
