{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fastcore.basics import Path, AttrDict\n",
    "import utils\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# This is used to import the evaluation script, not needed for training\n",
    "import sys\n",
    "sys.path.append('../') \n",
    "import evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AttrDict(\n",
    "    challenge_data_dir = Path('~/Projects/splid-comp/dataset').expanduser(),\n",
    "    valid_ratio = 0.1,\n",
    "    lag_steps = 5,\n",
    "    tolerance= 6, # Default evaluation tolerance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of feature columns\n",
    "feature_cols = [\n",
    "    \"Eccentricity\",\n",
    "    \"Semimajor Axis (m)\",\n",
    "    \"Inclination (deg)\",\n",
    "    \"RAAN (deg)\",\n",
    "    \"Argument of Periapsis (deg)\",\n",
    "    \"True Anomaly (deg)\",\n",
    "    \"Latitude (deg)\",\n",
    "    \"Longitude (deg)\",\n",
    "    \"Altitude (m)\",\n",
    "    \"X (m)\",\n",
    "    \"Y (m)\",\n",
    "    \"Z (m)\",\n",
    "    \"Vx (m/s)\",\n",
    "    \"Vy (m/s)\",\n",
    "    \"Vz (m/s)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory paths\n",
    "train_data_dir = config.challenge_data_dir / \"train\"\n",
    "\n",
    "# Load the ground truth data\n",
    "ground_truth = pd.read_csv(config.challenge_data_dir / 'train_labels.csv')\n",
    "\n",
    "# Apply the function to the ground truth data\n",
    "data, updated_feature_cols = utils.tabularize_data(train_data_dir, feature_cols, \n",
    "                                          ground_truth, lag_steps=config.lag_steps)\n",
    "\n",
    "# For each ObjectID, show the first rows of the columns TimeIndex, ObjectID, EW, and NS\n",
    "data[['ObjectID', 'TimeIndex' , 'EW', 'NS']].groupby('ObjectID').head(2).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation set without mixing the ObjectIDs\n",
    "object_ids = data['ObjectID'].unique()\n",
    "train_ids, valid_ids = train_test_split(object_ids, \n",
    "                                        test_size=config.valid_ratio, \n",
    "                                        random_state=42)\n",
    "\n",
    "train_data = data[data['ObjectID'].isin(train_ids)].copy()\n",
    "valid_data = data[data['ObjectID'].isin(valid_ids)].copy()\n",
    "\n",
    "ground_truth_train = ground_truth[ground_truth['ObjectID'].isin(train_ids)].copy()\n",
    "ground_truth_valid = ground_truth[ground_truth['ObjectID'].isin(valid_ids)].copy()\n",
    "\n",
    "# Count the number of objects in the training and validation sets\n",
    "print('Number of objects in the training set:', len(train_data['ObjectID'].unique()))\n",
    "print('Number of objects in the validation set:', len(valid_data['ObjectID'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will make sure that there every label, both in the direction EW and NS,\n",
    "is present both in the training and validation partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values of EW and NS in train and test data\n",
    "train_EW = set(train_data['EW'].unique())\n",
    "train_NS = set(train_data['NS'].unique())\n",
    "valid_EW = set(valid_data['EW'].unique())\n",
    "valid_NS = set(valid_data['NS'].unique())\n",
    "\n",
    "# Get the values of EW and NS that are in test data but not in train data\n",
    "missing_EW = valid_EW.difference(train_EW)\n",
    "missing_NS = valid_NS.difference(train_NS)\n",
    "\n",
    "# Check if all the values in EW are also present in NS\n",
    "if not set(train_data['EW'].unique()).issubset(set(train_data['NS'].unique())):\n",
    "    # Get the values of EW that are not present in NS\n",
    "    missing_EW_NS = set(train_data['EW'].unique()).difference(\n",
    "        set(train_data['NS'].unique())\n",
    "    )\n",
    "else:\n",
    "    missing_EW_NS = None\n",
    "\n",
    "# Print the missing values of EW and NS\n",
    "print(\"Missing values of EW in test data:\", missing_EW)\n",
    "print(\"Missing values of NS in test data:\", missing_NS)\n",
    "print(\"Values of EW not present in NS:\", missing_EW_NS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical data to numerical data\n",
    "le_EW = LabelEncoder()\n",
    "le_NS = LabelEncoder()\n",
    "\n",
    "# Encode the 'EW' and 'NS' columns\n",
    "train_data['EW_encoded'] = le_EW.fit_transform(train_data['EW'])\n",
    "train_data['NS_encoded'] = le_NS.fit_transform(train_data['NS'])\n",
    "\n",
    "# Define the Random Forest model for EW\n",
    "model_EW = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# Fit the model to the training data for EW\n",
    "model_EW.fit(train_data[updated_feature_cols], train_data['EW_encoded'])\n",
    "\n",
    "# Define the Random Forest model for NS\n",
    "model_NS = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# Fit the model to the training data for NS\n",
    "model_NS.fit(train_data[updated_feature_cols], train_data['NS_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the training data for EW\n",
    "train_data['Predicted_EW'] = le_EW.inverse_transform(\n",
    "    model_EW.predict(train_data[updated_feature_cols])\n",
    ")\n",
    "\n",
    "# Make predictions on the validation data for NS\n",
    "train_data['Predicted_NS'] = le_NS.inverse_transform(\n",
    "    model_NS.predict(train_data[updated_feature_cols])\n",
    ")\n",
    "\n",
    "# Print the first few rows of the test data with predictions for both EW and NS\n",
    "train_data[['TimeIndex', 'ObjectID', 'EW', \n",
    "            'Predicted_EW', 'NS', 'Predicted_NS']].groupby('ObjectID').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.valid_ratio > 0:\n",
    "    # Make predictions on the validation data for EW\n",
    "    valid_data['Predicted_EW'] = le_EW.inverse_transform(\n",
    "        model_EW.predict(valid_data[updated_feature_cols])\n",
    "    )\n",
    "\n",
    "    # Make predictions on the validation data for NS\n",
    "    valid_data['Predicted_NS'] = le_NS.inverse_transform(\n",
    "        model_NS.predict(valid_data[updated_feature_cols])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `NodeDetectionEvaluator` class in the evaluation module allows not only to\n",
    "compute the general score for a given dataset, but get evaluations per object, and\n",
    "even plots that show how the predictions look like in a timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = utils.convert_classifier_output(train_data)\n",
    "evaluator = evaluation.NodeDetectionEvaluator(ground_truth_train, train_results, \n",
    "                                              tolerance=config.tolerance)\n",
    "precision, recall, f2, rmse = evaluator.score()\n",
    "print(f'Precision for the train set: {precision:.2f}')\n",
    "print(f'Recall for the train set: {recall:.2f}')\n",
    "print(f'F2 for the train set: {f2:.2f}')\n",
    "print(f'RMSE for the train set: {rmse:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the evaluation timeline for a random ObjectID from the training set\n",
    "evaluator.plot(np.random.choice(train_data['ObjectID'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the Object IDs in the training set and call the evaluation\n",
    "# function for each object and aggregate the results\n",
    "total_tp = 0\n",
    "total_fp = 0\n",
    "total_fn = 0\n",
    "for oid in train_data['ObjectID'].unique():\n",
    "    tp, fp, fn, gt_object, p_object = evaluator.evaluate(oid)\n",
    "    total_tp += tp\n",
    "    total_fp += fp\n",
    "    total_fn += fn\n",
    "\n",
    "print(f'Total true positives: {total_tp}')\n",
    "print(f'Total false positives: {total_fp}')\n",
    "print(f'Total false negatives: {total_fn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.valid_ratio > 0:\n",
    "    valid_results = utils.convert_classifier_output(valid_data)\n",
    "    evaluator = evaluation.NodeDetectionEvaluator(ground_truth_valid, \n",
    "                                                  valid_results,\n",
    "                                                  tolerance=config.tolerance)\n",
    "precision, recall, f2, rmse = evaluator.score()\n",
    "print(f'Precision for the validation set: {precision:.2f}')\n",
    "print(f'Recall for the validation set: {recall:.2f}')\n",
    "print(f'F2 for the validation set: {f2:.2f}')\n",
    "print(f'RMSE for the validation set: {rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the evaluation timeline for a random ObjectID from the training set\n",
    "evaluator.plot(np.random.choice(valid_data['ObjectID'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained random forest models (and label encoders) to disk\n",
    "# Create the folder trained_model if it doesn't exist\n",
    "Path('trained_model').mkdir(exist_ok=True)\n",
    "pickle.dump(model_EW, open('trained_model/model_EW.pkl', 'wb'))\n",
    "pickle.dump(model_NS, open('trained_model/model_NS.pkl', 'wb'))\n",
    "pickle.dump(le_EW, open('trained_model/le_EW.pkl', 'wb'))\n",
    "pickle.dump(le_NS, open('trained_model/le_NS.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
