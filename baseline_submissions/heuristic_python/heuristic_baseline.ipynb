{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece574c0-ef5f-4775-972a-91b55385d1eb",
   "metadata": {},
   "source": [
    "# Heuristic baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622d40ce-591a-42b6-a76d-fa1927da6575",
   "metadata": {},
   "source": [
    "This jupyter notebook will guide the reader on implementing the Satellite Node Identification and Classification Tool (SNICT) as a heuristic method to identify and classify satellite nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2034595b-199a-40ed-b971-737d3c23ca33",
   "metadata": {},
   "source": [
    "## We will first begin by importing the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0bd7a99-5c92-4bce-8bf9-c43aa7c48757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T00:51:58.888494869Z",
     "start_time": "2024-01-15T00:51:58.681964276Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from node import Node\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will set up the input and output paths"
   ],
   "id": "7c77569c8cbd68fc"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T00:52:55.445546964Z",
     "start_time": "2024-01-15T00:52:55.402867446Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path('~/Projects/splid-devkit/dataset/train').expanduser()\n",
    "OUTPUT_FILEPATH = Path('~/Projects/splid-devkit/dataset/submission/submission.csv').expanduser()"
   ],
   "id": "ab6fdbfaec221618"
  },
  {
   "cell_type": "markdown",
   "id": "3c91839e-85bf-4fde-b7c9-b2314f66b781",
   "metadata": {},
   "source": [
    "For this work, we will be using custom classes to store node information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8f3f8a7-2a88-4753-b4b2-eb97587b7f6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T00:52:58.609241229Z",
     "start_time": "2024-01-15T00:52:58.605621798Z"
    }
   },
   "outputs": [],
   "source": [
    "class index_dict:\n",
    "    def __init__(self):\n",
    "        self.times = self.IDADIK()\n",
    "        self.indices = []\n",
    "        self.AD_dex =[]\n",
    "        self.modes = self.mode()\n",
    "\n",
    "    class IDADIK:\n",
    "        def __init__(self):\n",
    "            self.ID = []\n",
    "            self.AD = []\n",
    "            self.IK = []\n",
    "    \n",
    "    class mode:\n",
    "        def __init__(self):\n",
    "            self.SK = []\n",
    "            self.end = []\n",
    "\n",
    "detected = index_dict()\n",
    "filtered = index_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90f7377-08d3-43a4-8419-c89aba28371b",
   "metadata": {},
   "source": [
    "We will then search for the training data in the `dataset/train` folder and return a sorted list for the filepaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d94a281a-f556-4924-901b-f14d4e8f51ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T00:53:41.772523132Z",
     "start_time": "2024-01-15T00:53:41.728648164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../dataset/train/1.csv', '../../dataset/train/2.csv', '../../dataset/train/3.csv', '../../dataset/train/4.csv', '../../dataset/train/5.csv', '../../dataset/train/6.csv', '../../dataset/train/7.csv', '../../dataset/train/8.csv', '../../dataset/train/9.csv', '../../dataset/train/10.csv', '../../dataset/train/11.csv', '../../dataset/train/12.csv', '../../dataset/train/13.csv', '../../dataset/train/14.csv', '../../dataset/train/15.csv', '../../dataset/train/16.csv', '../../dataset/train/17.csv', '../../dataset/train/18.csv', '../../dataset/train/19.csv', '../../dataset/train/20.csv', '../../dataset/train/21.csv', '../../dataset/train/22.csv', '../../dataset/train/23.csv', '../../dataset/train/24.csv', '../../dataset/train/25.csv', '../../dataset/train/26.csv', '../../dataset/train/27.csv', '../../dataset/train/28.csv', '../../dataset/train/29.csv', '../../dataset/train/30.csv', '../../dataset/train/31.csv', '../../dataset/train/32.csv', '../../dataset/train/33.csv', '../../dataset/train/34.csv', '../../dataset/train/35.csv', '../../dataset/train/36.csv', '../../dataset/train/37.csv', '../../dataset/train/38.csv', '../../dataset/train/39.csv', '../../dataset/train/40.csv', '../../dataset/train/41.csv', '../../dataset/train/42.csv', '../../dataset/train/43.csv', '../../dataset/train/44.csv', '../../dataset/train/45.csv', '../../dataset/train/46.csv', '../../dataset/train/47.csv', '../../dataset/train/48.csv', '../../dataset/train/49.csv', '../../dataset/train/50.csv', '../../dataset/train/51.csv', '../../dataset/train/52.csv', '../../dataset/train/53.csv', '../../dataset/train/54.csv', '../../dataset/train/55.csv', '../../dataset/train/56.csv', '../../dataset/train/57.csv', '../../dataset/train/58.csv', '../../dataset/train/59.csv', '../../dataset/train/60.csv', '../../dataset/train/61.csv', '../../dataset/train/62.csv', '../../dataset/train/63.csv', '../../dataset/train/64.csv', '../../dataset/train/65.csv', '../../dataset/train/66.csv', '../../dataset/train/67.csv', '../../dataset/train/68.csv', '../../dataset/train/69.csv', '../../dataset/train/70.csv', '../../dataset/train/71.csv', '../../dataset/train/72.csv', '../../dataset/train/73.csv', '../../dataset/train/74.csv', '../../dataset/train/75.csv', '../../dataset/train/76.csv', '../../dataset/train/77.csv', '../../dataset/train/78.csv', '../../dataset/train/79.csv', '../../dataset/train/80.csv', '../../dataset/train/81.csv', '../../dataset/train/82.csv', '../../dataset/train/83.csv', '../../dataset/train/84.csv', '../../dataset/train/85.csv', '../../dataset/train/86.csv', '../../dataset/train/87.csv', '../../dataset/train/88.csv', '../../dataset/train/89.csv', '../../dataset/train/90.csv', '../../dataset/train/91.csv', '../../dataset/train/92.csv', '../../dataset/train/93.csv', '../../dataset/train/94.csv', '../../dataset/train/95.csv', '../../dataset/train/96.csv', '../../dataset/train/97.csv', '../../dataset/train/98.csv', '../../dataset/train/99.csv', '../../dataset/train/100.csv', '../../dataset/train/101.csv', '../../dataset/train/102.csv', '../../dataset/train/103.csv', '../../dataset/train/104.csv', '../../dataset/train/105.csv', '../../dataset/train/106.csv', '../../dataset/train/107.csv', '../../dataset/train/108.csv', '../../dataset/train/109.csv', '../../dataset/train/110.csv', '../../dataset/train/111.csv', '../../dataset/train/112.csv', '../../dataset/train/113.csv', '../../dataset/train/114.csv', '../../dataset/train/115.csv', '../../dataset/train/116.csv', '../../dataset/train/117.csv', '../../dataset/train/118.csv', '../../dataset/train/119.csv', '../../dataset/train/120.csv', '../../dataset/train/121.csv', '../../dataset/train/122.csv', '../../dataset/train/123.csv', '../../dataset/train/124.csv', '../../dataset/train/125.csv', '../../dataset/train/126.csv', '../../dataset/train/127.csv', '../../dataset/train/128.csv', '../../dataset/train/129.csv', '../../dataset/train/130.csv', '../../dataset/train/131.csv', '../../dataset/train/132.csv', '../../dataset/train/133.csv', '../../dataset/train/134.csv', '../../dataset/train/135.csv', '../../dataset/train/136.csv', '../../dataset/train/137.csv', '../../dataset/train/138.csv', '../../dataset/train/139.csv', '../../dataset/train/140.csv', '../../dataset/train/141.csv', '../../dataset/train/142.csv', '../../dataset/train/143.csv', '../../dataset/train/144.csv', '../../dataset/train/145.csv', '../../dataset/train/146.csv', '../../dataset/train/147.csv', '../../dataset/train/148.csv', '../../dataset/train/149.csv', '../../dataset/train/150.csv', '../../dataset/train/151.csv', '../../dataset/train/152.csv', '../../dataset/train/153.csv', '../../dataset/train/154.csv', '../../dataset/train/155.csv', '../../dataset/train/156.csv', '../../dataset/train/157.csv', '../../dataset/train/158.csv', '../../dataset/train/159.csv', '../../dataset/train/160.csv', '../../dataset/train/161.csv', '../../dataset/train/162.csv', '../../dataset/train/163.csv', '../../dataset/train/164.csv', '../../dataset/train/165.csv', '../../dataset/train/166.csv', '../../dataset/train/167.csv', '../../dataset/train/168.csv', '../../dataset/train/169.csv', '../../dataset/train/170.csv', '../../dataset/train/171.csv', '../../dataset/train/172.csv', '../../dataset/train/173.csv', '../../dataset/train/174.csv', '../../dataset/train/175.csv', '../../dataset/train/176.csv', '../../dataset/train/177.csv', '../../dataset/train/178.csv', '../../dataset/train/179.csv', '../../dataset/train/180.csv', '../../dataset/train/181.csv', '../../dataset/train/182.csv', '../../dataset/train/183.csv', '../../dataset/train/184.csv', '../../dataset/train/185.csv', '../../dataset/train/186.csv', '../../dataset/train/187.csv', '../../dataset/train/188.csv', '../../dataset/train/189.csv', '../../dataset/train/190.csv', '../../dataset/train/191.csv', '../../dataset/train/192.csv', '../../dataset/train/193.csv', '../../dataset/train/194.csv', '../../dataset/train/195.csv', '../../dataset/train/196.csv', '../../dataset/train/197.csv', '../../dataset/train/198.csv', '../../dataset/train/199.csv', '../../dataset/train/200.csv', '../../dataset/train/201.csv', '../../dataset/train/202.csv', '../../dataset/train/203.csv', '../../dataset/train/204.csv', '../../dataset/train/205.csv', '../../dataset/train/206.csv', '../../dataset/train/207.csv', '../../dataset/train/208.csv', '../../dataset/train/209.csv', '../../dataset/train/210.csv', '../../dataset/train/211.csv', '../../dataset/train/212.csv', '../../dataset/train/213.csv', '../../dataset/train/214.csv', '../../dataset/train/215.csv', '../../dataset/train/216.csv', '../../dataset/train/217.csv', '../../dataset/train/218.csv', '../../dataset/train/219.csv', '../../dataset/train/220.csv', '../../dataset/train/221.csv', '../../dataset/train/222.csv', '../../dataset/train/223.csv', '../../dataset/train/224.csv', '../../dataset/train/225.csv', '../../dataset/train/226.csv', '../../dataset/train/227.csv', '../../dataset/train/228.csv', '../../dataset/train/229.csv', '../../dataset/train/230.csv', '../../dataset/train/231.csv', '../../dataset/train/232.csv', '../../dataset/train/233.csv', '../../dataset/train/234.csv', '../../dataset/train/235.csv', '../../dataset/train/236.csv', '../../dataset/train/237.csv', '../../dataset/train/238.csv', '../../dataset/train/239.csv', '../../dataset/train/240.csv', '../../dataset/train/241.csv', '../../dataset/train/242.csv', '../../dataset/train/243.csv', '../../dataset/train/244.csv', '../../dataset/train/245.csv', '../../dataset/train/246.csv', '../../dataset/train/247.csv', '../../dataset/train/248.csv', '../../dataset/train/249.csv', '../../dataset/train/250.csv']\n"
     ]
    }
   ],
   "source": [
    "datalist = []\n",
    "\n",
    "# Searching for training data within the dataset folder\n",
    "for file in os.listdir(DATA_DIR):\n",
    "    if file.endswith(\".csv\"):\n",
    "        datalist.append(os.path.join(\"../../dataset/train/\", file))\n",
    "\n",
    "# Sort the training data and labels\n",
    "datalist = sorted(datalist, key=lambda i: int(os.path.splitext(os.path.basename(i))[0]))\n",
    "\n",
    "# Print the sorted filepath to the training data\n",
    "print(datalist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bff4fc-d4ac-4665-b8b9-8cc72a700caa",
   "metadata": {},
   "source": [
    "Next, we will be reading the training data as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T00:54:08.704362774Z",
     "start_time": "2024-01-15T00:54:05.349732922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 544860 entries, 0 to 544859\n",
      "Data columns (total 17 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   Eccentricity                 544860 non-null  float64\n",
      " 1   Semimajor Axis (m)           544860 non-null  float64\n",
      " 2   Inclination (deg)            544860 non-null  float64\n",
      " 3   RAAN (deg)                   544860 non-null  float64\n",
      " 4   Argument of Periapsis (deg)  544860 non-null  float64\n",
      " 5   True Anomaly (deg)           544860 non-null  float64\n",
      " 6   Latitude (deg)               544860 non-null  float64\n",
      " 7   Longitude (deg)              544860 non-null  float64\n",
      " 8   Altitude (m)                 544860 non-null  float64\n",
      " 9   X (m)                        544860 non-null  float64\n",
      " 10  Y (m)                        544860 non-null  float64\n",
      " 11  Z (m)                        544860 non-null  float64\n",
      " 12  Vx (m/s)                     544860 non-null  float64\n",
      " 13  Vy (m/s)                     544860 non-null  float64\n",
      " 14  Vz (m/s)                     544860 non-null  float64\n",
      " 15  ObjectID                     544860 non-null  int64  \n",
      " 16  TimeIndex                    544860 non-null  int64  \n",
      "dtypes: float64(15), int64(2)\n",
      "memory usage: 70.7 MB\n"
     ]
    }
   ],
   "source": [
    "data_paths = Path(DATA_DIR).glob('*.csv')\n",
    "# Check if test_data is empty\n",
    "if not data_paths:\n",
    "    raise ValueError(f'No csv files found in {DATA_DIR}')\n",
    "data = pd.DataFrame()\n",
    "for data_file in data_paths:\n",
    "    data_df = pd.read_csv(data_file)\n",
    "    data_df['ObjectID'] = int(data_file.stem)\n",
    "    data_df['TimeIndex'] = range(len(data_df))\n",
    "    data = pd.concat([data, data_df], ignore_index=True)\n",
    "data.info()"
   ],
   "id": "4c90ed8812de31e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e64bfcee-f900-41bb-b548-31d5e5255386",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T00:54:29.792207625Z",
     "start_time": "2024-01-15T00:54:29.747448643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the first training data. Note: Python starts counting from 0, instead of 1.\n",
    "idx_data = 102\n",
    "\n",
    "data_path = datalist[idx_data]\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc17be2-4c6b-4d6e-b0b5-0f685eec1faf",
   "metadata": {},
   "source": [
    "The SNICT uses longitudinal and inclination information to detect and characterize the changes in a satellite's behavioral mode. The longitudinal an inclination information are extracted from the pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6067296a-b7b8-4b8b-84c7-dd037233889d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T00:54:34.397351661Z",
     "start_time": "2024-01-15T00:54:34.395376033Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the objectID from the filename\n",
    "satcat=data_path[-7:-4]\n",
    "\n",
    "# Extracting longitudinal and inclination information from the pandas dataframe\n",
    "longitudes = data[\"Longitude (deg)\"]\n",
    "inclinations = data[\"Inclination (deg)\"]\n",
    "\n",
    "# Arbitrary assign start time and end time. Note: SNICT was developed to read in time-stamped data, \n",
    "# however, our training data are not label with a time stamp, hence an arbitrary start and end time\n",
    "# are selected\n",
    "starttime = datetime.fromisoformat(\"2023-01-01 00:00:00+00:00\")\n",
    "endtime = datetime.fromisoformat(\"2023-07-01 00:00:00+00:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a665d4b6-8fd0-451e-a7d3-7ea813e82328",
   "metadata": {},
   "source": [
    "## Detection of East-West Pattern-of-Life Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab967d-b2b9-4b6a-a9af-12a654db5c66",
   "metadata": {},
   "source": [
    "Identify possible East-West PoL nodes by analyzing the changes in longitudinal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8acd5827-535d-49dc-bf94-79d1adaeb9c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T00:54:49.751399374Z",
     "start_time": "2024-01-15T00:54:49.619122117Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get std for longitude over a 24 hours window\n",
    "lon_std = []\n",
    "nodes = []\n",
    "steps_per_day = 12\n",
    "lon_was_baseline = True\n",
    "lon_baseline = 0.03\n",
    "\n",
    "for i in range(len(data[\"Longitude (deg)\"])):\n",
    "    if i <= steps_per_day:\n",
    "        lon_std.append(np.std(data[\"Longitude (deg)\"][0:steps_per_day]))\n",
    "    else:\n",
    "        lon_std.append(np.std(data[\"Longitude (deg)\"][i-steps_per_day:i]))\n",
    "\n",
    "ssEW = Node(satcat=satcat,\n",
    "            t=starttime,\n",
    "            index=0,\n",
    "            ntype=\"SS\",\n",
    "            signal=\"EW\")\n",
    "es = Node(satcat=satcat,\n",
    "            t=endtime,\n",
    "            index=len(data[\"Longitude (deg)\"])-1,\n",
    "            ntype=\"ES\",\n",
    "            signal=\"ES\",\n",
    "            mtype=\"ES\")\n",
    "\n",
    "# Run LS detection\n",
    "for i in range(steps_per_day+1,len(lon_std)-steps_per_day):             # if at least 1 day has elapsed since t0\n",
    "    max_lon_std_24h = np.max(lon_std[i-steps_per_day:i])\n",
    "    min_lon_std_24h = np.min(lon_std[i-steps_per_day:i])\n",
    "    A = np.abs(max_lon_std_24h-min_lon_std_24h)/2\n",
    "    th_ = 0.95*A\n",
    "\n",
    "    # ID detection\n",
    "    if (lon_std[i]>lon_baseline) & lon_was_baseline:                    # if sd is elevated & last sd was at baseline\n",
    "        before = np.mean(data[\"Longitude (deg)\"][i-steps_per_day:i])    # mean of previous day's longitudes\n",
    "        after = np.mean(data[\"Longitude (deg)\"][i:i+steps_per_day])     # mean of next day's longitudes\n",
    "        # if not temporary noise, then real ID\n",
    "        if np.abs(before-after)>0.3:                                    # if means are different\n",
    "            lon_was_baseline = False                                    # the sd is not yet back at baseline\n",
    "            index = i\n",
    "            if i < steps_per_day+2:\n",
    "                ssEW.mtype = \"NK\"\n",
    "            else:\n",
    "                detected.times.ID.append(starttime+timedelta(hours=i*2))\n",
    "    # IK detection\n",
    "    elif (lon_std[i]<=lon_baseline) & (not lon_was_baseline):           # elif sd is not elevated and drift has already been initialized\n",
    "        drift_ended = True                                              # toggle end-of-drift boolean \n",
    "        for j in range(steps_per_day):                                  # for the next day, check...\n",
    "            if np.abs(data[\"Longitude (deg)\"][i]-data[\"Longitude (deg)\"][i+j])>0.3:       # if the longitude changes from the current value\n",
    "                drift_ended = False                                     # the drift has not ended\n",
    "        if drift_ended:                                                 # if the drift has ended\n",
    "            lon_was_baseline = True                                     # the sd is back to baseline\n",
    "            detected.times.IK.append(starttime+timedelta(hours=i*2))    # save tnow as end-of-drift\n",
    "            detected.indices.append([index,i])                          # save indices for t-start & t-end\n",
    "\n",
    "    # Last step\n",
    "    elif (i == (len(lon_std)-steps_per_day-1))\\\n",
    "        &(not lon_was_baseline):\n",
    "        detected.times.IK.append(starttime+timedelta(hours=i*2))\n",
    "        detected.indices.append([index,i])\n",
    "    \n",
    "    # AD detection\n",
    "    elif ((lon_std[i]-max_lon_std_24h>th_) or (min_lon_std_24h-lon_std[i]>th_)) & (not lon_was_baseline):          # elif sd is elevated and drift has already been initialized\n",
    "        if i >= steps_per_day+3:\n",
    "            detected.times.AD.append(starttime+timedelta(hours=i*2))\n",
    "            detected.AD_dex.append(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81afb5ba-c395-449e-bd05-cd2577d2a0f0",
   "metadata": {},
   "source": [
    "Next, we will filter the East-West nodes and merge nearby nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6368a047-8d57-4941-96de-c719d5b822be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T00:55:24.432340332Z",
     "start_time": "2024-01-15T00:55:24.386023175Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_node(n):\n",
    "    nodes[len(nodes)-1].char_mode(\n",
    "        next_index = n.index,\n",
    "        lons = longitudes,\n",
    "        incs = inclinations\n",
    "    )\n",
    "    if n.type == \"AD\":\n",
    "        nodes[len(nodes)-1].mtype = \"NK\"\n",
    "\n",
    "    if (nodes[len(nodes)-1].mtype != \"NK\"):\n",
    "        filtered.indices.append([nodes[len(nodes)-1].index,n.index])\n",
    "        filtered.modes.SK.append(nodes[len(nodes)-1].mtype)\n",
    "        stop_NS = True if n.type == \"ID\" else False\n",
    "        filtered.modes.end.append(stop_NS)\n",
    "    nodes.append(n)\n",
    "\n",
    "toggle = True\n",
    "nodes.append(ssEW)\n",
    "if len(detected.times.IK) == 1:\n",
    "    if len(detected.times.ID) == 1:\n",
    "        filtered.times.ID.append(detected.times.ID[0])                                  # keep the current ID\n",
    "        ID = Node(satcat,\n",
    "                detected.times.ID[0],\n",
    "                index=detected.indices[0][0],\n",
    "                ntype='ID',\n",
    "                lon=longitudes[detected.indices[0][0]],\n",
    "                signal=\"EW\")\n",
    "        add_node(ID)\n",
    "    filtered.times.IK.append(detected.times.IK[0]) \n",
    "    IK = Node(satcat,\n",
    "            detected.times.IK[0],\n",
    "            index=detected.indices[0][1],\n",
    "            ntype='IK',\n",
    "            lon=longitudes[detected.indices[0][1]],\n",
    "            signal=\"EW\")\n",
    "    apnd = True\n",
    "    if len(detected.times.AD) == 1:\n",
    "        AD = Node(satcat,\n",
    "                  detected.times.AD[0],\n",
    "                  index=detected.AD_dex[0],\n",
    "                  ntype=\"AD\",\n",
    "                  signal=\"EW\")\n",
    "        add_node(AD)\n",
    "    elif len(detected.times.AD) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        for j in range(len(detected.times.AD)):\n",
    "            ad = Node(satcat,\n",
    "                  detected.times.AD[j],\n",
    "                  index=detected.AD_dex[j],\n",
    "                  ntype=\"AD\",\n",
    "                  signal=\"EW\")\n",
    "            ad_next = Node(satcat,\n",
    "                  detected.times.AD[j+1],\n",
    "                  index=detected.AD_dex[j+1],\n",
    "                  ntype=\"AD\",\n",
    "                  signal=\"EW\") \\\n",
    "                if j < (len(detected.times.AD)-1) else None\n",
    "            if (ad.t>starttime+timedelta(hours=detected.indices[0][0]*2))&(ad.t<IK.t):\n",
    "                if apnd & (ad_next is not None):\n",
    "                    if ((ad_next.t-ad.t)>timedelta(hours=24)):\n",
    "                        add_node(ad)\n",
    "                    else:\n",
    "                        add_node(ad)\n",
    "                        apnd = False\n",
    "                elif apnd & (ad_next is None):\n",
    "                    add_node(ad)\n",
    "                elif (not apnd) & (ad_next is not None):\n",
    "                    if ((ad_next.t-ad.t)>timedelta(hours=24)):\n",
    "                        apnd = True\n",
    "    if detected.indices[0][1] != (len(lon_std)-steps_per_day-1):\n",
    "        add_node(IK)    \n",
    "\n",
    "for i in range(len(detected.times.IK)-1):                                 # for each longitudinal shift detection\n",
    "    if toggle:                                                            # if the last ID was not discarded\n",
    "        if ((starttime+timedelta(hours=detected.indices[i+1][0]*2)-detected.times.IK[i])>timedelta(hours=36)):# if the time between the current IK & next ID is longer than 48 hours\n",
    "            filtered.times.ID.append(detected.times.ID[i])                # keep the current ID\n",
    "            filtered.times.IK.append(detected.times.IK[i])                # keep the current IK\n",
    "            ID = Node(satcat,\n",
    "                    detected.times.ID[i],\n",
    "                    index=detected.indices[i][0],\n",
    "                    ntype='ID',\n",
    "                    lon=longitudes[detected.indices[i][0]],\n",
    "                    signal=\"EW\")\n",
    "            add_node(ID)\n",
    "            IK = Node(satcat,\n",
    "                    detected.times.IK[i],\n",
    "                    index=detected.indices[i][1],\n",
    "                    ntype='IK',\n",
    "                    lon=longitudes[detected.indices[i][1]],\n",
    "                    signal=\"EW\")\n",
    "            apnd = True\n",
    "            for j in range(len(detected.times.AD)):\n",
    "                ad = Node(satcat,\n",
    "                  detected.times.AD[j],\n",
    "                  index=detected.AD_dex[j],\n",
    "                  ntype=\"AD\",\n",
    "                  signal=\"EW\")\n",
    "                ad_next = Node(satcat,\n",
    "                  detected.times.AD[j+1],\n",
    "                  index=detected.AD_dex[j+1],\n",
    "                  ntype=\"AD\",\n",
    "                  signal=\"EW\") \\\n",
    "                    if j < (len(detected.times.AD)-1) else None\n",
    "                if (ad.t>ID.t)&(ad.t<IK.t):\n",
    "                    if apnd & (ad_next is not None):\n",
    "                        if ((ad_next.t-ad.t)>timedelta(hours=24)):\n",
    "                            add_node(ad)\n",
    "                        else:\n",
    "                            add_node(ad)\n",
    "                            apnd = False\n",
    "                    elif apnd & (ad_next is None):\n",
    "                        add_node(ad)\n",
    "                    elif (not apnd) & (ad_next is not None):\n",
    "                        if ((ad_next.t-ad.t)>timedelta(hours=24)):\n",
    "                            apnd = True\n",
    "            if detected.indices[0][1] != (\n",
    "                len(lon_std)-steps_per_day-1):\n",
    "                add_node(IK)    \n",
    "            if i == len(detected.times.IK)-2:                             # if the next drift is the last drift\n",
    "                filtered.times.ID.append(starttime+timedelta(hours=detected.indices[i+1][0]*2))                    # keep the next ID\n",
    "                ID = Node(satcat,\n",
    "                        starttime+timedelta(hours=detected.indices[i+1][0]*2),\n",
    "                        index=detected.indices[i+1][0],\n",
    "                        ntype='ID',\n",
    "                        lon=longitudes[detected.indices[i+1][0]],\n",
    "                        signal=\"EW\")\n",
    "                add_node(ID)\n",
    "                IK = Node(satcat,\n",
    "                        detected.times.IK[i+1],\n",
    "                        index=detected.indices[i+1][1],\n",
    "                        ntype='IK',\n",
    "                        lon=longitudes[detected.indices[i+1][1]],\n",
    "                        signal=\"EW\")\n",
    "                apnd = True\n",
    "                for j in range(len(detected.times.AD)):\n",
    "                    ad = Node(satcat,\n",
    "                        detected.times.AD[j],\n",
    "                        index=detected.AD_dex[j],\n",
    "                        ntype=\"AD\",\n",
    "                        signal=\"EW\")\n",
    "                    ad_next = Node(satcat,\n",
    "                        detected.times.AD[j+1],\n",
    "                        index=detected.AD_dex[j+1],\n",
    "                        ntype=\"AD\",\n",
    "                        signal=\"EW\") \\\n",
    "                        if j < (len(detected.times.AD)-1) else None\n",
    "                    if (ad.t>ID.t)&(ad.t<IK.t):\n",
    "                        if apnd & (ad_next is not None):\n",
    "                            if ((ad_next.t-ad.t)>timedelta(\n",
    "                                hours=24)):\n",
    "                                add_node(ad)\n",
    "                            else:\n",
    "                                add_node(ad)\n",
    "                                apnd = False\n",
    "                        elif apnd & (ad_next is None):\n",
    "                            add_node(ad)\n",
    "                        elif (not apnd) & (ad_next is not None):\n",
    "                            if ((ad_next.t-ad.t)>timedelta(\n",
    "                                hours=24)):\n",
    "                                apnd = True\n",
    "                if detected.indices[i][1] != (\n",
    "                    len(lon_std)-steps_per_day-1):\n",
    "                    filtered.times.IK.append(detected.times.IK[i+1])      # keep the next IK\n",
    "                    add_node(IK)\n",
    "        else:                                                             # if the next ID and the current IK are 48 hours apart or less\n",
    "            ID = Node(satcat,\n",
    "                    detected.times.ID[i],\n",
    "                    index=detected.indices[i][0],\n",
    "                    ntype='ID',\n",
    "                    lon=longitudes[detected.indices[i][0]],\n",
    "                    signal=\"EW\")                                          # keep the current ID\n",
    "            add_node(ID)\n",
    "            AD = Node(satcat,\n",
    "                    detected.times.IK[i],\n",
    "                    index=detected.indices[i][1],\n",
    "                    ntype='AD',\n",
    "                    lon=longitudes[detected.indices[i][1]],\n",
    "                    signal=\"EW\")                                          # change the current IK to an AD\n",
    "            IK = Node(satcat,\n",
    "                    detected.times.IK[i+1],\n",
    "                    index=detected.indices[i+1][1],\n",
    "                    ntype='IK',\n",
    "                    lon=longitudes[detected.indices[i+1][1]],\n",
    "                    signal=\"EW\")                                          # exchange the current IK for the next one\n",
    "            add_node(AD)\n",
    "            apnd = True\n",
    "            for j in range(len(detected.times.AD)):\n",
    "                ad = Node(satcat,\n",
    "                  detected.times.AD[j],\n",
    "                  index=detected.AD_dex[j],\n",
    "                  ntype=\"AD\",\n",
    "                  signal=\"EW\")\n",
    "                ad_next = Node(satcat,\n",
    "                  detected.times.AD[j+1],\n",
    "                  index=detected.AD_dex[j+1],\n",
    "                  ntype=\"AD\",\n",
    "                  signal=\"EW\") \\\n",
    "                    if j < (len(detected.times.AD)-1) else None\n",
    "                if (ad.t>ID.t)&(ad.t<IK.t):\n",
    "                    if apnd & (ad_next is not None):\n",
    "                        if ((ad_next.t-ad.t)>timedelta(hours=24)):\n",
    "                            add_node(ad)\n",
    "                        else:\n",
    "                            add_node(ad)\n",
    "                            apnd = False\n",
    "                    elif apnd & (ad_next is None):\n",
    "                        add_node(ad)\n",
    "                    elif (not apnd) & (ad_next is not None):\n",
    "                        if ((ad_next.t-ad.t)>timedelta(hours=24)):\n",
    "                            apnd = True\n",
    "            if detected.indices[0][1] != (\n",
    "                len(lon_std)-steps_per_day-1):\n",
    "                add_node(IK)    \n",
    "            filtered.times.ID.append(detected.times.ID[i])\n",
    "            filtered.times.AD.append(detected.times.IK[i])\n",
    "            filtered.times.IK.append(detected.times.IK[i+1])\n",
    "            toggle = False                                                # skip the redundant drift\n",
    "    else:\n",
    "        toggle = True\n",
    "add_node(es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3823ddfc-32a3-4c38-a539-e5576e9a19d0",
   "metadata": {},
   "source": [
    "## Detection of North-South Pattern-of-Life Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c60fd5-ad2a-43f7-8dbb-b47ff0ea8540",
   "metadata": {},
   "source": [
    "Identify possible North-South PoL nodes by analyzing the changes in inclination values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df246730-50fd-4af2-8498-f8ddf1f4f376",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T00:55:47.058913505Z",
     "start_time": "2024-01-15T00:55:47.012489829Z"
    }
   },
   "outputs": [],
   "source": [
    "ssNS = Node(\n",
    "        satcat=satcat,\n",
    "        t=starttime,\n",
    "        index=0,\n",
    "        ntype=\"SS\",\n",
    "        signal=\"NS\")\n",
    "for j in range(len(filtered.indices)):\n",
    "    indices = filtered.indices[j]\n",
    "    first = True if indices[0] == 0 else False\n",
    "    times = []\n",
    "    dexs = []\n",
    "    inc = inclinations[indices[0]:indices[1]].to_numpy()\n",
    "    t = np.arange(indices[0],indices[1])*2\n",
    "    rate = (steps_per_day/(indices[1]-indices[0]))*(np.max(inc)-np.min(inc))\n",
    "    XIPS_inc_per_day = 0.0005 #0.035/30\n",
    "    if (rate < XIPS_inc_per_day) and (indices[0] < steps_per_day) and (indices[1] > steps_per_day):\n",
    "        if filtered.modes.end[j]:\n",
    "            nodes.append(Node(\n",
    "                satcat=satcat,\n",
    "                t=starttime+timedelta(hours=indices[1]*2),\n",
    "                index=indices[1],\n",
    "                ntype=\"ID\",\n",
    "                signal=\"NS\",\n",
    "                mtype=\"NK\"\n",
    "            ))\n",
    "        ssNS.mtype = filtered.modes.SK[j]\n",
    "    elif (rate < XIPS_inc_per_day):\n",
    "        nodes.append(Node(\n",
    "            satcat=satcat,\n",
    "            t=times[0],\n",
    "            index=dexs[0],\n",
    "            ntype=\"IK\",\n",
    "            signal=\"NS\",\n",
    "            mtype=filtered.modes.SK[j]\n",
    "        ))\n",
    "        if filtered.modes.end[j]:\n",
    "            nodes.append(Node(\n",
    "                satcat=satcat,\n",
    "                t=starttime+timedelta(hours=indices[1]*2),\n",
    "                index=indices[1],\n",
    "                ntype=\"ID\",\n",
    "                signal=\"NS\",\n",
    "                mtype=\"NK\"\n",
    "            ))\n",
    "    else:\n",
    "        dt = [0.0]\n",
    "        for i in range(len(inc)-1):\n",
    "            dt.append((inc[i+1]-inc[i])/(2*60*60))\n",
    "        prev = 1.0\n",
    "        for i in range(len(dt)-1):\n",
    "            if np.abs(dt[i])> 5.5e-7:\n",
    "                times.append(starttime+timedelta(hours=float(t[i])))\n",
    "                dexs.append(i+indices[0])\n",
    "                if (np.abs(np.mean(inc[0:i])-np.mean(inc[i:len(inc)]))/np.std(inc[0:i]))/prev < 1.0:\n",
    "                    if first and len(times)==2:\n",
    "                        ssNS.mtype = filtered.modes.SK[0]\n",
    "                        first = False\n",
    "                elif len(times)==2:\n",
    "                    first = False\n",
    "                prev = np.abs(np.mean(inc[0:i])-np.mean(inc[i:len(inc)]))/np.std(inc[0:i])\n",
    "                \n",
    "        if len(times)>0:\n",
    "            nodes.append(Node(\n",
    "                satcat=satcat,\n",
    "                t=times[0],\n",
    "                index=dexs[0],\n",
    "                ntype=\"IK\",\n",
    "                signal=\"NS\",\n",
    "                mtype=filtered.modes.SK[j]\n",
    "            ))\n",
    "            if filtered.modes.end[j]:\n",
    "                nodes.append(Node(\n",
    "                    satcat=satcat,\n",
    "                    t=starttime+timedelta(hours=indices[1]*2),\n",
    "                    index=indices[1],\n",
    "                    ntype=\"ID\",\n",
    "                    signal=\"NS\",\n",
    "                    mtype=\"NK\"\n",
    "                ))\n",
    "        elif filtered.indices[0][0] == 0:\n",
    "            ssNS.mtype = filtered.modes.SK[0]\n",
    "        else:\n",
    "            ssNS.mtype = \"NK\"\n",
    "nodes.append(ssNS)\n",
    "nodes.sort(key=lambda x: x.t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be14dca3-164d-4321-8837-ae363fc7921b",
   "metadata": {},
   "source": [
    "## Data postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b15541-bc55-49b8-9adb-0e03bec20ffd",
   "metadata": {},
   "source": [
    "Lastly, we will tidy out the output of the SNICT algorithm to match the SPLID data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "926c3a27-899b-45a1-a209-4bffa79a41a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T00:55:54.273581154Z",
     "start_time": "2024-01-15T00:55:54.267065911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ObjectID  TimeIndex Direction Node  Type\n",
      "0       103          0        EW   SS    NK\n",
      "1       103          0        NS   SS  None\n",
      "2       103         15        EW   AD    NK\n",
      "3       103        214        EW   AD    NK\n",
      "4       103        342        EW   IK    CK\n",
      "5       103        360        NS   IK    CK\n",
      "6       103       2172        ES   ES    ES\n"
     ]
    }
   ],
   "source": [
    "# Convert timestamp back into timeindex and format the output to the correct format in a pandas dataframe\n",
    "ObjectID_list = []\n",
    "TimeIndex_list = []\n",
    "Direction_list = []\n",
    "Node_list = []\n",
    "Type_list = []\n",
    "for i in range(len(nodes)):\n",
    "    ObjectID_list.append(nodes[i].satcat)\n",
    "    TimeIndex_list.append(int(((nodes[i].t-starttime).days*24+(nodes[i].t-starttime).seconds/3600)/2))\n",
    "    Direction_list.append(nodes[i].signal)\n",
    "    Node_list.append(nodes[i].type)\n",
    "    Type_list.append(nodes[i].mtype)\n",
    "    \n",
    "# Initialize data of lists. \n",
    "data = {'ObjectID': ObjectID_list, \n",
    "        'TimeIndex': TimeIndex_list,\n",
    "        'Direction': Direction_list, \n",
    "        'Node': Node_list,\n",
    "        'Type': Type_list} \n",
    "  \n",
    "# Create the pandas DataFrame \n",
    "prediction = pd.DataFrame(data) \n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92d534c1-bbe9-4c72-9d13-253a27bfdbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the prediction into a csv file \n",
    "prediction.to_csv('prediction.csv', index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
